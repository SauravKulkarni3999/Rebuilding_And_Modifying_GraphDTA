# GraphDTA-3D: Structure-Aware Drugâ€“Target Binding Affinity Prediction

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

This repository provides the code for GraphDTA-3D, a model that integrates 3D protein structural information (from AlphaFold) to predict drug-target binding affinity. It also includes an implementation of the baseline GraphDTA model.

## Resources:

* **README.md**: This file.
* **Data**:
    * The original Davis and KIBA datasets (including `Y`, `ligands_can.txt`, `proteins.txt`, and fold splits) can be obtained from the [DeepDTA GitHub repository](https://github.com/hkmztrk/DeepDTA/tree/master/data).
    * This project expects these raw files to be placed in `data/davis/` and `data/kiba/` respectively.
    * Processed data (e.g., PyTorch Geometric data objects) will be generated by the preprocessing scripts and saved in `data/{dataset_name}/processed/`.
    * Protein PDB files (downloaded from AlphaFold) are expected in `data/{dataset_name}/pdb_files/`.
* **Project Report**: [Final Project Report.docx](Final%20Project%20Report.docx) (Contains detailed methodology, results, and discussion). [cite: 1]

## Source codes:

* `src/preprocessing/`: Contains scripts to prepare data.
    * `process_drug_graphs.py`: Creates drug graph data objects from SMILES.
    * `process_protein_sequences.py`: Encodes protein sequences into tensors (for GraphDTA).
    * `map_proteins_to_uniprot.py`: Maps protein sequences to UniProt IDs.
    * `download_pdb_structures.py`: Downloads PDB structures from AlphaFold.
    * `generate_protein_graphs.py`: Creates protein graph data objects from PDB files (for GraphDTA-3D).
* `src/utils/`: Includes utility functions for molecule/protein processing, metrics, and I/O.
    * `molecule_utils.py`: Functions for drug feature extraction and graph creation.
    * `protein_utils.py`: Functions for protein sequence and PDB processing.
    * `metrics.py`: Performance measures (RMSE, CI).
    * `io_utils.py`: Utility for saving results and setting seeds. [cite: 6]
* `src/dataloaders/`:
    * `datasets.py`: Defines `GraphDTADataset` and `GraphDTA3DDataset`.
    * `custom_collate.py`: Defines collate functions for the dataloaders.
* `src/models/`:
    * `graphdta_model.py`: Defines the `GraphDTANet` model (baseline). [cite: 7]
    * `graphdta3d_model.py`: Defines the `GraphDTA3DNet` model (structure-aware). [cite: 8]
* `src/training_workflow.py`: Contains the `train_epoch` and `evaluate_epoch` functions.
* `src/train_model.py`: Main script to train and evaluate GraphDTA or GraphDTA-3D models.
* `demo_predict.py`: Script to make predictions using a trained model.
* `weights/`: Directory containing pre-trained models and model weights.
   
   
## Step-by-step running:

### 0. Install Python libraries needed

Ensure you have Python 3.8+ and Conda (or Pip) installed.

* **Create Conda Environment (recommended):**
    ```bash
    conda env create -f environment.yml
    conda activate graphdta3d_env
    ```
    (The `environment.yml` should specify PyTorch, PyTorch Geometric, RDKit, Pandas, Biopython, etc.)

* **Or Install with Pip:**
    ```bash
    pip install -r requirements.txt
    ```
    (The `requirements.txt` file lists necessary packages including `torch`, `torch-geometric` (and its dependencies like `torch-scatter`, `torch-sparse`), `rdkit-pypi`, `pandas`, `biopython`.)

    *Note: Specific PyTorch Geometric dependencies might require careful installation matching your PyTorch and CUDA versions. The `requirements.txt` provided earlier attempts to use direct links from your notebook logs for torch 2.0.1 + cu118.*

### 1. Download and Prepare Raw Data

* Download the Davis and KIBA datasets from the [DeepDTA GitHub repository](https://github.com/hkmztrk/DeepDTA/tree/master/data).
* Place the contents into `./data/davis/` and `./data/kiba/` respectively.
    * For Davis, you should have: `ligands_can.txt`, `proteins.txt`, `Y`, and the `folds/` directory.
    * For KIBA, you should have: `ligands_can.txt`, `proteins.txt`, `Y`, and the `folds/` directory.
* **Create helper CSVs and FASTA files:** (Your project seems to use custom CSVs like `drugs.csv`, `proteins.csv` for Davis, and `kiba_affinity_df.csv` for KIBA. Ensure these are prepared and placed in `./data/davis/` and `./data/kiba/` or adjust preprocessing scripts accordingly.)
    * `data/davis/drugs.csv` (containing Drug_Index, Canonical_SMILES)
    * `data/davis/proteins.csv` (containing Protein_Index, Sequence)
    * `data/davis/drug_protein_affinity.csv` (Drug_Index, Protein_Index, Affinity)
    * `data/kiba/kiba_affinity_df.csv` (Drug_Index, Protein_Index, SMILES, Sequence, KIBA_Score)
    * `data/davis/davis_proteins.fasta` (FASTA format of Davis proteins)
    * `data/kiba/kiba_proteins.fasta` (FASTA format of KIBA proteins)
* Download UniProt Swiss-Prot FASTA:
    ```bash
    mkdir -p data/external
    wget -O data/external/uniprot_sprot.fasta.gz "[https://ftp.uniprot.org/pub/databases/uniprot/current_release/knowledgebase/complete/uniprot_sprot.fasta.gz](https://ftp.uniprot.org/pub/databases/uniprot/current_release/knowledgebase/complete/uniprot_sprot.fasta.gz)"
    gunzip data/external/uniprot_sprot.fasta.gz
    ```

### 2. Run Preprocessing Scripts
Activate your conda environment (`conda activate graphdta3d_env`) before running these. Paths are relative to the repository root.

* **Drug Graph Generation:**
    ```bash
    python src/preprocessing/process_drug_graphs.py --dataset_name davis --csv_path ./data/davis/drugs.csv --smiles_col Canonical_SMILES --drug_id_col Drug_Index --output_dir ./data/processed/davis
    python src/preprocessing/process_drug_graphs.py --dataset_name kiba --csv_path ./data/kiba/kiba_affinity_df.csv --smiles_col SMILES --drug_id_col Drug_Index --output_dir ./data/processed/kiba
    ```
* **Protein Sequence Encoding (for GraphDTA baseline):**
    ```bash
    python src/preprocessing/process_protein_sequences.py --dataset_name davis --csv_path ./data/davis/proteins.csv --sequence_col Sequence --protein_id_col Protein_Index --output_dir ./data/processed/davis
    python src/preprocessing/process_protein_sequences.py --dataset_name kiba --csv_path ./data/kiba/kiba_affinity_df.csv --sequence_col Sequence --protein_id_col Protein_Index --output_dir ./data/processed/kiba
    ```
* **Protein 3D Structure Processing (for GraphDTA-3D):**
    ```bash
    # Map to UniProt
    python src/preprocessing/map_proteins_to_uniprot.py --query_fasta ./data/davis/davis_proteins.fasta --uniprot_fasta ./data/external/uniprot_sprot.fasta --output_csv ./data/processed/davis/davis_uniprot_mapping.csv
    python src/preprocessing/map_proteins_to_uniprot.py --query_fasta ./data/kiba/kiba_proteins.fasta --uniprot_fasta ./data/external/uniprot_sprot.fasta --output_csv ./data/processed/kiba/kiba_uniprot_mapping.csv

    # Download PDBs
    python src/preprocessing/download_pdb_structures.py --mapping_csv ./data/processed/davis/davis_uniprot_mapping.csv --pdb_dir ./data/davis/pdb_files
    python src/preprocessing/download_pdb_structures.py --mapping_csv ./data/processed/kiba/kiba_uniprot_mapping.csv --pdb_dir ./data/kiba/pdb_files

    # Generate Protein Graphs from PDBs
    python src/preprocessing/generate_protein_graphs.py --pdb_dir ./data/davis/pdb_files --output_path ./data/processed/davis/davis_protein_graphs.pt
    python src/preprocessing/generate_protein_graphs.py --pdb_dir ./data/kiba/pdb_files --output_path ./data/processed/kiba/kiba_protein_graphs.pt
    ```
    This will create `.pt` files in `data/processed/{dataset_name}/`.

### 3. Train a Prediction Model

Use `src/train_model.py` to train the models. The script saves the best model (based on validation CI) and a JSON results file in the specified output directory.

* **Train GraphDTA-3D on Davis:**
    ```bash
    python src/train_model.py \
        --dataset_name davis \
        --model_architecture GraphDTA3D \
        --affinity_file drug_protein_affinity.csv \
        --affinity_col_name Affinity \
        --epochs 500 \
        --batch_size 512 \
        --learning_rate 0.0005 \
        --dropout_rate 0.3 \
        --output_dir ./training_runs/GraphDTA3D_Davis_run01
    ```

* **Train GraphDTA (baseline) on Davis:**
    ```bash
    python src/train_model.py \
        --dataset_name davis \
        --model_architecture GraphDTA \
        --affinity_file drug_protein_affinity.csv \
        --affinity_col_name Affinity \
        --epochs 500 \
        --batch_size 512 \
        --learning_rate 0.0005 \
        --dropout_rate 0.2 \
        --output_dir ./training_runs/GraphDTA_Davis_run01
    ```
    *(Adapt for KIBA dataset by changing `--dataset_name`, `--affinity_file`, and `--affinity_col_name` accordingly.)*

    The training script uses an 80/10/10 train/validation/test split. [cite: 13] The best model is chosen based on validation set Concordance Index (CI). [cite: 13]

### 4. Make Predictions with a Trained Model

Use `demo_predict.py` for predictions on new drug-protein pairs.

* **Example for GraphDTA-3D model (Davis):**
    ```bash
    python demo_predict.py \
      --model_architecture GraphDTA3D \
      --model_weights_path ./weights/graphdta3d_davis.pt \
      --smiles "YOUR_DRUG_SMILES_STRING" \
      --protein_pdb_path ./data/davis/pdb_files/Protein_0.pdb \
      --dropout_rate 0.3
    ```

* **Example for GraphDTA model (Davis):**
    ```bash
    python demo_predict.py \
      --model_architecture GraphDTA \
      --model_weights_path ./weights/graphdta_davis_baseline_best_model.pt \
      --smiles "YOUR_DRUG_SMILES_STRING" \
      --protein_sequence "YOUR_PROTEIN_SEQUENCE" \
      --dropout_rate 0.2
    ```

This README provides a more direct, command-oriented guide, similar to the example you showed. Remember to create the helper CSVs and FASTA files mentioned in "Step 1" or adjust the preprocessing scripts if your raw data format is different.