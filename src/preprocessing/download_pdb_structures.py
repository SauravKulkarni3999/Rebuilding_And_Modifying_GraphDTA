# -*- coding: utf-8 -*-
"""Untitled3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/10QcGbbtXIa25DWlMPM4bNDzbZ2oTvdpi
"""

# src/preprocessing/download_pdb_structures.py
import os
import argparse
import pandas as pd
import requests
from tqdm import tqdm

def download_alphafold_pdbs(mapping_csv_path, pdb_output_dir):
    """
    Downloads AlphaFold PDB files based on a UniProt mapping CSV.
    """
    try:
        df_map = pd.read_csv(mapping_csv_path)
    except FileNotFoundError:
        print(f"Error: Mapping CSV not found at {mapping_csv_path}")
        return

    os.makedirs(pdb_output_dir, exist_ok=True)
    print(f"Downloading PDBs to {pdb_output_dir}...")

    downloaded_count = 0
    not_found_count = 0
    error_count = 0

    for _, row in tqdm(df_map.iterrows(), total=len(df_map), desc="Downloading PDBs"):
        query_protein_id = row["Query_Protein_ID"] # Or "Protein_ID" as in your notebook
        uniprot_id = row["UniProt_ID"]

        if pd.isna(uniprot_id):
            print(f"Skipping {query_protein_id}: No UniProt ID.")
            not_found_count +=1
            continue

        # Standard AlphaFold PDB URL format (v4 is common, adjust if needed)
        # AF-{UniProt_ID}-F1-model_v{version}.pdb
        # Let's try v4 first, then v3, v2, v1 if v4 fails.
        versions_to_try = ["4", "3", "2", "1"]
        pdb_downloaded_for_id = False

        for version in versions_to_try:
            pdb_url = f"https://alphafold.ebi.ac.uk/files/AF-{uniprot_id}-F1-model_v{version}.pdb"
            # Use query_protein_id for filename to match how protein_graphs are keyed
            save_path = os.path.join(pdb_output_dir, f"{query_protein_id}.pdb")

            if os.path.exists(save_path):
                # print(f"PDB for {query_protein_id} ({uniprot_id}) already exists. Skipping.")
                pdb_downloaded_for_id = True
                downloaded_count +=1 # Count if already exists
                break

            try:
                response = requests.get(pdb_url, timeout=30)
                if response.status_code == 200:
                    with open(save_path, "wb") as f:
                        f.write(response.content)
                    # print(f"Downloaded PDB for {query_protein_id} ({uniprot_id}, model_v{version})")
                    downloaded_count += 1
                    pdb_downloaded_for_id = True
                    break # Successfully downloaded this version
                # elif response.status_code == 404 and version != versions_to_try[-1]:
                    # print(f"Not found: {uniprot_id} model_v{version}. Trying next version...")
                    # continue # Try next version
                # else: # Other error or last version tried and failed
                    # print(f"Failed to download PDB for {query_protein_id} ({uniprot_id}, model_v{version}). Status: {response.status_code}")
                    # pass # Will fall through to not_found_count if all versions fail

            except requests.exceptions.RequestException as e:
                print(f"Error downloading PDB for {query_protein_id} ({uniprot_id}, model_v{version}): {e}")
                # error_count +=1 # Only count error once per protein ID
                # break # Stop trying for this ID if a network error occurs

        if not pdb_downloaded_for_id and not os.path.exists(save_path) :
            not_found_count += 1
            # print(f"PDB for {query_protein_id} ({uniprot_id}) not found on AlphaFold DB after trying all versions.")


    print(f"\nDownload complete. Downloaded: {downloaded_count}, Not Found/Skipped: {not_found_count}, Errors: {error_count}")


def main():
    parser = argparse.ArgumentParser(description="Download AlphaFold PDB structures.")
    parser.add_argument("--mapping_csv", type=str, required=True,
                        help="Path to the CSV file mapping protein IDs to UniProt IDs.")
    parser.add_argument("--pdb_dir", type=str, required=True,
                        help="Directory to save the downloaded PDB files.")

    args = parser.parse_args()
    download_alphafold_pdbs(args.mapping_csv, args.pdb_dir)

if __name__ == "__main__":
    # Example:
    # python src/preprocessing/download_pdb_structures.py --mapping_csv ./data/processed/davis/davis_uniprot_mapping.csv --pdb_dir ./data/davis/pdb_files
    # python src/preprocessing/download_pdb_structures.py --mapping_csv ./data/processed/kiba/kiba_uniprot_mapping.csv --pdb_dir ./data/kiba/pdb_files
    main()