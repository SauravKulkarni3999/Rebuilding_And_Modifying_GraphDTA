# -*- coding: utf-8 -*-
"""Untitled7.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1VrJe4ajfZDqcXU9V1WuGZO-ElzpFGe7A
"""

# src/models/graphdta_model.py
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch_geometric.nn import GCNConv, global_max_pool

class GraphDTANet(nn.Module): # Renamed to avoid conflict if another GraphDTA class exists
    def __init__(self, num_atom_features=83, protein_vocab_size=21, # 20 AAs + 1 padding/unknown
                 protein_embed_dim=128, protein_cnn_out_channels=128, protein_cnn_kernel_size=8,
                 drug_gcn_hidden_multipliers=[2, 4, 8], # Multipliers for num_atom_features
                 drug_fc_dims=[1024, 512], drug_output_dim=128,
                 protein_output_dim=128, # Output from protein CNN after pooling
                 mlp_hidden_dims=[1024, 512], dropout_rate=0.2, output_dim=1):

        super(GraphDTANet, self).__init__()

        # Drug Encoder (GCN)
        self.gcn_layers = nn.ModuleList()
        current_dim = num_atom_features
        for mult in drug_gcn_hidden_multipliers:
            self.gcn_layers.append(GCNConv(current_dim, num_atom_features * mult))
            current_dim = num_atom_features * mult

        # Drug FC layers after GCN and pooling
        self.drug_fc_layers = nn.ModuleList()
        current_fc_dim = current_dim # Output dim from last GCN layer
        for fc_dim in drug_fc_dims:
            self.drug_fc_layers.append(nn.Linear(current_fc_dim, fc_dim))
            current_fc_dim = fc_dim
        self.drug_fc_layers.append(nn.Linear(current_fc_dim, drug_output_dim))


        # Protein Encoder (1D CNN)
        self.protein_embedding = nn.Embedding(protein_vocab_size, protein_embed_dim, padding_idx=0)
        # Based on GraphDTA paper, typically 3 conv layers
        self.protein_conv1 = nn.Conv1d(protein_embed_dim, protein_cnn_out_channels, kernel_size=protein_cnn_kernel_size)
        # The original notebook used two, but GraphDTA paper often has more. Let's stick to notebook first.
        # self.protein_conv2 = nn.Conv1d(protein_cnn_out_channels, protein_cnn_out_channels * 2, kernel_size=protein_cnn_kernel_size)
        # self.protein_conv3 = nn.Conv1d(protein_cnn_out_channels * 2, protein_cnn_out_channels * 4, kernel_size=protein_cnn_kernel_size)
        # Sticking to the 2-conv structure from the provided `03_GraphDTA_Baseline.ipynb` for `GraphDTA`
        self.protein_conv2 = nn.Conv1d(protein_cnn_out_channels, protein_output_dim, kernel_size=protein_cnn_kernel_size)


        self.protein_pool = nn.AdaptiveMaxPool1d(1) # Global max pooling

        # Fusion MLP
        self.fc_combined_layers = nn.ModuleList()
        combined_input_dim = drug_output_dim + protein_output_dim # from protein_output_dim
        current_mlp_dim = combined_input_dim
        for hidden_dim in mlp_hidden_dims:
            self.fc_combined_layers.append(nn.Linear(current_mlp_dim, hidden_dim))
            current_mlp_dim = hidden_dim
        self.output_layer = nn.Linear(current_mlp_dim, output_dim)

        self.relu = nn.ReLU()
        self.dropout = nn.Dropout(p=dropout_rate)

    def forward(self, drug_data, protein_seq_data):
        # Drug processing
        x_drug, edge_index_drug, batch_drug = drug_data.x, drug_data.edge_index, drug_data.batch

        for gcn_layer in self.gcn_layers:
            x_drug = self.relu(gcn_layer(x_drug, edge_index_drug))
        # No dropout between GCN layers in the reference notebook's model structure implied by the simple gcn1,2,3 calls

        drug_emb_pooled = global_max_pool(x_drug, batch_drug) # Max pooling over nodes

        x_drug_fc = drug_emb_pooled
        for i, fc_layer in enumerate(self.drug_fc_layers):
            x_drug_fc = self.relu(fc_layer(x_drug_fc))
            if i < len(self.drug_fc_layers) -1 : # Apply dropout to all but the last FC layer's output for drug
                 x_drug_fc = self.dropout(x_drug_fc)
        drug_final_emb = x_drug_fc # This is `drug_output_dim`

        # Protein processing
        protein_embedded_seq = self.protein_embedding(protein_seq_data) # (batch, max_seq_len, embed_dim)
        protein_embedded_seq = protein_embedded_seq.permute(0, 2, 1) # (batch, embed_dim, max_seq_len)

        protein_conv = self.relu(self.protein_conv1(protein_embedded_seq))
        protein_conv = self.relu(self.protein_conv2(protein_conv))
        # Add more conv layers here if desired for deeper protein CNN

        protein_pooled = self.protein_pool(protein_conv).squeeze(-1) # (batch, protein_output_dim)
        protein_final_emb = protein_pooled # This is `protein_output_dim`

        # Fusion
        combined_representation = torch.cat((drug_final_emb, protein_final_emb), dim=1)

        x_combined = combined_representation
        for i, fc_layer in enumerate(self.fc_combined_layers):
            x_combined = self.relu(fc_layer(x_combined))
            x_combined = self.dropout(x_combined) # Dropout after each hidden MLP layer

        output = self.output_layer(x_combined)

        if output.shape[-1] == 1: # For regression
            output = output.squeeze(-1)

        return output