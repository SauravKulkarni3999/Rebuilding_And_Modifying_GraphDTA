# -*- coding: utf-8 -*-
"""Untitled7.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/10sORkzMSZDPYVNDpDgZA1MLGFdvHmpSV
"""

# src/models/graphdta3d_model.py
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch_geometric.nn import GCNConv, global_max_pool

class GraphDTA3DNet(nn.Module): # Renamed for clarity
    def __init__(self,
                 atom_dim=83,      # Input dimension for drug atom features
                 residue_dim=20,   # Input dimension for protein residue features (e.g., one-hot AA for C-alpha graph)
                 drug_gcn_hidden_dims=[64, 128, 128],
                 protein_gcn_hidden_dims=[64, 128, 128],
                 mlp_hidden_dims=[1024, 512],
                 dropout_rate=0.3, # From your notebook
                 output_dim_final=1):
        super(GraphDTA3DNet, self).__init__()

        # Drug Encoder (GCN)
        self.drug_gcn_layers = nn.ModuleList()
        current_dim_drug = atom_dim
        for hidden_dim in drug_gcn_hidden_dims:
            self.drug_gcn_layers.append(GCNConv(current_dim_drug, hidden_dim))
            current_dim_drug = hidden_dim
        self.drug_output_dim = current_dim_drug

        # Protein Encoder (GCN for protein graphs)
        self.protein_gcn_layers = nn.ModuleList()
        current_dim_protein = residue_dim
        for hidden_dim in protein_gcn_hidden_dims:
            self.protein_gcn_layers.append(GCNConv(current_dim_protein, hidden_dim))
            current_dim_protein = hidden_dim
        self.protein_output_dim = current_dim_protein

        # Fusion MLP
        combined_input_dim = self.drug_output_dim + self.protein_output_dim

        self.fc_combined_layers = nn.ModuleList()
        current_mlp_dim = combined_input_dim
        for hidden_dim in mlp_hidden_dims:
            self.fc_combined_layers.append(nn.Linear(current_mlp_dim, hidden_dim))
            current_mlp_dim = hidden_dim
        self.output_layer = nn.Linear(current_mlp_dim, output_dim_final)

        self.relu = nn.ReLU()
        self.dropout = nn.Dropout(dropout_rate) # Single dropout instance for reusability

    def forward(self, drug_data, protein_graph_data):
        # Drug encoding
        x_drug, edge_index_drug, batch_drug = drug_data.x, drug_data.edge_index, drug_data.batch
        for layer in self.drug_gcn_layers:
            x_drug = self.relu(layer(x_drug, edge_index_drug))
            # x_drug = self.dropout(x_drug) # Optional: dropout after GCN layers
        drug_emb_pooled = global_max_pool(x_drug, batch_drug)

        # Protein graph encoding
        x_protein, edge_index_protein, batch_protein = protein_graph_data.x, protein_graph_data.edge_index, protein_graph_data.batch
        for layer in self.protein_gcn_layers:
            x_protein = self.relu(layer(x_protein, edge_index_protein))
            # x_protein = self.dropout(x_protein) # Optional: dropout after GCN layers
        protein_emb_pooled = global_max_pool(x_protein, batch_protein)

        # Fusion
        combined_emb = torch.cat([drug_emb_pooled, protein_emb_pooled], dim=1)

        x_fused = combined_emb
        for i, fc_layer in enumerate(self.fc_combined_layers):
            x_fused = self.relu(fc_layer(x_fused))
            x_fused = self.dropout(x_fused) # Dropout after each hidden MLP layer

        output = self.output_layer(x_fused)

        if output.shape[-1] == 1: # For regression
            output = output.squeeze(-1)
        return output