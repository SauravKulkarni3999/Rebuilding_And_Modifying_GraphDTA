# -*- coding: utf-8 -*-
"""Untitled9.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1qC4-3ePs6M8lgtUB-e3jNUQ_ivcSknHK
"""

# src/train_model.py
import os
import argparse
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, random_split
import pandas as pd
import json

from utils.io_utils import set_seed, save_json_metrics
from utils.protein_utils import MAX_SEQ_LEN, AA_TO_IDX_SEQ # For GraphDTA
from utils.molecule_utils import ATOM_TYPES # For feature dim consistency if needed by model directly

from dataloaders.datasets import GraphDTADataset, GraphDTA3DDataset
from dataloaders.custom_collate import collate_fn_graphdta, collate_fn_graphdta3d

from models.graphdta_model import GraphDTANet
from models.graphdta3d_model import GraphDTA3DNet

from training_workflow import train_epoch, evaluate_epoch


def main(args):
    set_seed(args.seed)
    device = torch.device("cuda" if torch.cuda.is_available() and args.use_cuda else "cpu")
    print(f"Using device: {device}")

    # --- Load Data ---
    print("Loading data...")
    affinity_df = pd.read_csv(os.path.join(args.data_dir, args.dataset_name, args.affinity_file))
    drug_graphs = torch.load(os.path.join(args.data_dir, "processed", args.dataset_name, f"{args.dataset_name}_drug_graphs.pt"))

    if args.model_architecture == "GraphDTA":
        protein_data = torch.load(os.path.join(args.data_dir, "processed", args.dataset_name, f"{args.dataset_name}_protein_sequence_tensors.pt"))
        full_dataset = GraphDTADataset(affinity_df, drug_graphs, protein_data, affinity_col=args.affinity_col_name)
        collate_fn = collate_fn_graphdta
        model_type_str = "GraphDTA"
    elif args.model_architecture == "GraphDTA3D":
        protein_data = torch.load(os.path.join(args.data_dir, "processed", args.dataset_name, f"{args.dataset_name}_protein_graphs.pt"))

        # Filter affinity_df for proteins that have graph representations
        valid_protein_indices = set(protein_data.keys())
        original_len = len(affinity_df)
        affinity_df = affinity_df[affinity_df[args.protein_id_col_name].isin(valid_protein_indices)].reset_index(drop=True)
        print(f"Filtered affinity data: {len(affinity_df)} pairs from {original_len} (due to protein graph availability).")

        full_dataset = GraphDTA3DDataset(affinity_df, drug_graphs, protein_data, affinity_col=args.affinity_col_name)
        collate_fn = collate_fn_graphdta3d
        model_type_str = "GraphDTA3D"
    else:
        raise ValueError("Invalid model_architecture specified.")

    print(f"Total dataset size: {len(full_dataset)}")
    if len(full_dataset) == 0:
        print("Error: Dataset is empty. Check data paths and preprocessing steps.")
        return

    # --- Dataset Splitting ---
    train_size = int(args.train_split * len(full_dataset))
    val_size = int(args.val_split * len(full_dataset))
    test_size = len(full_dataset) - train_size - val_size

    if train_size == 0 or val_size == 0 or test_size == 0:
        print(f"Error: Not enough data for splits. Train: {train_size}, Val: {val_size}, Test: {test_size}")
        return

    split_generator = torch.Generator().manual_seed(args.seed) # For reproducible splits
    train_set, val_set, test_set = random_split(full_dataset, [train_size, val_size, test_size], generator=split_generator)
    print(f"Train set size: {len(train_set)}, Validation set size: {len(val_set)}, Test set size: {len(test_set)}")

    train_loader = DataLoader(train_set, batch_size=args.batch_size, shuffle=True, collate_fn=collate_fn, num_workers=args.num_workers)
    val_loader = DataLoader(val_set, batch_size=args.batch_size, shuffle=False, collate_fn=collate_fn, num_workers=args.num_workers)
    test_loader = DataLoader(test_set, batch_size=args.batch_size, shuffle=False, collate_fn=collate_fn, num_workers=args.num_workers)

    # --- Model Initialization ---
    # Atom features: use the length of the feature vector from the first drug graph
    # This assumes all drug graphs have the same feature dimension.
    sample_drug_graph_key = next(iter(drug_graphs)) # Get a key from drug_graphs
    num_atom_features = drug_graphs[sample_drug_graph_key].x.shape[1]
    print(f"Detected {num_atom_features} atom features.")


    if args.model_architecture == "GraphDTA":
        model = GraphDTANet(
            num_atom_features=num_atom_features, # From actual data
            protein_vocab_size=len(AA_TO_IDX_SEQ) + 1, # Vocab size + padding/unknown
            protein_embed_dim=args.protein_embed_dim,
            protein_cnn_out_channels=args.protein_cnn_out_channels,
            protein_cnn_kernel_size=args.protein_cnn_kernel_size,
            drug_output_dim=args.drug_output_dim,
            protein_output_dim=args.protein_output_dim_cnn, # Specific for GraphDTA's protein CNN
            mlp_hidden_dims=args.mlp_hidden_dims,
            dropout_rate=args.dropout_rate
        ).to(device)
    elif args.model_architecture == "GraphDTA3D":
        # Protein features: use the length from the first protein graph
        sample_prot_graph_key = next(iter(protein_data))
        protein_residue_dim = protein_data[sample_prot_graph_key].x.shape[1]
        print(f"Detected {protein_residue_dim} protein residue features for GraphDTA-3D.")
        model = GraphDTA3DNet(
            atom_dim=num_atom_features,
            residue_dim=protein_residue_dim, # From actual protein graph data
            drug_gcn_hidden_dims=args.drug_gcn_dims,
            protein_gcn_hidden_dims=args.protein_gcn_dims,
            mlp_hidden_dims=args.mlp_hidden_dims,
            dropout_rate=args.dropout_rate
        ).to(device)

    optimizer = optim.Adam(model.parameters(), lr=args.learning_rate)
    criterion = nn.MSELoss()

    # --- Training Loop ---
    print(f"Starting training for {model_type_str} on {args.dataset_name}...")
    best_val_ci = -1.0 # Initialize with a value that will be easily beaten
    best_epoch = 0
    train_history = []

    for epoch in range(1, args.epochs + 1):
        train_loss = train_epoch(model, device, train_loader, optimizer, criterion, model_type=model_type_str)
        val_metrics = evaluate_epoch(model, device, val_loader, criterion, model_type=model_type_str)

        epoch_results = {
            "epoch": epoch,
            "train_loss": train_loss,
            "val_loss": val_metrics['loss'],
            "val_rmse": val_metrics['rmse'],
            "val_ci": val_metrics['ci']
        }
        train_history.append(epoch_results)

        print(f"Epoch {epoch:03d}/{args.epochs:03d} | Train Loss: {train_loss:.4f} | Val Loss: {val_metrics['loss']:.4f} | Val RMSE: {val_metrics['rmse']:.4f} | Val CI: {val_metrics['ci']:.4f}")

        if val_metrics['ci'] > best_val_ci:
            best_val_ci = val_metrics['ci']
            best_epoch = epoch
            if args.output_dir:
                os.makedirs(args.output_dir, exist_ok=True)
                model_save_path = os.path.join(args.output_dir, f"{args.model_architecture.lower()}_{args.dataset_name}_best_model.pt")
                torch.save(model.state_dict(), model_save_path)
                print(f"Best model saved to {model_save_path} (Epoch {best_epoch}, Val CI: {best_val_ci:.4f})")

        # Early stopping (optional, based on your report's setup)
        # Early stopping based on validation CI as per report [cite: 13]

    print(f"\nTraining complete. Best Val CI: {best_val_ci:.4f} at Epoch {best_epoch}")

    # --- Testing ---
    if args.output_dir:
        print("Loading best model for testing...")
        best_model_path = os.path.join(args.output_dir, f"{args.model_architecture.lower()}_{args.dataset_name}_best_model.pt")
        if os.path.exists(best_model_path):
            model.load_state_dict(torch.load(best_model_path, map_location=device))
        else:
            print("Warning: Best model file not found. Testing with the last epoch model.")

    test_metrics = evaluate_epoch(model, device, test_loader, criterion, model_type=model_type_str)
    print(f"\nTest Set Performance ({model_type_str} on {args.dataset_name}):")
    print(f"  Test Loss: {test_metrics['loss']:.4f}")
    print(f"  Test RMSE: {test_metrics['rmse']:.4f}")
    print(f"  Test CI:   {test_metrics['ci']:.4f}")

    # --- Save Metrics ---
    if args.output_dir:
        final_metrics_summary = {
            "dataset": args.dataset_name,
            "model_architecture": args.model_architecture,
            "best_epoch": best_epoch,
            "best_val_ci": best_val_ci,
            "test_loss": test_metrics['loss'],
            "test_rmse": test_metrics['rmse'],
            "test_ci": test_metrics['ci'],
            "config": vars(args),
            "training_history": train_history
        }
        metrics_filename = f"{args.model_architecture.lower()}_{args.dataset_name}_results.json"
        save_json_metrics(final_metrics_summary, os.path.join(args.output_dir, metrics_filename))


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Train GraphDTA or GraphDTA-3D models.")

    # Common arguments
    parser.add_argument("--data_dir", type=str, default="./data", help="Root directory for data")
    parser.add_argument("--dataset_name", type=str, required=True, choices=["davis", "kiba"], help="Dataset name")
    parser.add_argument("--model_architecture", type=str, required=True, choices=["GraphDTA", "GraphDTA3D"], help="Model to train")
    parser.add_argument("--affinity_file", type=str, help="Name of the affinity CSV file (e.g., drug_protein_affinity.csv or kiba_affinity_df.csv)")
    parser.add_argument("--affinity_col_name", type=str, help="Name of the affinity column in the CSV (e.g. Affinity or KIBA_Score)")
    parser.add_argument("--protein_id_col_name", type=str, default="Protein_Index", help="Name of the protein ID column")


    parser.add_argument("--output_dir", type=str, default="./results_and_weights", help="Directory to save models and results")
    parser.add_argument("--seed", type=int, default=42, help="Random seed for reproducibility") # [cite: 6]
    parser.add_argument("--epochs", type=int, default=100, help="Number of training epochs") # Example, adjust as needed
    parser.add_argument("--batch_size", type=int, default=512, help="Batch size") # [cite: 13]
    parser.add_argument("--learning_rate", type=float, default=0.0005, help="Learning rate for Adam optimizer") # From notebooks
    parser.add_argument("--dropout_rate", type=float, default=0.2, help="Dropout rate") # Common value from notebooks
    parser.add_argument("--use_cuda", type=bool, default=True, help="Whether to use CUDA if available")
    parser.add_argument("--num_workers", type=int, default=0, help="Number of workers for DataLoader")

    parser.add_argument("--train_split", type=float, default=0.8, help="Fraction for training set") # [cite: 13]
    parser.add_argument("--val_split", type=float, default=0.1, help="Fraction for validation set") # [cite: 13]


    # GraphDTA specific
    parser.add_argument("--protein_embed_dim", type=int, default=128, help="Embedding dimension for protein sequences (GraphDTA)")
    parser.add_argument("--protein_cnn_out_channels", type=int, default=128, help="Output channels for protein CNN (GraphDTA)")
    parser.add_argument("--protein_cnn_kernel_size", type=int, default=8, help="Kernel size for protein CNN (GraphDTA)")
    parser.add_argument("--protein_output_dim_cnn", type=int, default=128, help="Final output dimension for protein CNN part of GraphDTA")


    # GraphDTA & GraphDTA3D shared GCN/MLP params (can be specialized if needed)
    # For GraphDTA, drug_gcn_dims will be used to define multipliers
    # For GraphDTA3D, drug_gcn_dims are direct hidden layer sizes
    parser.add_argument("--drug_gcn_dims", nargs='+', type=int, default=[64,128,128], help="Hidden dimensions for drug GCN layers")
    parser.add_argument("--drug_output_dim", type=int, default=128, help="Output dimension for drug encoder")

    # GraphDTA3D specific (protein GCN)
    parser.add_argument("--protein_gcn_dims", nargs='+', type=int, default=[64,128,128], help="Hidden dimensions for protein GCN layers (GraphDTA3D)")
    # protein_output_dim for GraphDTA3D is implicitly the last element of protein_gcn_dims

    parser.add_argument("--mlp_hidden_dims", nargs='+', type=int, default=[1024, 512], help="Hidden dimensions for the final MLP layers")


    args = parser.parse_args()

    # Set default affinity file and column based on dataset if not provided
    if not args.affinity_file:
        if args.dataset_name == "davis":
            args.affinity_file = "drug_protein_affinity.csv"
            args.affinity_col_name = "Affinity"
        elif args.dataset_name == "kiba":
            args.affinity_file = "kiba_affinity_df.csv"
            args.affinity_col_name = "KIBA_Score"

    if not args.affinity_col_name: # If still not set (e.g. affinity_file was provided but not col_name)
        if args.dataset_name == "davis": args.affinity_col_name = "Affinity"
        elif args.dataset_name == "kiba": args.affinity_col_name = "KIBA_Score"


    main(args)

    # Example Usage:
    # Train GraphDTA on Davis:
    # python src/train_model.py --dataset_name davis --model_architecture GraphDTA --epochs 100 --batch_size 512 --learning_rate 0.0005 --dropout_rate 0.2 --output_dir ./results/davis_graphdta

    # Train GraphDTA-3D on Davis:
    # python src/train_model.py --dataset_name davis --model_architecture GraphDTA3D --epochs 100 --batch_size 512 --learning_rate 0.0005 --dropout_rate 0.3 --output_dir ./results/davis_graphdta3d

    # Train GraphDTA on KIBA:
    # python src/train_model.py --dataset_name kiba --model_architecture GraphDTA --epochs 100 --batch_size 512 --learning_rate 0.0005 --dropout_rate 0.2 --output_dir ./results/kiba_graphdta

    # Train GraphDTA-3D on KIBA:
    # python src/train_model.py --dataset_name kiba --model_architecture GraphDTA3D --epochs 100 --batch_size 512 --learning_rate 0.0005 --dropout_rate 0.3 --output_dir ./results/kiba_graphdta3d