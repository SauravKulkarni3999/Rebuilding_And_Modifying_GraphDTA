# -*- coding: utf-8 -*-
"""Untitled10.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14OQe42Owj35_x38Z5IzL3Fb43Rmzi5lQ
"""

# demo_predict.py
import argparse
import torch
import os
import sys

# Ensure src directory is in Python path to import custom modules
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), 'src')))

from utils.molecule_utils import smiles_to_graph, ATOM_TYPES # ATOM_TYPES for num_atom_features consistency
from utils.protein_utils import sequence_to_tensor, pdb_to_graph, AA_TO_IDX_SEQ, MAX_SEQ_LEN, AMINO_ACIDS_GRAPH
from models.graphdta_model import GraphDTANet
from models.graphdta3d_model import GraphDTA3DNet
from torch_geometric.data import Batch # To create a batch for single samples

def predict_affinity(args):
    device = torch.device("cuda" if torch.cuda.is_available() and args.use_cuda else "cpu")
    print(f"Using device: {device}")

    # --- 1. Load Model ---
    print(f"Loading model: {args.model_architecture} with weights from {args.model_weights_path}")

    # Determine feature dimensions (these should match how the model was trained)
    # Based on your KIBA_Drug_Graph_Generation.ipynb, atom features = 83
    num_atom_features = len(ATOM_TYPES) + \
                        len(utils.molecule_utils.DEGREE_SET) + 1 + \
                        len(utils.molecule_utils.TOTAL_NUM_HS_SET) + 1 + \
                        len(utils.molecule_utils.TOTAL_VALENCE_SET) + 1 + \
                        len(utils.molecule_utils.IS_AROMATIC_SET) + 1 + \
                        len(utils.molecule_utils.CHIRAL_TAG_SET) + 1
    # num_atom_features is 83 as per corrected molecule_utils.py

    if args.model_architecture == "GraphDTA":
        # Default parameters similar to those in your GraphDTA_Baseline notebook
        model = GraphDTANet(
            num_atom_features=num_atom_features, # Should be 83
            protein_vocab_size=len(AA_TO_IDX_SEQ) + 1, # 20 + 1 for padding
            protein_embed_dim=128,
            protein_cnn_out_channels=128,
            protein_cnn_kernel_size=8,
            drug_output_dim=128,
            protein_output_dim=128, # Output from protein CNN part
            mlp_hidden_dims=[1024, 512],
            dropout_rate=args.dropout_rate # Use dropout from args, or a default like 0.2
        ).to(device)
    elif args.model_architecture == "GraphDTA3D":
        # Default parameters similar to those in your GraphDTA3D notebook
        model = GraphDTA3DNet(
            atom_dim=num_atom_features, # Should be 83
            residue_dim=len(AMINO_ACIDS_GRAPH), # 20 for one-hot encoded AA in protein graphs
            drug_gcn_hidden_dims=[64, 128, 128],
            protein_gcn_hidden_dims=[64, 128, 128],
            mlp_hidden_dims=[1024, 512],
            dropout_rate=args.dropout_rate # Use dropout from args, or a default like 0.3
        ).to(device)
    else:
        raise ValueError("Invalid model_architecture. Choose 'GraphDTA' or 'GraphDTA3D'.")

    try:
        model.load_state_dict(torch.load(args.model_weights_path, map_location=device))
    except FileNotFoundError:
        print(f"Error: Model weights file not found at {args.model_weights_path}")
        return
    except Exception as e:
        print(f"Error loading model weights: {e}")
        return

    model.eval()
    print("Model loaded successfully.")

    # --- 2. Preprocess Inputs ---
    print("Preprocessing inputs...")
    # Drug
    drug_graph = smiles_to_graph(args.smiles)
    if drug_graph is None:
        print(f"Error: Could not process SMILES string: {args.smiles}")
        return
    # Create a batch for a single drug graph
    drug_data = Batch.from_data_list([drug_graph]).to(device)


    # Protein
    if args.model_architecture == "GraphDTA":
        if not args.protein_sequence:
            print("Error: Protein sequence (--protein_sequence) is required for GraphDTA model.")
            return
        protein_tensor = sequence_to_tensor(args.protein_sequence, max_len=MAX_SEQ_LEN)
        # Pad to MAX_SEQ_LEN - models are usually trained with fixed-size input post-padding
        # For a single sequence, we still need to ensure it matches the expected input dimensions of the CNN
        # The collate_fn pads sequences in a batch. For a single prediction:
        if len(protein_tensor) < MAX_SEQ_LEN:
             protein_tensor_padded = torch.cat([
                protein_tensor,
                torch.zeros(MAX_SEQ_LEN - len(protein_tensor), dtype=torch.long)
            ])
        else:
            protein_tensor_padded = protein_tensor[:MAX_SEQ_LEN]

        protein_input_processed = protein_tensor_padded.unsqueeze(0).to(device) # Add batch dimension

    elif args.model_architecture == "GraphDTA3D":
        if not args.protein_pdb_path:
            print("Error: Protein PDB file path (--protein_pdb_path) is required for GraphDTA3D model.")
            return
        if not os.path.exists(args.protein_pdb_path):
            print(f"Error: Protein PDB file not found at {args.protein_pdb_path}")
            return
        protein_graph = pdb_to_graph(args.protein_pdb_path)
        if protein_graph is None:
            print(f"Error: Could not process PDB file: {args.protein_pdb_path}")
            return
        # Create a batch for a single protein graph
        protein_input_processed = Batch.from_data_list([protein_graph]).to(device)

    print("Inputs preprocessed.")

    # --- 3. Make Prediction ---
    print("Making prediction...")
    with torch.no_grad():
        predicted_affinity = model(drug_data, protein_input_processed)

    # --- 4. Display Result ---
    print(f"\n--- Prediction Result ---")
    print(f"  Drug SMILES: {args.smiles}")
    if args.model_architecture == "GraphDTA":
        print(f"  Protein Sequence (first 30 AA): {args.protein_sequence[:30]}...")
    else:
        print(f"  Protein PDB: {args.protein_pdb_path}")
    print(f"  Predicted Binding Affinity: {predicted_affinity.item():.4f}")


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Predict drug-target binding affinity using a pre-trained model.")

    parser.add_argument("--model_architecture", type=str, required=True, choices=["GraphDTA", "GraphDTA3D"],
                        help="Specify the model architecture: 'GraphDTA' or 'GraphDTA3D'.")
    parser.add_argument("--model_weights_path", type=str, required=True,
                        help="Path to the pre-trained model weights file (.pt). E.g., ./weights/graphdta3d_davis.pt")

    parser.add_argument("--smiles", type=str, required=True,
                        help="SMILES string of the drug molecule.")

    # Mutually exclusive group for protein input
    protein_group = parser.add_mutually_exclusive_group(required=True)
    protein_group.add_argument("--protein_sequence", type=str,
                               help="Protein sequence (for GraphDTA model).")
    protein_group.add_argument("--protein_pdb_path", type=str,
                               help="Path to the protein PDB file (for GraphDTA3D model).")

    parser.add_argument("--dropout_rate", type=float, default=0.2,
                        help="Dropout rate used during model training (to correctly initialize model). Default for GraphDTA is 0.2, for GraphDTA3D it was 0.3 in your notebook.")
    parser.add_argument("--use_cuda", type=bool, default=True, help="Whether to use CUDA if available.")

    args = parser.parse_args()

    # Validate inputs based on model architecture
    if args.model_architecture == "GraphDTA" and not args.protein_sequence:
        parser.error("--protein_sequence is required for GraphDTA model.")
    if args.model_architecture == "GraphDTA3D" and not args.protein_pdb_path:
        parser.error("--protein_pdb_path is required for GraphDTA3D model.")

    # Adjust dropout if it was different for GraphDTA3D
    if args.model_architecture == "GraphDTA3D" and args.dropout_rate == 0.2: # Check if user didn't specify and it should be 0.3
        print("Note: GraphDTA-3D was trained with dropout 0.3 in your notebook. Adjusting if necessary or ensure correct --dropout_rate is passed.")
        # args.dropout_rate = 0.3 # Or let user specify it correctly.

    predict_affinity(args)

    # --- Example Usage ---
    #
    # For GraphDTA (baseline model):
    # python demo_predict.py \
    #   --model_architecture GraphDTA \
    #   --model_weights_path ./weights/graphdta_davis_baseline_best_model.pt \
    #   --smiles "CC1=C2C=C(C=CC2=NN1)C3=CC(=CN=C3)OCC(CC4=CC=CC=C4)N" \
    #   --protein_sequence "MKKFFDSRREQGGSGLGSGSSGGGGSTSGLGSGYIGRVFGIGRQQVINTISRLKLEDLRQFETSLQEVRNYVLDTVQHLHXAFLLSMTXTXLDGFSEHTCGGVYTFTDTLDDQIVTGLAWDPNGLSDVPLLLRFRQTRAPLGPQRPREPHSLFTFDKYVITPATAPERSPGAFTLDGNPLLSYHVAVTTVTXGFTRWTDRRFPLGGGAGGGGSAGGKHGGGSVGGGLVFGALLVLLALAGLAVYLWKRKCNFWWLGGSGGGSGGGSGGGSGGSTTTTLSPGASL" \
    #   --dropout_rate 0.2
    #
    # For GraphDTA-3D model (using a dummy PDB, replace with a real one):
    # (First, create a dummy PDB file named 'sample_protein.pdb' in the current directory for the example to run)
    # echo "ATOM      1  CA  ALA A   1      10.000  10.000  10.000" > sample_protein.pdb
    # echo "ATOM      2  CA  GLY A   2      15.000  10.000  10.000" >> sample_protein.pdb
    #
    # python demo_predict.py \
    #   --model_architecture GraphDTA3D \
    #   --model_weights_path ./weights/graphdta3d_davis.pt \
    #   --smiles "CC1=C2C=C(C=CC2=NN1)C3=CC(=CN=C3)OCC(CC4=CC=CC=C4)N" \
    #   --protein_pdb_path ./sample_protein.pdb \
    #   --dropout_rate 0.3
    #
    # Remember to replace paths and inputs with your actual data.
    # You'll need to have your trained model weights (e.g., graphdta3d_davis.pt) in the specified path.