{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BcY_qHY6a3Ch",
        "outputId": "f9bd961d-6880-46f4-fba7-f732b2260f0d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install PyTorch Geometric and dependencies for PyTorch 2.0.1 + CUDA 11.8\n",
        "!pip install torch==2.0.1+cu118 torchvision==0.15.2+cu118 torchaudio==2.0.2 --index-url https://download.pytorch.org/whl/cu118\n",
        "!pip install torch-geometric torch-scatter torch-sparse torch-cluster torch-spline-conv -f https://data.pyg.org/whl/torch-2.0.1+cu118.html\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "agSwilGMbJbC",
        "outputId": "784f93b2-6af0-46e3-c9aa-fd2aa3f54109"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://download.pytorch.org/whl/cu118\n",
            "Collecting torch==2.0.1+cu118\n",
            "  Downloading https://download.pytorch.org/whl/cu118/torch-2.0.1%2Bcu118-cp311-cp311-linux_x86_64.whl (2267.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 GB\u001b[0m \u001b[31m987.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torchvision==0.15.2+cu118\n",
            "  Downloading https://download.pytorch.org/whl/cu118/torchvision-0.15.2%2Bcu118-cp311-cp311-linux_x86_64.whl (6.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.1/6.1 MB\u001b[0m \u001b[31m124.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torchaudio==2.0.2\n",
            "  Downloading https://download.pytorch.org/whl/cu118/torchaudio-2.0.2%2Bcu118-cp311-cp311-linux_x86_64.whl (4.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m117.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.0.1+cu118) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from torch==2.0.1+cu118) (4.13.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch==2.0.1+cu118) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.0.1+cu118) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.0.1+cu118) (3.1.6)\n",
            "Collecting triton==2.0.0 (from torch==2.0.1+cu118)\n",
            "  Downloading https://download.pytorch.org/whl/triton-2.0.0-1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (63.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.3/63.3 MB\u001b[0m \u001b[31m39.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision==0.15.2+cu118) (2.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torchvision==0.15.2+cu118) (2.32.3)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision==0.15.2+cu118) (11.2.1)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.11/dist-packages (from triton==2.0.0->torch==2.0.1+cu118) (3.31.6)\n",
            "Collecting lit (from triton==2.0.0->torch==2.0.1+cu118)\n",
            "  Downloading https://download.pytorch.org/whl/lit-15.0.7.tar.gz (132 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.3/132.3 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.0.1+cu118) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torchvision==0.15.2+cu118) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torchvision==0.15.2+cu118) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->torchvision==0.15.2+cu118) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torchvision==0.15.2+cu118) (2025.4.26)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->torch==2.0.1+cu118) (1.3.0)\n",
            "Building wheels for collected packages: lit\n",
            "  Building wheel for lit (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for lit: filename=lit-15.0.7-py3-none-any.whl size=89990 sha256=63cd5094eb4c51dd752b4537c78644f1fff2069c5ccfa3f094c5dbcca40cbbfd\n",
            "  Stored in directory: /root/.cache/pip/wheels/fc/5d/45/34fe9945d5e45e261134e72284395be36c2d4828af38e2b0fe\n",
            "Successfully built lit\n",
            "Installing collected packages: lit, triton, torch, torchvision, torchaudio\n",
            "  Attempting uninstall: triton\n",
            "    Found existing installation: triton 3.2.0\n",
            "    Uninstalling triton-3.2.0:\n",
            "      Successfully uninstalled triton-3.2.0\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.6.0+cu124\n",
            "    Uninstalling torch-2.6.0+cu124:\n",
            "      Successfully uninstalled torch-2.6.0+cu124\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.21.0+cu124\n",
            "    Uninstalling torchvision-0.21.0+cu124:\n",
            "      Successfully uninstalled torchvision-0.21.0+cu124\n",
            "  Attempting uninstall: torchaudio\n",
            "    Found existing installation: torchaudio 2.6.0+cu124\n",
            "    Uninstalling torchaudio-2.6.0+cu124:\n",
            "      Successfully uninstalled torchaudio-2.6.0+cu124\n",
            "Successfully installed lit-15.0.7 torch-2.0.1+cu118 torchaudio-2.0.2+cu118 torchvision-0.15.2+cu118 triton-2.0.0\n",
            "Looking in links: https://data.pyg.org/whl/torch-2.0.1+cu118.html\n",
            "Collecting torch-geometric\n",
            "  Downloading torch_geometric-2.6.1-py3-none-any.whl.metadata (63 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.1/63.1 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torch-scatter\n",
            "  Downloading https://data.pyg.org/whl/torch-2.0.0%2Bcu118/torch_scatter-2.1.2%2Bpt20cu118-cp311-cp311-linux_x86_64.whl (10.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m100.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torch-sparse\n",
            "  Downloading https://data.pyg.org/whl/torch-2.0.0%2Bcu118/torch_sparse-0.6.18%2Bpt20cu118-cp311-cp311-linux_x86_64.whl (4.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m87.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torch-cluster\n",
            "  Downloading https://data.pyg.org/whl/torch-2.0.0%2Bcu118/torch_cluster-1.6.3%2Bpt20cu118-cp311-cp311-linux_x86_64.whl (3.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m74.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torch-spline-conv\n",
            "  Downloading https://data.pyg.org/whl/torch-2.0.0%2Bcu118/torch_spline_conv-1.2.2%2Bpt20cu118-cp311-cp311-linux_x86_64.whl (886 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m886.5/886.5 kB\u001b[0m \u001b[31m63.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.11.15)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (2025.3.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.1.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (2.0.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (5.9.5)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.2.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (4.67.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from torch-sparse) (1.15.3)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.20.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch-geometric) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (2025.4.26)\n",
            "Downloading torch_geometric-2.6.1-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m64.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torch-spline-conv, torch-scatter, torch-sparse, torch-cluster, torch-geometric\n",
            "Successfully installed torch-cluster-1.6.3+pt20cu118 torch-geometric-2.6.1 torch-scatter-2.1.2+pt20cu118 torch-sparse-0.6.18+pt20cu118 torch-spline-conv-1.2.2+pt20cu118\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Setting up paths\n",
        "\n",
        "import os\n",
        "import torch\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from torch_geometric.data import Data\n",
        "\n",
        "#Check device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using Device: {device}\")\n",
        "\n",
        "#Define paths\n",
        "project_root = \"/content/drive/MyDrive/Rebuilding_and_Modifying_GraphDTA\"\n",
        "data_path = f\"{project_root}/data\"\n",
        "\n",
        "affinity_df = pd.read_csv(f\"{data_path}/kiba_affinity_df.csv\")\n",
        "print(affinity_df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GKRi4O4mbMmS",
        "outputId": "795799ea-f4e2-44fd-d0b0-a83683169c61"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "A module that was compiled using NumPy 1.x cannot be run in\n",
            "NumPy 2.0.2 as it may crash. To support both 1.x and 2.x\n",
            "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
            "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
            "\n",
            "If you are a user of the module, the easiest solution will be to\n",
            "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
            "We expect that some modules will need time to support NumPy 2.\n",
            "\n",
            "Traceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
            "  File \"<frozen runpy>\", line 88, in _run_code\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/colab_kernel_launcher.py\", line 37, in <module>\n",
            "    ColabKernelApp.launch_instance()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n",
            "    app.start()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelapp.py\", line 712, in start\n",
            "    self.io_loop.start()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/tornado/platform/asyncio.py\", line 205, in start\n",
            "    self.asyncio_loop.run_forever()\n",
            "  File \"/usr/lib/python3.11/asyncio/base_events.py\", line 608, in run_forever\n",
            "    self._run_once()\n",
            "  File \"/usr/lib/python3.11/asyncio/base_events.py\", line 1936, in _run_once\n",
            "    handle._run()\n",
            "  File \"/usr/lib/python3.11/asyncio/events.py\", line 84, in _run\n",
            "    self._context.run(self._callback, *self._args)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 510, in dispatch_queue\n",
            "    await self.process_one()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 499, in process_one\n",
            "    await dispatch(*args)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 406, in dispatch_shell\n",
            "    await result\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 730, in execute_request\n",
            "    reply_content = await reply_content\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/ipkernel.py\", line 383, in do_execute\n",
            "    res = shell.run_cell(\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/zmqshell.py\", line 528, in run_cell\n",
            "    return super().run_cell(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n",
            "    result = self._run_cell(\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n",
            "    return runner(coro)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n",
            "    coro.send(None)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n",
            "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n",
            "    if (await self.run_code(code, result,  async_=asy)):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"<ipython-input-3-cafebd010989>\", line 7, in <cell line: 0>\n",
            "    from torch_geometric.data import Data\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch_geometric/__init__.py\", line 21, in <module>\n",
            "    import torch_geometric.datasets\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch_geometric/datasets/__init__.py\", line 18, in <module>\n",
            "    from .qm9 import QM9\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch_geometric/datasets/qm9.py\", line 22, in <module>\n",
            "    conversion = torch.tensor([\n",
            "/usr/local/lib/python3.11/dist-packages/torch_geometric/datasets/qm9.py:22: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:84.)\n",
            "  conversion = torch.tensor([\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using Device: cuda\n",
            "   Drug_Index  Protein_Index  \\\n",
            "0           0              0   \n",
            "1           0              1   \n",
            "2           0              2   \n",
            "3           0              3   \n",
            "4           0              4   \n",
            "\n",
            "                                              SMILES  \\\n",
            "0  COC1=C(C=C2C(=C1)N=CN2C3=CC(=C(S3)C#N)OCC4=CC=...   \n",
            "1  COC1=C(C=C2C(=C1)N=CN2C3=CC(=C(S3)C#N)OCC4=CC=...   \n",
            "2  COC1=C(C=C2C(=C1)N=CN2C3=CC(=C(S3)C#N)OCC4=CC=...   \n",
            "3  COC1=C(C=C2C(=C1)N=CN2C3=CC(=C(S3)C#N)OCC4=CC=...   \n",
            "4  COC1=C(C=C2C(=C1)N=CN2C3=CC(=C(S3)C#N)OCC4=CC=...   \n",
            "\n",
            "                                            Sequence  KIBA_Score  \n",
            "0  MSAAVTAGKLARAPADPGKAGVPGVAAPGAPAAAPPAKEIPEVLVD...         0.0  \n",
            "1  MRPSGTAGAALLALLAALCPASRALEEKKVCQGTSNKLTQLGTFED...         1.1  \n",
            "2  MELAALCRWGLLLALLPPGAASTQVCTGTDMKLRLPASPETHLDML...         1.1  \n",
            "3  MSGGGPSGGGPGGSGRARTSSFAEPGGGGGGGGGGPGGSASGPGGT...         1.3  \n",
            "4  MSGRPRTTSFAESCKPVQQPSAFGSMKVSRDKDGSKVTTVVATPGQ...         1.3  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Load drug graph tensors\n",
        "import torch\n",
        "\n",
        "\n",
        "drug_graphs = torch.load(f\"{data_path}/kiba_drug_graphs.pt\", weights_only=False)\n",
        "\n",
        "\n",
        "#Load protein sequences\n",
        "\n",
        "protein_seq = torch.load(f\"{data_path}/kiba_protein_seqs.pt\", weights_only=False)"
      ],
      "metadata": {
        "id": "tSumPmeWbbxy"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "\n",
        "class DTADataset(Dataset):\n",
        "    def __init__(self, affinity_df, drug_graphs, protein_sequences):\n",
        "        self.data = []\n",
        "        for _, row in affinity_df.iterrows():\n",
        "            d_idx = row[\"Drug_Index\"]\n",
        "            p_idx = row[\"Protein_Index\"]\n",
        "            y = row[\"KIBA_Score\"]\n",
        "\n",
        "            if d_idx in drug_graphs and p_idx in protein_sequences:\n",
        "                drug_graph = drug_graphs[d_idx]\n",
        "                protein_seq = protein_sequences[p_idx]\n",
        "                self.data.append((drug_graph, protein_seq, torch.tensor([y], dtype=torch.float)))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx]"
      ],
      "metadata": {
        "id": "TxOH5kaub-yt"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Build collate function\n",
        "\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch_geometric.data import Batch\n",
        "\n",
        "def collate_fn(batch):\n",
        "  drug_graphs, protein_seqs, labels = zip(*batch)\n",
        "  drug_batch = Batch.from_data_list(drug_graphs)\n",
        "  padded_proteins = pad_sequence(protein_seqs, batch_first=True, padding_value = 0)\n",
        "  labels = torch.stack(labels)\n",
        "  return drug_batch, padded_proteins, labels"
      ],
      "metadata": {
        "id": "VYNhlhMacStW"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Create train/val/test and loaders\n",
        "\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "\n",
        "full_dataset = DTADataset(affinity_df, drug_graphs, protein_seq)\n",
        "train_size = int(0.8*len(full_dataset))\n",
        "val_size = int(0.1*len(full_dataset))\n",
        "test_size = len(full_dataset) - train_size - val_size\n",
        "\n",
        "train_set, val_set, test_set = random_split(full_dataset, [train_size, val_size, test_size])\n",
        "\n",
        "train_loader = DataLoader(train_set, batch_size = 512, shuffle=True, collate_fn=collate_fn)\n",
        "val_loader = DataLoader(val_set, batch_size=512, collate_fn=collate_fn)\n",
        "test_loader = DataLoader(test_set, batch_size=512, collate_fn=collate_fn)\n"
      ],
      "metadata": {
        "id": "bKmNLEtRcXbA"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#GraphDTA model  (GCN + 1D CNN)\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import GCNConv, global_max_pool\n",
        "\n",
        "class GraphDTA(nn.Module):\n",
        "  def __init__(self, num_atom_features=83, protein_vocab_size=21, protein_embed_dim=128, out_channel=128, kernel_size=8, drug_output_dim=128, protein_output_dim=128, fc_hidden_dims=[1024, 512], dropout_rate=0.2, output_dim=1):\n",
        "\n",
        "    super(GraphDTA, self).__init__()\n",
        "\n",
        "    #Drug encoder\n",
        "    self.gcn1 = GCNConv(num_atom_features, num_atom_features*2)\n",
        "    self.gcn2 = GCNConv(num_atom_features*2, num_atom_features*4)\n",
        "    self.gcn3 = GCNConv(num_atom_features*4, num_atom_features*8)\n",
        "    self.drug_fc1 = nn.Linear(num_atom_features*8, 1024)\n",
        "    self.drug_fc2 = nn.Linear(1024, 512)\n",
        "    self.drug_fc3 = nn.Linear(512, drug_output_dim)\n",
        "    self.relu = nn.ReLU()\n",
        "    self.dropout = nn.Dropout(p=dropout_rate)\n",
        "\n",
        "    #Protein encoder\n",
        "    self.protein_embedding = nn.Embedding(protein_vocab_size, protein_embed_dim, padding_idx=0)\n",
        "    self.conv1_protein = nn.Conv1d(protein_embed_dim, out_channels=out_channel, kernel_size=kernel_size)\n",
        "    self.conv2_protein = nn.Conv1d(out_channel, protein_output_dim, kernel_size=kernel_size)\n",
        "    self.protein_pool = nn.AdaptiveMaxPool1d(1) #output will be [batch_size, protein_output_dim, 1]\n",
        "\n",
        "    #Fusion layers (MLP)\n",
        "    self.fc1_combined = nn.Linear(drug_output_dim+protein_output_dim, fc_hidden_dims[0])\n",
        "    self.dropout1 = nn.Dropout(p=dropout_rate)\n",
        "    self.fc2_combined = nn.Linear(fc_hidden_dims[0], fc_hidden_dims[1])\n",
        "    self.dropout2 = nn.Dropout(p=dropout_rate)\n",
        "    self.out = nn.Linear(fc_hidden_dims[1], output_dim)\n",
        "\n",
        "    self.relu = nn.ReLU()\n",
        "    self.dropout = nn.Dropout(dropout_rate) #General dropout rate\n",
        "\n",
        "  def forward(self, drug_data, protein_seq):\n",
        "    #Drug graph forward\n",
        "    x_drug, edge_index_drug, batch_drug = drug_data.x, drug_data.edge_index, drug_data.batch\n",
        "    x_drug = self.relu(self.gcn1(x_drug, edge_index_drug))\n",
        "    x_drug = self.relu(self.gcn2(x_drug, edge_index_drug))\n",
        "    x_drug = self.relu(self.gcn3(x_drug, edge_index_drug))\n",
        "    drug_emb_pooled = global_max_pool(x_drug, batch_drug)\n",
        "\n",
        "    drug_emb = self.relu(self.drug_fc1(drug_emb_pooled))\n",
        "    drug_emb = self.dropout(drug_emb)\n",
        "    drug_emb = self.relu(self.drug_fc2(drug_emb))\n",
        "    drug_emb = self.dropout(drug_emb)\n",
        "    drug_emb = self.relu(self.drug_fc3(drug_emb))\n",
        "    drug_emb = self.dropout(drug_emb)\n",
        "\n",
        "    #Protein sequence forward\n",
        "    seq_emb = self.protein_embedding(protein_seq)\n",
        "    seq_emb = seq_emb.permute(0, 2, 1)\n",
        "    seq_conv = self.relu(self.conv1_protein(seq_emb))\n",
        "    seq_conv = self.relu(self.conv2_protein(seq_conv))\n",
        "    protein_emb = self.protein_pool(seq_conv).squeeze(-1)\n",
        "\n",
        "    #Fusion\n",
        "    combined_emb = torch.cat((drug_emb, protein_emb), dim = 1)\n",
        "    x_combined = self.relu(self.fc1_combined(combined_emb))\n",
        "    x_combined = self.dropout1(x_combined)\n",
        "    x_combined = self.relu(self.fc2_combined(x_combined))\n",
        "    x_combined = self.dropout2(x_combined)\n",
        "\n",
        "    output = self.out(x_combined)\n",
        "    if output.shape[-1] == 1:\n",
        "      output = output.squeeze(-1)\n",
        "    return output\n"
      ],
      "metadata": {
        "id": "_ITlWdiaccz3"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Define training functions\n",
        "def train(model, device, loader, optimizer, criterion):\n",
        "  model.train()\n",
        "  total_loss = 0\n",
        "  for drug_graph, protein_seq, affinity in loader:\n",
        "    drug_graph = drug_graph.to(device)\n",
        "    protein_seq = protein_seq.to(device)\n",
        "    affinity = affinity.to(device).squeeze()\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    output = model(drug_graph, protein_seq)\n",
        "    loss = criterion(output, affinity)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    total_loss += loss.item()\n",
        "    return total_loss / len(loader)\n"
      ],
      "metadata": {
        "id": "993N7DJ2cjXk"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Define RMSE (PyTorch)\n",
        "def rmse_torch(pred, true):\n",
        "    return torch.sqrt(torch.mean((pred - true) ** 2)).item()\n",
        "\n",
        "#Define CI(PyTorch)\n",
        "def concordance_index_torch(y_true, y_pred):\n",
        "    \"\"\"Returns CI (pure PyTorch)\"\"\"\n",
        "    concordant = 0.0\n",
        "    permissible = 0.0\n",
        "    n = len(y_true)\n",
        "\n",
        "    for i in range(n):\n",
        "        for j in range(i + 1, n):\n",
        "            if y_true[i] != y_true[j]:\n",
        "                permissible += 1\n",
        "                if (y_pred[i] - y_pred[j]) * (y_true[i] - y_true[j]) > 0:\n",
        "                    concordant += 1\n",
        "    return concordant / permissible if permissible != 0 else 0.0\n",
        "\n",
        "\n",
        "#Define evaluation function\n",
        "def evaluate(model, device, val_loader, loader):\n",
        "  model.eval()\n",
        "  y_pred, y_true = [], []\n",
        "  total_val_loss = 0\n",
        "  num_samples = 0\n",
        "  with torch.no_grad():\n",
        "    for drug_graph, protein_seq, affinity in val_loader:\n",
        "      drug_graph = drug_graph.to(device)\n",
        "      protein_seq = protein_seq.to(device)\n",
        "      affinity_label_device = affinity.to(device).squeeze()\n",
        "\n",
        "      output = model(drug_graph, protein_seq)\n",
        "\n",
        "      loss = criterion(output, affinity_label_device)\n",
        "      total_val_loss += loss.item() * affinity_label_device.size(0)\n",
        "      num_samples += affinity_label_device.size(0)\n",
        "\n",
        "      y_pred.extend(output.detach().cpu().tolist())\n",
        "      y_true.extend(affinity.squeeze().tolist())\n",
        "\n",
        "  y_pred_tensor = torch.tensor(y_pred)\n",
        "  y_true_tensor = torch.tensor(y_true)\n",
        "\n",
        "  avg_val_loss = total_val_loss / num_samples\n",
        "\n",
        "  metrics =  {\n",
        "        'val_loss': avg_val_loss,\n",
        "        'rmse': rmse_torch(y_pred_tensor, y_true_tensor),\n",
        "        'ci': concordance_index_torch(y_true, y_pred)\n",
        "    }\n",
        "  return metrics"
      ],
      "metadata": {
        "id": "7OOW44NCclSm"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Initialize model, optimizer and loader\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = GraphDTA(num_atom_features=83, protein_vocab_size=21, protein_embed_dim=128, out_channel=128, kernel_size=8, drug_output_dim=128, protein_output_dim=128, fc_hidden_dims=[1024, 512], dropout_rate=0.2, output_dim=1).to(device)\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0005)"
      ],
      "metadata": {
        "id": "DFDgWqvecl8q"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Train the model\n",
        "\n",
        "for epoch in range(1, 501):\n",
        "  train_loss = train(model, device, train_loader, optimizer, criterion)\n",
        "  val_metrics = evaluate(model, device, val_loader, criterion)\n",
        "  print(f\"Epoch {epoch:02d} | Loss: {train_loss: .4f} | Val RMSE: {val_metrics['rmse']:.4f} | CI: {val_metrics['ci']:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5-1_UjS2co5y",
        "outputId": "32b923ab-5b0b-48ea-cbcb-f3c08530aaa1"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 01 | Loss:  0.7466 | Val RMSE: 11.2222 | CI: 0.5366\n",
            "Epoch 02 | Loss:  0.6810 | Val RMSE: 10.5258 | CI: 0.5418\n",
            "Epoch 03 | Loss:  0.5982 | Val RMSE: 9.4049 | CI: 0.5473\n",
            "Epoch 04 | Loss:  0.4763 | Val RMSE: 7.5714 | CI: 0.5530\n",
            "Epoch 05 | Loss:  0.3103 | Val RMSE: 4.6289 | CI: 0.5565\n",
            "Epoch 06 | Loss:  0.1148 | Val RMSE: 0.9336 | CI: 0.5593\n",
            "Epoch 07 | Loss:  0.0066 | Val RMSE: 6.2731 | CI: 0.5573\n",
            "Epoch 08 | Loss:  0.2176 | Val RMSE: 5.3984 | CI: 0.5601\n",
            "Epoch 09 | Loss:  0.1624 | Val RMSE: 2.7818 | CI: 0.5605\n",
            "Epoch 10 | Loss:  0.0464 | Val RMSE: 0.8669 | CI: 0.5569\n",
            "Epoch 11 | Loss:  0.0056 | Val RMSE: 2.0667 | CI: 0.5520\n",
            "Epoch 12 | Loss:  0.0254 | Val RMSE: 3.1628 | CI: 0.5467\n",
            "Epoch 13 | Loss:  0.0531 | Val RMSE: 3.7169 | CI: 0.5437\n",
            "Epoch 14 | Loss:  0.0761 | Val RMSE: 3.8147 | CI: 0.5408\n",
            "Epoch 15 | Loss:  0.0777 | Val RMSE: 3.5308 | CI: 0.5373\n",
            "Epoch 16 | Loss:  0.0726 | Val RMSE: 2.8977 | CI: 0.5347\n",
            "Epoch 17 | Loss:  0.0481 | Val RMSE: 1.9814 | CI: 0.5318\n",
            "Epoch 18 | Loss:  0.0221 | Val RMSE: 1.0427 | CI: 0.5283\n",
            "Epoch 19 | Loss:  0.0085 | Val RMSE: 1.2596 | CI: 0.5248\n",
            "Epoch 20 | Loss:  0.0105 | Val RMSE: 2.2014 | CI: 0.5240\n",
            "Epoch 21 | Loss:  0.0266 | Val RMSE: 2.7082 | CI: 0.5236\n",
            "Epoch 22 | Loss:  0.0416 | Val RMSE: 2.5105 | CI: 0.5246\n",
            "Epoch 23 | Loss:  0.0372 | Val RMSE: 1.8106 | CI: 0.5266\n",
            "Epoch 24 | Loss:  0.0189 | Val RMSE: 1.0748 | CI: 0.5307\n",
            "Epoch 25 | Loss:  0.0078 | Val RMSE: 0.9697 | CI: 0.5338\n",
            "Epoch 26 | Loss:  0.0071 | Val RMSE: 1.4189 | CI: 0.5372\n",
            "Epoch 27 | Loss:  0.0122 | Val RMSE: 1.8028 | CI: 0.5388\n",
            "Epoch 28 | Loss:  0.0195 | Val RMSE: 1.9586 | CI: 0.5405\n",
            "Epoch 29 | Loss:  0.0217 | Val RMSE: 1.8828 | CI: 0.5419\n",
            "Epoch 30 | Loss:  0.0200 | Val RMSE: 1.6094 | CI: 0.5431\n",
            "Epoch 31 | Loss:  0.0151 | Val RMSE: 1.2189 | CI: 0.5443\n",
            "Epoch 32 | Loss:  0.0084 | Val RMSE: 0.9071 | CI: 0.5452\n",
            "Epoch 33 | Loss:  0.0058 | Val RMSE: 0.9627 | CI: 0.5465\n",
            "Epoch 34 | Loss:  0.0070 | Val RMSE: 1.2582 | CI: 0.5472\n",
            "Epoch 35 | Loss:  0.0111 | Val RMSE: 1.4507 | CI: 0.5480\n",
            "Epoch 36 | Loss:  0.0126 | Val RMSE: 1.4253 | CI: 0.5495\n",
            "Epoch 37 | Loss:  0.0140 | Val RMSE: 1.1909 | CI: 0.5502\n",
            "Epoch 38 | Loss:  0.0091 | Val RMSE: 0.9389 | CI: 0.5523\n",
            "Epoch 39 | Loss:  0.0067 | Val RMSE: 0.8786 | CI: 0.5538\n",
            "Epoch 40 | Loss:  0.0057 | Val RMSE: 1.0296 | CI: 0.5555\n",
            "Epoch 41 | Loss:  0.0069 | Val RMSE: 1.2016 | CI: 0.5572\n",
            "Epoch 42 | Loss:  0.0094 | Val RMSE: 1.2778 | CI: 0.5588\n",
            "Epoch 43 | Loss:  0.0102 | Val RMSE: 1.2286 | CI: 0.5603\n",
            "Epoch 44 | Loss:  0.0100 | Val RMSE: 1.0795 | CI: 0.5614\n",
            "Epoch 45 | Loss:  0.0072 | Val RMSE: 0.9186 | CI: 0.5630\n",
            "Epoch 46 | Loss:  0.0065 | Val RMSE: 0.8565 | CI: 0.5645\n",
            "Epoch 47 | Loss:  0.0054 | Val RMSE: 0.9243 | CI: 0.5656\n",
            "Epoch 48 | Loss:  0.0067 | Val RMSE: 1.0208 | CI: 0.5670\n",
            "Epoch 49 | Loss:  0.0074 | Val RMSE: 1.0429 | CI: 0.5685\n",
            "Epoch 50 | Loss:  0.0076 | Val RMSE: 0.9797 | CI: 0.5699\n",
            "Epoch 51 | Loss:  0.0074 | Val RMSE: 0.8876 | CI: 0.5712\n",
            "Epoch 52 | Loss:  0.0068 | Val RMSE: 0.8479 | CI: 0.5732\n",
            "Epoch 53 | Loss:  0.0055 | Val RMSE: 0.8913 | CI: 0.5752\n",
            "Epoch 54 | Loss:  0.0069 | Val RMSE: 0.9594 | CI: 0.5767\n",
            "Epoch 55 | Loss:  0.0062 | Val RMSE: 0.9977 | CI: 0.5782\n",
            "Epoch 56 | Loss:  0.0071 | Val RMSE: 0.9719 | CI: 0.5796\n",
            "Epoch 57 | Loss:  0.0061 | Val RMSE: 0.9131 | CI: 0.5811\n",
            "Epoch 58 | Loss:  0.0062 | Val RMSE: 0.8545 | CI: 0.5823\n",
            "Epoch 59 | Loss:  0.0059 | Val RMSE: 0.8417 | CI: 0.5838\n",
            "Epoch 60 | Loss:  0.0048 | Val RMSE: 0.8779 | CI: 0.5854\n",
            "Epoch 61 | Loss:  0.0052 | Val RMSE: 0.9017 | CI: 0.5866\n",
            "Epoch 62 | Loss:  0.0065 | Val RMSE: 0.8913 | CI: 0.5880\n",
            "Epoch 63 | Loss:  0.0053 | Val RMSE: 0.8566 | CI: 0.5894\n",
            "Epoch 64 | Loss:  0.0060 | Val RMSE: 0.8344 | CI: 0.5914\n",
            "Epoch 65 | Loss:  0.0068 | Val RMSE: 0.8398 | CI: 0.5937\n",
            "Epoch 66 | Loss:  0.0054 | Val RMSE: 0.8626 | CI: 0.5953\n",
            "Epoch 67 | Loss:  0.0057 | Val RMSE: 0.8819 | CI: 0.5972\n",
            "Epoch 68 | Loss:  0.0070 | Val RMSE: 0.8751 | CI: 0.5991\n",
            "Epoch 69 | Loss:  0.0052 | Val RMSE: 0.8543 | CI: 0.6004\n",
            "Epoch 70 | Loss:  0.0054 | Val RMSE: 0.8320 | CI: 0.6020\n",
            "Epoch 71 | Loss:  0.0062 | Val RMSE: 0.8272 | CI: 0.6035\n",
            "Epoch 72 | Loss:  0.0056 | Val RMSE: 0.8358 | CI: 0.6050\n",
            "Epoch 73 | Loss:  0.0049 | Val RMSE: 0.8434 | CI: 0.6064\n",
            "Epoch 74 | Loss:  0.0055 | Val RMSE: 0.8396 | CI: 0.6078\n",
            "Epoch 75 | Loss:  0.0065 | Val RMSE: 0.8316 | CI: 0.6089\n",
            "Epoch 76 | Loss:  0.0056 | Val RMSE: 0.8224 | CI: 0.6102\n",
            "Epoch 77 | Loss:  0.0052 | Val RMSE: 0.8230 | CI: 0.6116\n",
            "Epoch 78 | Loss:  0.0053 | Val RMSE: 0.8314 | CI: 0.6128\n",
            "Epoch 79 | Loss:  0.0055 | Val RMSE: 0.8437 | CI: 0.6140\n",
            "Epoch 80 | Loss:  0.0053 | Val RMSE: 0.8510 | CI: 0.6151\n",
            "Epoch 81 | Loss:  0.0047 | Val RMSE: 0.8447 | CI: 0.6163\n",
            "Epoch 82 | Loss:  0.0050 | Val RMSE: 0.8333 | CI: 0.6173\n",
            "Epoch 83 | Loss:  0.0053 | Val RMSE: 0.8222 | CI: 0.6184\n",
            "Epoch 84 | Loss:  0.0064 | Val RMSE: 0.8156 | CI: 0.6195\n",
            "Epoch 85 | Loss:  0.0058 | Val RMSE: 0.8170 | CI: 0.6204\n",
            "Epoch 86 | Loss:  0.0052 | Val RMSE: 0.8241 | CI: 0.6212\n",
            "Epoch 87 | Loss:  0.0050 | Val RMSE: 0.8211 | CI: 0.6223\n",
            "Epoch 88 | Loss:  0.0062 | Val RMSE: 0.8140 | CI: 0.6233\n",
            "Epoch 89 | Loss:  0.0053 | Val RMSE: 0.8136 | CI: 0.6242\n",
            "Epoch 90 | Loss:  0.0053 | Val RMSE: 0.8216 | CI: 0.6253\n",
            "Epoch 91 | Loss:  0.0046 | Val RMSE: 0.8318 | CI: 0.6263\n",
            "Epoch 92 | Loss:  0.0053 | Val RMSE: 0.8388 | CI: 0.6277\n",
            "Epoch 93 | Loss:  0.0052 | Val RMSE: 0.8314 | CI: 0.6288\n",
            "Epoch 94 | Loss:  0.0050 | Val RMSE: 0.8198 | CI: 0.6299\n",
            "Epoch 95 | Loss:  0.0057 | Val RMSE: 0.8109 | CI: 0.6307\n",
            "Epoch 96 | Loss:  0.0064 | Val RMSE: 0.8086 | CI: 0.6314\n",
            "Epoch 97 | Loss:  0.0056 | Val RMSE: 0.8101 | CI: 0.6323\n",
            "Epoch 98 | Loss:  0.0049 | Val RMSE: 0.8126 | CI: 0.6333\n",
            "Epoch 99 | Loss:  0.0052 | Val RMSE: 0.8106 | CI: 0.6342\n",
            "Epoch 100 | Loss:  0.0054 | Val RMSE: 0.8083 | CI: 0.6353\n",
            "Epoch 101 | Loss:  0.0059 | Val RMSE: 0.8068 | CI: 0.6362\n",
            "Epoch 102 | Loss:  0.0046 | Val RMSE: 0.8063 | CI: 0.6372\n",
            "Epoch 103 | Loss:  0.0050 | Val RMSE: 0.8089 | CI: 0.6381\n",
            "Epoch 104 | Loss:  0.0044 | Val RMSE: 0.8123 | CI: 0.6389\n",
            "Epoch 105 | Loss:  0.0050 | Val RMSE: 0.8125 | CI: 0.6395\n",
            "Epoch 106 | Loss:  0.0047 | Val RMSE: 0.8086 | CI: 0.6403\n",
            "Epoch 107 | Loss:  0.0046 | Val RMSE: 0.8059 | CI: 0.6412\n",
            "Epoch 108 | Loss:  0.0050 | Val RMSE: 0.8045 | CI: 0.6419\n",
            "Epoch 109 | Loss:  0.0057 | Val RMSE: 0.8036 | CI: 0.6426\n",
            "Epoch 110 | Loss:  0.0051 | Val RMSE: 0.8035 | CI: 0.6431\n",
            "Epoch 111 | Loss:  0.0051 | Val RMSE: 0.8041 | CI: 0.6435\n",
            "Epoch 112 | Loss:  0.0058 | Val RMSE: 0.8044 | CI: 0.6439\n",
            "Epoch 113 | Loss:  0.0052 | Val RMSE: 0.8050 | CI: 0.6445\n",
            "Epoch 114 | Loss:  0.0049 | Val RMSE: 0.8023 | CI: 0.6451\n",
            "Epoch 115 | Loss:  0.0046 | Val RMSE: 0.8029 | CI: 0.6458\n",
            "Epoch 116 | Loss:  0.0049 | Val RMSE: 0.8117 | CI: 0.6464\n",
            "Epoch 117 | Loss:  0.0050 | Val RMSE: 0.8203 | CI: 0.6470\n",
            "Epoch 118 | Loss:  0.0063 | Val RMSE: 0.8172 | CI: 0.6475\n",
            "Epoch 119 | Loss:  0.0048 | Val RMSE: 0.8075 | CI: 0.6481\n",
            "Epoch 120 | Loss:  0.0049 | Val RMSE: 0.8020 | CI: 0.6486\n",
            "Epoch 121 | Loss:  0.0048 | Val RMSE: 0.8005 | CI: 0.6493\n",
            "Epoch 122 | Loss:  0.0049 | Val RMSE: 0.8001 | CI: 0.6498\n",
            "Epoch 123 | Loss:  0.0042 | Val RMSE: 0.8006 | CI: 0.6505\n",
            "Epoch 124 | Loss:  0.0047 | Val RMSE: 0.8015 | CI: 0.6509\n",
            "Epoch 125 | Loss:  0.0057 | Val RMSE: 0.8021 | CI: 0.6514\n",
            "Epoch 126 | Loss:  0.0047 | Val RMSE: 0.8013 | CI: 0.6518\n",
            "Epoch 127 | Loss:  0.0052 | Val RMSE: 0.7998 | CI: 0.6524\n",
            "Epoch 128 | Loss:  0.0053 | Val RMSE: 0.7987 | CI: 0.6528\n",
            "Epoch 129 | Loss:  0.0053 | Val RMSE: 0.7986 | CI: 0.6531\n",
            "Epoch 130 | Loss:  0.0047 | Val RMSE: 0.7983 | CI: 0.6534\n",
            "Epoch 131 | Loss:  0.0045 | Val RMSE: 0.7988 | CI: 0.6537\n",
            "Epoch 132 | Loss:  0.0049 | Val RMSE: 0.7996 | CI: 0.6540\n",
            "Epoch 133 | Loss:  0.0056 | Val RMSE: 0.8003 | CI: 0.6545\n",
            "Epoch 134 | Loss:  0.0049 | Val RMSE: 0.8033 | CI: 0.6550\n",
            "Epoch 135 | Loss:  0.0049 | Val RMSE: 0.8023 | CI: 0.6555\n",
            "Epoch 136 | Loss:  0.0054 | Val RMSE: 0.7987 | CI: 0.6560\n",
            "Epoch 137 | Loss:  0.0053 | Val RMSE: 0.7970 | CI: 0.6566\n",
            "Epoch 138 | Loss:  0.0043 | Val RMSE: 0.7971 | CI: 0.6571\n",
            "Epoch 139 | Loss:  0.0057 | Val RMSE: 0.7975 | CI: 0.6575\n",
            "Epoch 140 | Loss:  0.0052 | Val RMSE: 0.7968 | CI: 0.6579\n",
            "Epoch 141 | Loss:  0.0046 | Val RMSE: 0.7961 | CI: 0.6585\n",
            "Epoch 142 | Loss:  0.0053 | Val RMSE: 0.7959 | CI: 0.6589\n",
            "Epoch 143 | Loss:  0.0045 | Val RMSE: 0.7957 | CI: 0.6594\n",
            "Epoch 144 | Loss:  0.0051 | Val RMSE: 0.7956 | CI: 0.6598\n",
            "Epoch 145 | Loss:  0.0051 | Val RMSE: 0.7953 | CI: 0.6604\n",
            "Epoch 146 | Loss:  0.0047 | Val RMSE: 0.7958 | CI: 0.6611\n",
            "Epoch 147 | Loss:  0.0042 | Val RMSE: 0.7970 | CI: 0.6616\n",
            "Epoch 148 | Loss:  0.0057 | Val RMSE: 0.7975 | CI: 0.6619\n",
            "Epoch 149 | Loss:  0.0050 | Val RMSE: 0.7957 | CI: 0.6622\n",
            "Epoch 150 | Loss:  0.0049 | Val RMSE: 0.7950 | CI: 0.6626\n",
            "Epoch 151 | Loss:  0.0044 | Val RMSE: 0.7950 | CI: 0.6630\n",
            "Epoch 152 | Loss:  0.0044 | Val RMSE: 0.7958 | CI: 0.6634\n",
            "Epoch 153 | Loss:  0.0044 | Val RMSE: 0.7962 | CI: 0.6638\n",
            "Epoch 154 | Loss:  0.0049 | Val RMSE: 0.7970 | CI: 0.6642\n",
            "Epoch 155 | Loss:  0.0058 | Val RMSE: 0.7966 | CI: 0.6645\n",
            "Epoch 156 | Loss:  0.0043 | Val RMSE: 0.7973 | CI: 0.6648\n",
            "Epoch 157 | Loss:  0.0048 | Val RMSE: 0.7998 | CI: 0.6651\n",
            "Epoch 158 | Loss:  0.0069 | Val RMSE: 0.7990 | CI: 0.6654\n",
            "Epoch 159 | Loss:  0.0046 | Val RMSE: 0.7978 | CI: 0.6656\n",
            "Epoch 160 | Loss:  0.0055 | Val RMSE: 0.7957 | CI: 0.6658\n",
            "Epoch 161 | Loss:  0.0042 | Val RMSE: 0.7946 | CI: 0.6661\n",
            "Epoch 162 | Loss:  0.0051 | Val RMSE: 0.7943 | CI: 0.6663\n",
            "Epoch 163 | Loss:  0.0044 | Val RMSE: 0.7960 | CI: 0.6667\n",
            "Epoch 164 | Loss:  0.0045 | Val RMSE: 0.7965 | CI: 0.6670\n",
            "Epoch 165 | Loss:  0.0044 | Val RMSE: 0.7985 | CI: 0.6673\n",
            "Epoch 166 | Loss:  0.0052 | Val RMSE: 0.7992 | CI: 0.6677\n",
            "Epoch 167 | Loss:  0.0058 | Val RMSE: 0.7951 | CI: 0.6681\n",
            "Epoch 168 | Loss:  0.0052 | Val RMSE: 0.7923 | CI: 0.6684\n",
            "Epoch 169 | Loss:  0.0054 | Val RMSE: 0.7921 | CI: 0.6687\n",
            "Epoch 170 | Loss:  0.0053 | Val RMSE: 0.7935 | CI: 0.6689\n",
            "Epoch 171 | Loss:  0.0049 | Val RMSE: 0.7929 | CI: 0.6692\n",
            "Epoch 172 | Loss:  0.0050 | Val RMSE: 0.7918 | CI: 0.6693\n",
            "Epoch 173 | Loss:  0.0045 | Val RMSE: 0.7920 | CI: 0.6695\n",
            "Epoch 174 | Loss:  0.0048 | Val RMSE: 0.7944 | CI: 0.6698\n",
            "Epoch 175 | Loss:  0.0055 | Val RMSE: 0.7962 | CI: 0.6701\n",
            "Epoch 176 | Loss:  0.0044 | Val RMSE: 0.7964 | CI: 0.6702\n",
            "Epoch 177 | Loss:  0.0050 | Val RMSE: 0.7963 | CI: 0.6703\n",
            "Epoch 178 | Loss:  0.0048 | Val RMSE: 0.7939 | CI: 0.6703\n",
            "Epoch 179 | Loss:  0.0052 | Val RMSE: 0.7909 | CI: 0.6704\n",
            "Epoch 180 | Loss:  0.0050 | Val RMSE: 0.7927 | CI: 0.6704\n",
            "Epoch 181 | Loss:  0.0056 | Val RMSE: 0.7935 | CI: 0.6706\n",
            "Epoch 182 | Loss:  0.0050 | Val RMSE: 0.7910 | CI: 0.6707\n",
            "Epoch 183 | Loss:  0.0054 | Val RMSE: 0.7907 | CI: 0.6708\n",
            "Epoch 184 | Loss:  0.0048 | Val RMSE: 0.7916 | CI: 0.6708\n",
            "Epoch 185 | Loss:  0.0065 | Val RMSE: 0.7980 | CI: 0.6708\n",
            "Epoch 186 | Loss:  0.0048 | Val RMSE: 0.7987 | CI: 0.6708\n",
            "Epoch 187 | Loss:  0.0049 | Val RMSE: 0.7934 | CI: 0.6708\n",
            "Epoch 188 | Loss:  0.0056 | Val RMSE: 0.7906 | CI: 0.6708\n",
            "Epoch 189 | Loss:  0.0047 | Val RMSE: 0.7907 | CI: 0.6709\n",
            "Epoch 190 | Loss:  0.0043 | Val RMSE: 0.7916 | CI: 0.6710\n",
            "Epoch 191 | Loss:  0.0043 | Val RMSE: 0.7910 | CI: 0.6710\n",
            "Epoch 192 | Loss:  0.0053 | Val RMSE: 0.7905 | CI: 0.6711\n",
            "Epoch 193 | Loss:  0.0049 | Val RMSE: 0.7906 | CI: 0.6712\n",
            "Epoch 194 | Loss:  0.0050 | Val RMSE: 0.7927 | CI: 0.6712\n",
            "Epoch 195 | Loss:  0.0050 | Val RMSE: 0.7952 | CI: 0.6713\n",
            "Epoch 196 | Loss:  0.0053 | Val RMSE: 0.7940 | CI: 0.6713\n",
            "Epoch 197 | Loss:  0.0047 | Val RMSE: 0.7914 | CI: 0.6714\n",
            "Epoch 198 | Loss:  0.0050 | Val RMSE: 0.7900 | CI: 0.6715\n",
            "Epoch 199 | Loss:  0.0066 | Val RMSE: 0.7901 | CI: 0.6717\n",
            "Epoch 200 | Loss:  0.0049 | Val RMSE: 0.7902 | CI: 0.6717\n",
            "Epoch 201 | Loss:  0.0052 | Val RMSE: 0.7935 | CI: 0.6718\n",
            "Epoch 202 | Loss:  0.0051 | Val RMSE: 0.7957 | CI: 0.6719\n",
            "Epoch 203 | Loss:  0.0048 | Val RMSE: 0.7967 | CI: 0.6719\n",
            "Epoch 204 | Loss:  0.0042 | Val RMSE: 0.7926 | CI: 0.6721\n",
            "Epoch 205 | Loss:  0.0055 | Val RMSE: 0.7891 | CI: 0.6722\n",
            "Epoch 206 | Loss:  0.0049 | Val RMSE: 0.7908 | CI: 0.6723\n",
            "Epoch 207 | Loss:  0.0046 | Val RMSE: 0.7906 | CI: 0.6725\n",
            "Epoch 208 | Loss:  0.0052 | Val RMSE: 0.7904 | CI: 0.6726\n",
            "Epoch 209 | Loss:  0.0052 | Val RMSE: 0.7887 | CI: 0.6726\n",
            "Epoch 210 | Loss:  0.0052 | Val RMSE: 0.7898 | CI: 0.6728\n",
            "Epoch 211 | Loss:  0.0055 | Val RMSE: 0.7925 | CI: 0.6729\n",
            "Epoch 212 | Loss:  0.0039 | Val RMSE: 0.7942 | CI: 0.6730\n",
            "Epoch 213 | Loss:  0.0049 | Val RMSE: 0.7929 | CI: 0.6732\n",
            "Epoch 214 | Loss:  0.0052 | Val RMSE: 0.7909 | CI: 0.6734\n",
            "Epoch 215 | Loss:  0.0048 | Val RMSE: 0.7888 | CI: 0.6737\n",
            "Epoch 216 | Loss:  0.0046 | Val RMSE: 0.7880 | CI: 0.6739\n",
            "Epoch 217 | Loss:  0.0047 | Val RMSE: 0.7878 | CI: 0.6742\n",
            "Epoch 218 | Loss:  0.0055 | Val RMSE: 0.7879 | CI: 0.6745\n",
            "Epoch 219 | Loss:  0.0049 | Val RMSE: 0.7877 | CI: 0.6748\n",
            "Epoch 220 | Loss:  0.0050 | Val RMSE: 0.7876 | CI: 0.6749\n",
            "Epoch 221 | Loss:  0.0044 | Val RMSE: 0.7886 | CI: 0.6750\n",
            "Epoch 222 | Loss:  0.0053 | Val RMSE: 0.7909 | CI: 0.6750\n",
            "Epoch 223 | Loss:  0.0064 | Val RMSE: 0.7962 | CI: 0.6751\n",
            "Epoch 224 | Loss:  0.0057 | Val RMSE: 0.7931 | CI: 0.6751\n",
            "Epoch 225 | Loss:  0.0055 | Val RMSE: 0.7908 | CI: 0.6751\n",
            "Epoch 226 | Loss:  0.0039 | Val RMSE: 0.7879 | CI: 0.6752\n",
            "Epoch 227 | Loss:  0.0050 | Val RMSE: 0.7873 | CI: 0.6752\n",
            "Epoch 228 | Loss:  0.0051 | Val RMSE: 0.7877 | CI: 0.6752\n",
            "Epoch 229 | Loss:  0.0047 | Val RMSE: 0.7875 | CI: 0.6753\n",
            "Epoch 230 | Loss:  0.0044 | Val RMSE: 0.7875 | CI: 0.6754\n",
            "Epoch 231 | Loss:  0.0050 | Val RMSE: 0.7912 | CI: 0.6755\n",
            "Epoch 232 | Loss:  0.0056 | Val RMSE: 0.7945 | CI: 0.6757\n",
            "Epoch 233 | Loss:  0.0045 | Val RMSE: 0.7939 | CI: 0.6759\n",
            "Epoch 234 | Loss:  0.0050 | Val RMSE: 0.7900 | CI: 0.6761\n",
            "Epoch 235 | Loss:  0.0066 | Val RMSE: 0.7868 | CI: 0.6763\n",
            "Epoch 236 | Loss:  0.0048 | Val RMSE: 0.7869 | CI: 0.6764\n",
            "Epoch 237 | Loss:  0.0054 | Val RMSE: 0.7869 | CI: 0.6766\n",
            "Epoch 238 | Loss:  0.0048 | Val RMSE: 0.7867 | CI: 0.6768\n",
            "Epoch 239 | Loss:  0.0047 | Val RMSE: 0.7868 | CI: 0.6770\n",
            "Epoch 240 | Loss:  0.0057 | Val RMSE: 0.7867 | CI: 0.6772\n",
            "Epoch 241 | Loss:  0.0049 | Val RMSE: 0.7879 | CI: 0.6773\n",
            "Epoch 242 | Loss:  0.0048 | Val RMSE: 0.7877 | CI: 0.6774\n",
            "Epoch 243 | Loss:  0.0044 | Val RMSE: 0.7877 | CI: 0.6775\n",
            "Epoch 244 | Loss:  0.0056 | Val RMSE: 0.7868 | CI: 0.6775\n",
            "Epoch 245 | Loss:  0.0045 | Val RMSE: 0.7874 | CI: 0.6775\n",
            "Epoch 246 | Loss:  0.0049 | Val RMSE: 0.7866 | CI: 0.6775\n",
            "Epoch 247 | Loss:  0.0048 | Val RMSE: 0.7876 | CI: 0.6775\n",
            "Epoch 248 | Loss:  0.0048 | Val RMSE: 0.7883 | CI: 0.6775\n",
            "Epoch 249 | Loss:  0.0044 | Val RMSE: 0.7878 | CI: 0.6774\n",
            "Epoch 250 | Loss:  0.0047 | Val RMSE: 0.7886 | CI: 0.6774\n",
            "Epoch 251 | Loss:  0.0047 | Val RMSE: 0.7877 | CI: 0.6772\n",
            "Epoch 252 | Loss:  0.0052 | Val RMSE: 0.7864 | CI: 0.6771\n",
            "Epoch 253 | Loss:  0.0047 | Val RMSE: 0.7865 | CI: 0.6771\n",
            "Epoch 254 | Loss:  0.0051 | Val RMSE: 0.7877 | CI: 0.6771\n",
            "Epoch 255 | Loss:  0.0051 | Val RMSE: 0.7862 | CI: 0.6771\n",
            "Epoch 256 | Loss:  0.0050 | Val RMSE: 0.7893 | CI: 0.6770\n",
            "Epoch 257 | Loss:  0.0047 | Val RMSE: 0.7992 | CI: 0.6768\n",
            "Epoch 258 | Loss:  0.0046 | Val RMSE: 0.8028 | CI: 0.6767\n",
            "Epoch 259 | Loss:  0.0050 | Val RMSE: 0.7900 | CI: 0.6766\n",
            "Epoch 260 | Loss:  0.0051 | Val RMSE: 0.7878 | CI: 0.6764\n",
            "Epoch 261 | Loss:  0.0050 | Val RMSE: 0.8018 | CI: 0.6763\n",
            "Epoch 262 | Loss:  0.0049 | Val RMSE: 0.7963 | CI: 0.6762\n",
            "Epoch 263 | Loss:  0.0052 | Val RMSE: 0.7862 | CI: 0.6761\n",
            "Epoch 264 | Loss:  0.0047 | Val RMSE: 0.7956 | CI: 0.6760\n",
            "Epoch 265 | Loss:  0.0051 | Val RMSE: 0.8012 | CI: 0.6760\n",
            "Epoch 266 | Loss:  0.0054 | Val RMSE: 0.7961 | CI: 0.6760\n",
            "Epoch 267 | Loss:  0.0045 | Val RMSE: 0.7862 | CI: 0.6761\n",
            "Epoch 268 | Loss:  0.0047 | Val RMSE: 0.7917 | CI: 0.6762\n",
            "Epoch 269 | Loss:  0.0044 | Val RMSE: 0.7943 | CI: 0.6764\n",
            "Epoch 270 | Loss:  0.0047 | Val RMSE: 0.7901 | CI: 0.6765\n",
            "Epoch 271 | Loss:  0.0052 | Val RMSE: 0.7857 | CI: 0.6765\n",
            "Epoch 272 | Loss:  0.0052 | Val RMSE: 0.7902 | CI: 0.6765\n",
            "Epoch 273 | Loss:  0.0043 | Val RMSE: 0.7999 | CI: 0.6765\n",
            "Epoch 274 | Loss:  0.0046 | Val RMSE: 0.7965 | CI: 0.6766\n",
            "Epoch 275 | Loss:  0.0045 | Val RMSE: 0.7878 | CI: 0.6768\n",
            "Epoch 276 | Loss:  0.0046 | Val RMSE: 0.7855 | CI: 0.6770\n",
            "Epoch 277 | Loss:  0.0046 | Val RMSE: 0.7882 | CI: 0.6770\n",
            "Epoch 278 | Loss:  0.0047 | Val RMSE: 0.7868 | CI: 0.6771\n",
            "Epoch 279 | Loss:  0.0045 | Val RMSE: 0.7849 | CI: 0.6772\n",
            "Epoch 280 | Loss:  0.0050 | Val RMSE: 0.7854 | CI: 0.6772\n",
            "Epoch 281 | Loss:  0.0045 | Val RMSE: 0.7869 | CI: 0.6773\n",
            "Epoch 282 | Loss:  0.0044 | Val RMSE: 0.7871 | CI: 0.6775\n",
            "Epoch 283 | Loss:  0.0052 | Val RMSE: 0.7850 | CI: 0.6777\n",
            "Epoch 284 | Loss:  0.0055 | Val RMSE: 0.7850 | CI: 0.6779\n",
            "Epoch 285 | Loss:  0.0053 | Val RMSE: 0.7886 | CI: 0.6780\n",
            "Epoch 286 | Loss:  0.0051 | Val RMSE: 0.7906 | CI: 0.6781\n",
            "Epoch 287 | Loss:  0.0042 | Val RMSE: 0.7836 | CI: 0.6784\n",
            "Epoch 288 | Loss:  0.0044 | Val RMSE: 0.7937 | CI: 0.6786\n",
            "Epoch 289 | Loss:  0.0054 | Val RMSE: 0.7984 | CI: 0.6788\n",
            "Epoch 290 | Loss:  0.0048 | Val RMSE: 0.7904 | CI: 0.6789\n",
            "Epoch 291 | Loss:  0.0043 | Val RMSE: 0.7836 | CI: 0.6789\n",
            "Epoch 292 | Loss:  0.0047 | Val RMSE: 0.7833 | CI: 0.6790\n",
            "Epoch 293 | Loss:  0.0051 | Val RMSE: 0.7848 | CI: 0.6790\n",
            "Epoch 294 | Loss:  0.0053 | Val RMSE: 0.7825 | CI: 0.6791\n",
            "Epoch 295 | Loss:  0.0050 | Val RMSE: 0.7841 | CI: 0.6793\n",
            "Epoch 296 | Loss:  0.0046 | Val RMSE: 0.7861 | CI: 0.6795\n",
            "Epoch 297 | Loss:  0.0042 | Val RMSE: 0.7842 | CI: 0.6796\n",
            "Epoch 298 | Loss:  0.0046 | Val RMSE: 0.7814 | CI: 0.6798\n",
            "Epoch 299 | Loss:  0.0048 | Val RMSE: 0.7820 | CI: 0.6800\n",
            "Epoch 300 | Loss:  0.0051 | Val RMSE: 0.7865 | CI: 0.6801\n",
            "Epoch 301 | Loss:  0.0042 | Val RMSE: 0.7837 | CI: 0.6802\n",
            "Epoch 302 | Loss:  0.0056 | Val RMSE: 0.7805 | CI: 0.6804\n",
            "Epoch 303 | Loss:  0.0047 | Val RMSE: 0.7890 | CI: 0.6804\n",
            "Epoch 304 | Loss:  0.0048 | Val RMSE: 0.7981 | CI: 0.6805\n",
            "Epoch 305 | Loss:  0.0042 | Val RMSE: 0.7872 | CI: 0.6805\n",
            "Epoch 306 | Loss:  0.0049 | Val RMSE: 0.7808 | CI: 0.6805\n",
            "Epoch 307 | Loss:  0.0048 | Val RMSE: 0.7894 | CI: 0.6805\n",
            "Epoch 308 | Loss:  0.0050 | Val RMSE: 0.7851 | CI: 0.6806\n",
            "Epoch 309 | Loss:  0.0039 | Val RMSE: 0.7810 | CI: 0.6807\n",
            "Epoch 310 | Loss:  0.0044 | Val RMSE: 0.8023 | CI: 0.6808\n",
            "Epoch 311 | Loss:  0.0050 | Val RMSE: 0.8179 | CI: 0.6810\n",
            "Epoch 312 | Loss:  0.0066 | Val RMSE: 0.7906 | CI: 0.6811\n",
            "Epoch 313 | Loss:  0.0045 | Val RMSE: 0.7798 | CI: 0.6812\n",
            "Epoch 314 | Loss:  0.0048 | Val RMSE: 0.7864 | CI: 0.6814\n",
            "Epoch 315 | Loss:  0.0052 | Val RMSE: 0.7813 | CI: 0.6815\n",
            "Epoch 316 | Loss:  0.0052 | Val RMSE: 0.7792 | CI: 0.6816\n",
            "Epoch 317 | Loss:  0.0050 | Val RMSE: 0.7809 | CI: 0.6818\n",
            "Epoch 318 | Loss:  0.0048 | Val RMSE: 0.7812 | CI: 0.6818\n",
            "Epoch 319 | Loss:  0.0052 | Val RMSE: 0.7792 | CI: 0.6818\n",
            "Epoch 320 | Loss:  0.0046 | Val RMSE: 0.7801 | CI: 0.6817\n",
            "Epoch 321 | Loss:  0.0043 | Val RMSE: 0.7789 | CI: 0.6818\n",
            "Epoch 322 | Loss:  0.0040 | Val RMSE: 0.7820 | CI: 0.6820\n",
            "Epoch 323 | Loss:  0.0052 | Val RMSE: 0.7813 | CI: 0.6820\n",
            "Epoch 324 | Loss:  0.0048 | Val RMSE: 0.7789 | CI: 0.6821\n",
            "Epoch 325 | Loss:  0.0051 | Val RMSE: 0.7785 | CI: 0.6821\n",
            "Epoch 326 | Loss:  0.0040 | Val RMSE: 0.7782 | CI: 0.6822\n",
            "Epoch 327 | Loss:  0.0050 | Val RMSE: 0.7781 | CI: 0.6822\n",
            "Epoch 328 | Loss:  0.0052 | Val RMSE: 0.7784 | CI: 0.6824\n",
            "Epoch 329 | Loss:  0.0049 | Val RMSE: 0.7820 | CI: 0.6827\n",
            "Epoch 330 | Loss:  0.0043 | Val RMSE: 0.7862 | CI: 0.6829\n",
            "Epoch 331 | Loss:  0.0050 | Val RMSE: 0.7783 | CI: 0.6830\n",
            "Epoch 332 | Loss:  0.0051 | Val RMSE: 0.7786 | CI: 0.6828\n",
            "Epoch 333 | Loss:  0.0045 | Val RMSE: 0.7784 | CI: 0.6829\n",
            "Epoch 334 | Loss:  0.0045 | Val RMSE: 0.7785 | CI: 0.6831\n",
            "Epoch 335 | Loss:  0.0054 | Val RMSE: 0.7882 | CI: 0.6835\n",
            "Epoch 336 | Loss:  0.0045 | Val RMSE: 0.7955 | CI: 0.6837\n",
            "Epoch 337 | Loss:  0.0041 | Val RMSE: 0.7893 | CI: 0.6838\n",
            "Epoch 338 | Loss:  0.0046 | Val RMSE: 0.7777 | CI: 0.6837\n",
            "Epoch 339 | Loss:  0.0042 | Val RMSE: 0.7785 | CI: 0.6837\n",
            "Epoch 340 | Loss:  0.0046 | Val RMSE: 0.7801 | CI: 0.6837\n",
            "Epoch 341 | Loss:  0.0053 | Val RMSE: 0.7796 | CI: 0.6836\n",
            "Epoch 342 | Loss:  0.0044 | Val RMSE: 0.7779 | CI: 0.6838\n",
            "Epoch 343 | Loss:  0.0053 | Val RMSE: 0.7822 | CI: 0.6838\n",
            "Epoch 344 | Loss:  0.0054 | Val RMSE: 0.7835 | CI: 0.6838\n",
            "Epoch 345 | Loss:  0.0035 | Val RMSE: 0.7791 | CI: 0.6836\n",
            "Epoch 346 | Loss:  0.0044 | Val RMSE: 0.7761 | CI: 0.6835\n",
            "Epoch 347 | Loss:  0.0049 | Val RMSE: 0.7769 | CI: 0.6835\n",
            "Epoch 348 | Loss:  0.0043 | Val RMSE: 0.7783 | CI: 0.6838\n",
            "Epoch 349 | Loss:  0.0048 | Val RMSE: 0.7769 | CI: 0.6841\n",
            "Epoch 350 | Loss:  0.0050 | Val RMSE: 0.7767 | CI: 0.6844\n",
            "Epoch 351 | Loss:  0.0051 | Val RMSE: 0.7823 | CI: 0.6847\n",
            "Epoch 352 | Loss:  0.0043 | Val RMSE: 0.7851 | CI: 0.6848\n",
            "Epoch 353 | Loss:  0.0042 | Val RMSE: 0.7839 | CI: 0.6850\n",
            "Epoch 354 | Loss:  0.0049 | Val RMSE: 0.7754 | CI: 0.6849\n",
            "Epoch 355 | Loss:  0.0046 | Val RMSE: 0.7829 | CI: 0.6847\n",
            "Epoch 356 | Loss:  0.0047 | Val RMSE: 0.7765 | CI: 0.6848\n",
            "Epoch 357 | Loss:  0.0048 | Val RMSE: 0.7754 | CI: 0.6849\n",
            "Epoch 358 | Loss:  0.0046 | Val RMSE: 0.7806 | CI: 0.6849\n",
            "Epoch 359 | Loss:  0.0052 | Val RMSE: 0.7792 | CI: 0.6845\n",
            "Epoch 360 | Loss:  0.0046 | Val RMSE: 0.7787 | CI: 0.6843\n",
            "Epoch 361 | Loss:  0.0052 | Val RMSE: 0.7739 | CI: 0.6838\n",
            "Epoch 362 | Loss:  0.0043 | Val RMSE: 0.7744 | CI: 0.6835\n",
            "Epoch 363 | Loss:  0.0051 | Val RMSE: 0.7739 | CI: 0.6832\n",
            "Epoch 364 | Loss:  0.0045 | Val RMSE: 0.7735 | CI: 0.6832\n",
            "Epoch 365 | Loss:  0.0047 | Val RMSE: 0.7764 | CI: 0.6831\n",
            "Epoch 366 | Loss:  0.0047 | Val RMSE: 0.7731 | CI: 0.6824\n",
            "Epoch 367 | Loss:  0.0047 | Val RMSE: 0.7768 | CI: 0.6819\n",
            "Epoch 368 | Loss:  0.0045 | Val RMSE: 0.7737 | CI: 0.6819\n",
            "Epoch 369 | Loss:  0.0048 | Val RMSE: 0.7759 | CI: 0.6827\n",
            "Epoch 370 | Loss:  0.0051 | Val RMSE: 0.7749 | CI: 0.6830\n",
            "Epoch 371 | Loss:  0.0038 | Val RMSE: 0.7743 | CI: 0.6834\n",
            "Epoch 372 | Loss:  0.0040 | Val RMSE: 0.7727 | CI: 0.6837\n",
            "Epoch 373 | Loss:  0.0052 | Val RMSE: 0.7751 | CI: 0.6839\n",
            "Epoch 374 | Loss:  0.0050 | Val RMSE: 0.7729 | CI: 0.6846\n",
            "Epoch 375 | Loss:  0.0054 | Val RMSE: 0.7719 | CI: 0.6851\n",
            "Epoch 376 | Loss:  0.0044 | Val RMSE: 0.7739 | CI: 0.6855\n",
            "Epoch 377 | Loss:  0.0045 | Val RMSE: 0.7764 | CI: 0.6858\n",
            "Epoch 378 | Loss:  0.0041 | Val RMSE: 0.7744 | CI: 0.6859\n",
            "Epoch 379 | Loss:  0.0046 | Val RMSE: 0.7720 | CI: 0.6860\n",
            "Epoch 380 | Loss:  0.0051 | Val RMSE: 0.7714 | CI: 0.6859\n",
            "Epoch 381 | Loss:  0.0043 | Val RMSE: 0.7720 | CI: 0.6860\n",
            "Epoch 382 | Loss:  0.0042 | Val RMSE: 0.7702 | CI: 0.6863\n",
            "Epoch 383 | Loss:  0.0048 | Val RMSE: 0.7709 | CI: 0.6866\n",
            "Epoch 384 | Loss:  0.0054 | Val RMSE: 0.7694 | CI: 0.6865\n",
            "Epoch 385 | Loss:  0.0047 | Val RMSE: 0.7708 | CI: 0.6864\n",
            "Epoch 386 | Loss:  0.0045 | Val RMSE: 0.7689 | CI: 0.6865\n",
            "Epoch 387 | Loss:  0.0048 | Val RMSE: 0.7782 | CI: 0.6868\n",
            "Epoch 388 | Loss:  0.0047 | Val RMSE: 0.7759 | CI: 0.6867\n",
            "Epoch 389 | Loss:  0.0046 | Val RMSE: 0.7685 | CI: 0.6864\n",
            "Epoch 390 | Loss:  0.0044 | Val RMSE: 0.7685 | CI: 0.6861\n",
            "Epoch 391 | Loss:  0.0043 | Val RMSE: 0.7692 | CI: 0.6862\n",
            "Epoch 392 | Loss:  0.0051 | Val RMSE: 0.7729 | CI: 0.6863\n",
            "Epoch 393 | Loss:  0.0060 | Val RMSE: 0.7744 | CI: 0.6863\n",
            "Epoch 394 | Loss:  0.0045 | Val RMSE: 0.7738 | CI: 0.6864\n",
            "Epoch 395 | Loss:  0.0044 | Val RMSE: 0.7691 | CI: 0.6860\n",
            "Epoch 396 | Loss:  0.0049 | Val RMSE: 0.7715 | CI: 0.6861\n",
            "Epoch 397 | Loss:  0.0043 | Val RMSE: 0.7682 | CI: 0.6865\n",
            "Epoch 398 | Loss:  0.0038 | Val RMSE: 0.7676 | CI: 0.6867\n",
            "Epoch 399 | Loss:  0.0048 | Val RMSE: 0.7673 | CI: 0.6869\n",
            "Epoch 400 | Loss:  0.0045 | Val RMSE: 0.7678 | CI: 0.6872\n",
            "Epoch 401 | Loss:  0.0046 | Val RMSE: 0.7679 | CI: 0.6872\n",
            "Epoch 402 | Loss:  0.0043 | Val RMSE: 0.7674 | CI: 0.6872\n",
            "Epoch 403 | Loss:  0.0044 | Val RMSE: 0.7675 | CI: 0.6874\n",
            "Epoch 404 | Loss:  0.0040 | Val RMSE: 0.7658 | CI: 0.6874\n",
            "Epoch 405 | Loss:  0.0043 | Val RMSE: 0.7669 | CI: 0.6876\n",
            "Epoch 406 | Loss:  0.0046 | Val RMSE: 0.7660 | CI: 0.6876\n",
            "Epoch 407 | Loss:  0.0047 | Val RMSE: 0.7644 | CI: 0.6877\n",
            "Epoch 408 | Loss:  0.0049 | Val RMSE: 0.7688 | CI: 0.6872\n",
            "Epoch 409 | Loss:  0.0047 | Val RMSE: 0.7679 | CI: 0.6872\n",
            "Epoch 410 | Loss:  0.0044 | Val RMSE: 0.7649 | CI: 0.6876\n",
            "Epoch 411 | Loss:  0.0047 | Val RMSE: 0.7721 | CI: 0.6878\n",
            "Epoch 412 | Loss:  0.0041 | Val RMSE: 0.7683 | CI: 0.6875\n",
            "Epoch 413 | Loss:  0.0041 | Val RMSE: 0.7627 | CI: 0.6868\n",
            "Epoch 414 | Loss:  0.0047 | Val RMSE: 0.7677 | CI: 0.6859\n",
            "Epoch 415 | Loss:  0.0052 | Val RMSE: 0.7623 | CI: 0.6868\n",
            "Epoch 416 | Loss:  0.0051 | Val RMSE: 0.7648 | CI: 0.6874\n",
            "Epoch 417 | Loss:  0.0044 | Val RMSE: 0.7622 | CI: 0.6872\n",
            "Epoch 418 | Loss:  0.0042 | Val RMSE: 0.7641 | CI: 0.6878\n",
            "Epoch 419 | Loss:  0.0047 | Val RMSE: 0.7655 | CI: 0.6881\n",
            "Epoch 420 | Loss:  0.0046 | Val RMSE: 0.7620 | CI: 0.6880\n",
            "Epoch 421 | Loss:  0.0045 | Val RMSE: 0.7633 | CI: 0.6878\n",
            "Epoch 422 | Loss:  0.0049 | Val RMSE: 0.7623 | CI: 0.6885\n",
            "Epoch 423 | Loss:  0.0044 | Val RMSE: 0.7717 | CI: 0.6892\n",
            "Epoch 424 | Loss:  0.0039 | Val RMSE: 0.7703 | CI: 0.6895\n",
            "Epoch 425 | Loss:  0.0047 | Val RMSE: 0.7616 | CI: 0.6892\n",
            "Epoch 426 | Loss:  0.0042 | Val RMSE: 0.7738 | CI: 0.6891\n",
            "Epoch 427 | Loss:  0.0048 | Val RMSE: 0.7618 | CI: 0.6900\n",
            "Epoch 428 | Loss:  0.0041 | Val RMSE: 0.7745 | CI: 0.6908\n",
            "Epoch 429 | Loss:  0.0051 | Val RMSE: 0.7714 | CI: 0.6908\n",
            "Epoch 430 | Loss:  0.0041 | Val RMSE: 0.7616 | CI: 0.6903\n",
            "Epoch 431 | Loss:  0.0048 | Val RMSE: 0.7727 | CI: 0.6901\n",
            "Epoch 432 | Loss:  0.0045 | Val RMSE: 0.7632 | CI: 0.6912\n",
            "Epoch 433 | Loss:  0.0050 | Val RMSE: 0.7739 | CI: 0.6915\n",
            "Epoch 434 | Loss:  0.0043 | Val RMSE: 0.7640 | CI: 0.6914\n",
            "Epoch 435 | Loss:  0.0049 | Val RMSE: 0.7746 | CI: 0.6900\n",
            "Epoch 436 | Loss:  0.0051 | Val RMSE: 0.7670 | CI: 0.6898\n",
            "Epoch 437 | Loss:  0.0042 | Val RMSE: 0.7635 | CI: 0.6909\n",
            "Epoch 438 | Loss:  0.0041 | Val RMSE: 0.7691 | CI: 0.6908\n",
            "Epoch 439 | Loss:  0.0043 | Val RMSE: 0.7599 | CI: 0.6900\n",
            "Epoch 440 | Loss:  0.0041 | Val RMSE: 0.7592 | CI: 0.6897\n",
            "Epoch 441 | Loss:  0.0048 | Val RMSE: 0.7573 | CI: 0.6890\n",
            "Epoch 442 | Loss:  0.0041 | Val RMSE: 0.7571 | CI: 0.6891\n",
            "Epoch 443 | Loss:  0.0039 | Val RMSE: 0.7564 | CI: 0.6898\n",
            "Epoch 444 | Loss:  0.0040 | Val RMSE: 0.7644 | CI: 0.6914\n",
            "Epoch 445 | Loss:  0.0039 | Val RMSE: 0.7678 | CI: 0.6921\n",
            "Epoch 446 | Loss:  0.0043 | Val RMSE: 0.7558 | CI: 0.6920\n",
            "Epoch 447 | Loss:  0.0047 | Val RMSE: 0.7632 | CI: 0.6915\n",
            "Epoch 448 | Loss:  0.0043 | Val RMSE: 0.7550 | CI: 0.6925\n",
            "Epoch 449 | Loss:  0.0036 | Val RMSE: 0.7652 | CI: 0.6935\n",
            "Epoch 450 | Loss:  0.0040 | Val RMSE: 0.7647 | CI: 0.6937\n",
            "Epoch 451 | Loss:  0.0044 | Val RMSE: 0.7541 | CI: 0.6929\n",
            "Epoch 452 | Loss:  0.0053 | Val RMSE: 0.7611 | CI: 0.6922\n",
            "Epoch 453 | Loss:  0.0041 | Val RMSE: 0.7553 | CI: 0.6933\n",
            "Epoch 454 | Loss:  0.0042 | Val RMSE: 0.7643 | CI: 0.6938\n",
            "Epoch 455 | Loss:  0.0046 | Val RMSE: 0.7594 | CI: 0.6938\n",
            "Epoch 456 | Loss:  0.0042 | Val RMSE: 0.7547 | CI: 0.6939\n",
            "Epoch 457 | Loss:  0.0044 | Val RMSE: 0.7582 | CI: 0.6936\n",
            "Epoch 458 | Loss:  0.0046 | Val RMSE: 0.7580 | CI: 0.6940\n",
            "Epoch 459 | Loss:  0.0040 | Val RMSE: 0.7613 | CI: 0.6950\n",
            "Epoch 460 | Loss:  0.0049 | Val RMSE: 0.7582 | CI: 0.6951\n",
            "Epoch 461 | Loss:  0.0044 | Val RMSE: 0.7534 | CI: 0.6951\n",
            "Epoch 462 | Loss:  0.0037 | Val RMSE: 0.7531 | CI: 0.6953\n",
            "Epoch 463 | Loss:  0.0041 | Val RMSE: 0.7533 | CI: 0.6957\n",
            "Epoch 464 | Loss:  0.0043 | Val RMSE: 0.7578 | CI: 0.6962\n",
            "Epoch 465 | Loss:  0.0045 | Val RMSE: 0.7601 | CI: 0.6964\n",
            "Epoch 466 | Loss:  0.0059 | Val RMSE: 0.7527 | CI: 0.6964\n",
            "Epoch 467 | Loss:  0.0044 | Val RMSE: 0.7504 | CI: 0.6960\n",
            "Epoch 468 | Loss:  0.0046 | Val RMSE: 0.7548 | CI: 0.6951\n",
            "Epoch 469 | Loss:  0.0036 | Val RMSE: 0.7488 | CI: 0.6957\n",
            "Epoch 470 | Loss:  0.0041 | Val RMSE: 0.7600 | CI: 0.6967\n",
            "Epoch 471 | Loss:  0.0047 | Val RMSE: 0.7499 | CI: 0.6957\n",
            "Epoch 472 | Loss:  0.0043 | Val RMSE: 0.7489 | CI: 0.6945\n",
            "Epoch 473 | Loss:  0.0045 | Val RMSE: 0.7585 | CI: 0.6935\n",
            "Epoch 474 | Loss:  0.0050 | Val RMSE: 0.7579 | CI: 0.6964\n",
            "Epoch 475 | Loss:  0.0046 | Val RMSE: 0.7531 | CI: 0.6964\n",
            "Epoch 476 | Loss:  0.0036 | Val RMSE: 0.7465 | CI: 0.6959\n",
            "Epoch 477 | Loss:  0.0042 | Val RMSE: 0.7462 | CI: 0.6967\n",
            "Epoch 478 | Loss:  0.0047 | Val RMSE: 0.7613 | CI: 0.6958\n",
            "Epoch 479 | Loss:  0.0048 | Val RMSE: 0.7458 | CI: 0.6978\n",
            "Epoch 480 | Loss:  0.0043 | Val RMSE: 0.7571 | CI: 0.6991\n",
            "Epoch 481 | Loss:  0.0045 | Val RMSE: 0.7518 | CI: 0.6991\n",
            "Epoch 482 | Loss:  0.0040 | Val RMSE: 0.7541 | CI: 0.6974\n",
            "Epoch 483 | Loss:  0.0039 | Val RMSE: 0.7473 | CI: 0.6979\n",
            "Epoch 484 | Loss:  0.0042 | Val RMSE: 0.7489 | CI: 0.6992\n",
            "Epoch 485 | Loss:  0.0045 | Val RMSE: 0.7475 | CI: 0.6993\n",
            "Epoch 486 | Loss:  0.0048 | Val RMSE: 0.7754 | CI: 0.6964\n",
            "Epoch 487 | Loss:  0.0051 | Val RMSE: 0.7506 | CI: 0.6978\n",
            "Epoch 488 | Loss:  0.0042 | Val RMSE: 0.7975 | CI: 0.6999\n",
            "Epoch 489 | Loss:  0.0051 | Val RMSE: 0.7973 | CI: 0.6998\n",
            "Epoch 490 | Loss:  0.0055 | Val RMSE: 0.7439 | CI: 0.7005\n",
            "Epoch 491 | Loss:  0.0044 | Val RMSE: 0.7802 | CI: 0.6989\n",
            "Epoch 492 | Loss:  0.0049 | Val RMSE: 0.7446 | CI: 0.7011\n",
            "Epoch 493 | Loss:  0.0044 | Val RMSE: 0.8142 | CI: 0.7002\n",
            "Epoch 494 | Loss:  0.0056 | Val RMSE: 0.7726 | CI: 0.7009\n",
            "Epoch 495 | Loss:  0.0044 | Val RMSE: 0.7546 | CI: 0.7017\n",
            "Epoch 496 | Loss:  0.0040 | Val RMSE: 0.7759 | CI: 0.7017\n",
            "Epoch 497 | Loss:  0.0043 | Val RMSE: 0.7469 | CI: 0.7019\n",
            "Epoch 498 | Loss:  0.0039 | Val RMSE: 0.8160 | CI: 0.6993\n",
            "Epoch 499 | Loss:  0.0045 | Val RMSE: 0.8153 | CI: 0.6995\n",
            "Epoch 500 | Loss:  0.0048 | Val RMSE: 0.7477 | CI: 0.7021\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Save model weights for fine tuning\n",
        "\n",
        "model_path = f\"{data_path}/kiba_graphdta_baseline_weights.pt\"\n",
        "torch.save(model.state_dict(), model_path)\n",
        "print(f\"Saved model_weights to {model_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wret7h3T7sZ0",
        "outputId": "429bf917-5dda-4123-cb5b-10f6404d574a"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved model_weights to /content/drive/MyDrive/Rebuilding_and_Modifying_GraphDTA/data/kiba_graphdta_baseline_weights.pt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Save model\n",
        "\n",
        "full_model_path = f\"{data_path}/kiba_graphdta_baseline_model.pt\"\n",
        "torch.save(model, full_model_path)\n",
        "print(f\"Saved model to {full_model_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OrnuOTOC8Lro",
        "outputId": "50473b30-27d2-4967-d923-17ee9828d6df"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved model to /content/drive/MyDrive/Rebuilding_and_Modifying_GraphDTA/data/kiba_graphdta_baseline_model.pt\n"
          ]
        }
      ]
    }
  ]
}