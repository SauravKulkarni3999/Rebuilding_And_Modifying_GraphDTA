{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Install PyTorch Geometric and dependencies for PyTorch 2.0.1 + CUDA 11.8\n",
        "!pip install torch==2.0.1+cu118 torchvision==0.15.2+cu118 torchaudio==2.0.2 --index-url https://download.pytorch.org/whl/cu118\n",
        "!pip install torch-geometric torch-scatter torch-sparse torch-cluster torch-spline-conv -f https://data.pyg.org/whl/torch-2.0.1+cu118.html\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9amlt-i5oQhV",
        "outputId": "278aa57c-f673-49f9-8623-6902a374922b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://download.pytorch.org/whl/cu118\n",
            "Collecting torch==2.0.1+cu118\n",
            "  Downloading https://download.pytorch.org/whl/cu118/torch-2.0.1%2Bcu118-cp311-cp311-linux_x86_64.whl (2267.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 GB\u001b[0m \u001b[31m962.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torchvision==0.15.2+cu118\n",
            "  Downloading https://download.pytorch.org/whl/cu118/torchvision-0.15.2%2Bcu118-cp311-cp311-linux_x86_64.whl (6.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.1/6.1 MB\u001b[0m \u001b[31m124.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torchaudio==2.0.2\n",
            "  Downloading https://download.pytorch.org/whl/cu118/torchaudio-2.0.2%2Bcu118-cp311-cp311-linux_x86_64.whl (4.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m118.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.0.1+cu118) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from torch==2.0.1+cu118) (4.13.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch==2.0.1+cu118) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.0.1+cu118) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.0.1+cu118) (3.1.6)\n",
            "Collecting triton==2.0.0 (from torch==2.0.1+cu118)\n",
            "  Downloading https://download.pytorch.org/whl/triton-2.0.0-1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (63.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.3/63.3 MB\u001b[0m \u001b[31m39.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision==0.15.2+cu118) (2.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torchvision==0.15.2+cu118) (2.32.3)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision==0.15.2+cu118) (11.2.1)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.11/dist-packages (from triton==2.0.0->torch==2.0.1+cu118) (3.31.6)\n",
            "Collecting lit (from triton==2.0.0->torch==2.0.1+cu118)\n",
            "  Downloading https://download.pytorch.org/whl/lit-15.0.7.tar.gz (132 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.3/132.3 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.0.1+cu118) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torchvision==0.15.2+cu118) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torchvision==0.15.2+cu118) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->torchvision==0.15.2+cu118) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torchvision==0.15.2+cu118) (2025.4.26)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->torch==2.0.1+cu118) (1.3.0)\n",
            "Building wheels for collected packages: lit\n",
            "  Building wheel for lit (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for lit: filename=lit-15.0.7-py3-none-any.whl size=89990 sha256=12850be73662a174f93d52c7025ea170e8fd8b3ff3c645b5369abdbd8dba71cc\n",
            "  Stored in directory: /root/.cache/pip/wheels/fc/5d/45/34fe9945d5e45e261134e72284395be36c2d4828af38e2b0fe\n",
            "Successfully built lit\n",
            "Installing collected packages: lit, triton, torch, torchvision, torchaudio\n",
            "  Attempting uninstall: triton\n",
            "    Found existing installation: triton 3.2.0\n",
            "    Uninstalling triton-3.2.0:\n",
            "      Successfully uninstalled triton-3.2.0\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.6.0+cu124\n",
            "    Uninstalling torch-2.6.0+cu124:\n",
            "      Successfully uninstalled torch-2.6.0+cu124\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.21.0+cu124\n",
            "    Uninstalling torchvision-0.21.0+cu124:\n",
            "      Successfully uninstalled torchvision-0.21.0+cu124\n",
            "  Attempting uninstall: torchaudio\n",
            "    Found existing installation: torchaudio 2.6.0+cu124\n",
            "    Uninstalling torchaudio-2.6.0+cu124:\n",
            "      Successfully uninstalled torchaudio-2.6.0+cu124\n",
            "Successfully installed lit-15.0.7 torch-2.0.1+cu118 torchaudio-2.0.2+cu118 torchvision-0.15.2+cu118 triton-2.0.0\n",
            "Looking in links: https://data.pyg.org/whl/torch-2.0.1+cu118.html\n",
            "Collecting torch-geometric\n",
            "  Downloading torch_geometric-2.6.1-py3-none-any.whl.metadata (63 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.1/63.1 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torch-scatter\n",
            "  Downloading https://data.pyg.org/whl/torch-2.0.0%2Bcu118/torch_scatter-2.1.2%2Bpt20cu118-cp311-cp311-linux_x86_64.whl (10.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m66.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torch-sparse\n",
            "  Downloading https://data.pyg.org/whl/torch-2.0.0%2Bcu118/torch_sparse-0.6.18%2Bpt20cu118-cp311-cp311-linux_x86_64.whl (4.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m120.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torch-cluster\n",
            "  Downloading https://data.pyg.org/whl/torch-2.0.0%2Bcu118/torch_cluster-1.6.3%2Bpt20cu118-cp311-cp311-linux_x86_64.whl (3.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m46.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torch-spline-conv\n",
            "  Downloading https://data.pyg.org/whl/torch-2.0.0%2Bcu118/torch_spline_conv-1.2.2%2Bpt20cu118-cp311-cp311-linux_x86_64.whl (886 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m886.5/886.5 kB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.11.15)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (2025.3.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.1.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (2.0.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (5.9.5)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.2.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (4.67.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from torch-sparse) (1.15.3)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.20.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch-geometric) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (2025.4.26)\n",
            "Downloading torch_geometric-2.6.1-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torch-spline-conv, torch-scatter, torch-sparse, torch-cluster, torch-geometric\n",
            "Successfully installed torch-cluster-1.6.3+pt20cu118 torch-geometric-2.6.1 torch-scatter-2.1.2+pt20cu118 torch-sparse-0.6.18+pt20cu118 torch-spline-conv-1.2.2+pt20cu118\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YxZPpKINmtXo",
        "outputId": "2ae1051f-6c44-4041-f063-b6a402951a83"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "id": "sCz2WAComC_V",
        "outputId": "5e96f90e-ace6-4bf7-908a-39ee66621496"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   Protein_Index Accession_Number                   Gene_Name  \\\n",
              "0              0      NP_055726.3                        AAK1   \n",
              "1              1      NP_005148.2  ABL1(E255K)-phosphorylated   \n",
              "2              3      NP_005148.2  ABL1(F317I)-phosphorylated   \n",
              "3              5      NP_005148.2  ABL1(F317L)-phosphorylated   \n",
              "4              7      NP_005148.2  ABL1(H396P)-phosphorylated   \n",
              "\n",
              "                                            Sequence  \n",
              "0  MKKFFDSRREQGGSGLGSGSSGGGGSTSGLGSGYIGRVFGIGRQQV...  \n",
              "1  MLEICLKLVGCKSKKGLSSSSSCYLEEALQRPVASDFEPQGLSEAA...  \n",
              "2  MLEICLKLVGCKSKKGLSSSSSCYLEEALQRPVASDFEPQGLSEAA...  \n",
              "3  MLEICLKLVGCKSKKGLSSSSSCYLEEALQRPVASDFEPQGLSEAA...  \n",
              "4  MLEICLKLVGCKSKKGLSSSSSCYLEEALQRPVASDFEPQGLSEAA...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-a8b94cc2-e33c-4e41-8435-65d61a7e1f4b\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Protein_Index</th>\n",
              "      <th>Accession_Number</th>\n",
              "      <th>Gene_Name</th>\n",
              "      <th>Sequence</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>NP_055726.3</td>\n",
              "      <td>AAK1</td>\n",
              "      <td>MKKFFDSRREQGGSGLGSGSSGGGGSTSGLGSGYIGRVFGIGRQQV...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>NP_005148.2</td>\n",
              "      <td>ABL1(E255K)-phosphorylated</td>\n",
              "      <td>MLEICLKLVGCKSKKGLSSSSSCYLEEALQRPVASDFEPQGLSEAA...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>NP_005148.2</td>\n",
              "      <td>ABL1(F317I)-phosphorylated</td>\n",
              "      <td>MLEICLKLVGCKSKKGLSSSSSCYLEEALQRPVASDFEPQGLSEAA...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>5</td>\n",
              "      <td>NP_005148.2</td>\n",
              "      <td>ABL1(F317L)-phosphorylated</td>\n",
              "      <td>MLEICLKLVGCKSKKGLSSSSSCYLEEALQRPVASDFEPQGLSEAA...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>7</td>\n",
              "      <td>NP_005148.2</td>\n",
              "      <td>ABL1(H396P)-phosphorylated</td>\n",
              "      <td>MLEICLKLVGCKSKKGLSSSSSCYLEEALQRPVASDFEPQGLSEAA...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a8b94cc2-e33c-4e41-8435-65d61a7e1f4b')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-a8b94cc2-e33c-4e41-8435-65d61a7e1f4b button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-a8b94cc2-e33c-4e41-8435-65d61a7e1f4b');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-84e4f8ce-54d7-4183-9b76-7a98b7391db2\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-84e4f8ce-54d7-4183-9b76-7a98b7391db2')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-84e4f8ce-54d7-4183-9b76-7a98b7391db2 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "proteins_df",
              "summary": "{\n  \"name\": \"proteins_df\",\n  \"rows\": 433,\n  \"fields\": [\n    {\n      \"column\": \"Protein_Index\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 126,\n        \"min\": 0,\n        \"max\": 441,\n        \"num_unique_values\": 433,\n        \"samples\": [\n          434,\n          83,\n          190\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Accession_Number\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 383,\n        \"samples\": [\n          \"NP_002639.1\",\n          \"CAA47004.1\",\n          \"NP_066958.2\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Gene_Name\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 433,\n        \"samples\": [\n          \"YANK1\",\n          \"CLK4\",\n          \"IRAK3\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Sequence\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 433,\n        \"samples\": [\n          \"MGANTSRKPPVFDENEDVNFDHFEILRAIGKGSFGKVCIVQKNDTKKMYAMKYMNKQKCVERNEVRNVFKELQIMQGLEHPFLVNLWYSFQDEEDMFMVVDLLLGGDLRYHLQQNVHFKEETVKLFICELVMALDYLQNQRIIHRDMKPDNILLDEHGHVHITDFNIAAMLPRETQITTMAGTKPYMAPEMFSSRKGAGYSFAVDWWSLGVTAYELLRGRRPYHIRSSTSSKEIVHTFETTVVTYPSAWSQEMVSLLKKLLEPNPDQRFSQLSDVQNFPYMNDINWDAVFQKRLIPGFIPNKGRLNCDPTFELEEMILESKPLHKKKKRLAKKEKDMRKCDSSQTCLLQEHLDSVQKEFIIFNREKVNRDFNKRQPNLALEQTKDPQGEDGQNNNL\",\n          \"MRHSKRTHCPDWDSRESWGHESYRGSHKRKRRSHSSTQENRHCKPHHQFKESDCHYLEARSLNERDYRDRRYVDEYRNDYCEGYVPRHYHRDIESGYRIHCSKSSVRSRRSSPKRKRNRHCSSHQSRSKSHRRKRSRSIEDDEEGHLICQSGDVLRARYEIVDTLGEGAFGKVVECIDHGMDGMHVAVKIVKNVGRYREAARSEIQVLEHLNSTDPNSVFRCVQMLEWFDHHGHVCIVFELLGLSTYDFIKENSFLPFQIDHIRQMAYQICQSINFLHHNKLTHTDLKPENILFVKSDYVVKYNSKMKRDERTLKNTDIKVVDFGSATYDDEHHSTLVSTRHYRAPEVILALGWSQPCDVWSIGCILIEYYLGFTVFQTHDSKEHLAMMERILGPIPQHMIQKTRKRKYFHHNQLDWDEHSSAGRYVRRRCKPLKEFMLCHDEEHEKLFDLVRRMLEYDPTQRITLDEALQHPFFDLLKKK\",\n          \"MAGNCGARGALSAHTLLFDLPPALLGELCAVLDSCDGALGWRGLAERLSSSWLDVRHIEKYVDQGKSGTRELLWSWAQKNKTIGDLLQVLQEMGHRRAIHLITNYGAVLSPSEKSYQEGGFPNILFKETANVTVDNVLIPEHNEKGVLLKSSISFQNIIEGTRNFHKDFLIGEGEIFEVYRVEIQNLTYAVKLFKQEKKMQCKKHWKRFLSELEVLLLFHHPNILELAAYFTETEKFCLIYPYMRNGTLFDRLQCVGDTAPLPWHIRIGILIGISKAIHYLHNVQPCSVICGSISSANILLDDQFQPKLTDFAMAHFRSHLEHQSCTINMTSSSSKHLWYMPEEYIRQGKLSIKTDVYSFGIVIMEVLTGCRVVLDDPKHIQLRDLLRELMEKRGLDSCLSFLDKKVPPCPRNFSAKLFCLAGRCAATRAKLRPSMDEVLNTLESTQASLYFAEDPPTSLKSFRCPSPLFLENVPSIPVEDDESQNNNLLPSDEGLRIDRMTQKTPFECSQSEVMFLSLDKKPESKRNEEACNMPSSSCEESWFPKYIVPSQDLRPYKVNIDPSSEAPGHSCRSRPVESSCSSKFSWDEYEQYKKE\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "#Define project and data path\n",
        "project_root = \"/content/drive/MyDrive/Rebuilding_and_Modifying_GraphDTA\"\n",
        "data_path = f\"{project_root}/data\"\n",
        "\n",
        "#Reload the proteins.csv\n",
        "proteins_df = pd.read_csv(f\"{data_path}/proteins.csv\")\n",
        "\n",
        "proteins_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Amino acid mapping\n",
        "AMINO_ACIDS = 'ACDEFGHIKLMNPQRSTVWY'\n",
        "AA_TO_IDX = {aa: idx + 1 for idx, aa in enumerate(AMINO_ACIDS)}\n"
      ],
      "metadata": {
        "id": "EtMaiFaxvNCE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Building raw variable-length tensors\n",
        "\n",
        "protein_tensors ={}\n",
        "\n",
        "for _, row in proteins_df.iterrows():\n",
        "  idx = row['Protein_Index']\n",
        "  sequence = row['Sequence']\n",
        "  indices = [AA_TO_IDX.get(aa, 0) for aa in sequence]\n",
        "  protein_tensors[idx] = indices\n"
      ],
      "metadata": {
        "id": "XvcSgzDcvnLw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Save as pickle file for future use\n",
        "\n",
        "import pickle\n",
        "\n",
        "#Save as a dictionary of raw index lists\n",
        "with open(f\"{data_path}/protein_index_sequences.pkl\", \"wb\") as f:\n",
        "  pickle.dump(protein_tensors, f)\n",
        "\n",
        "print(f\"Saved {len(protein_tensors)} raw protein sequences.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1xkXPDJJwLoH",
        "outputId": "eb76bc62-513b-45af-b61e-d43877e1f83c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved 433 raw protein sequences.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "#Define project and data path\n",
        "project_root = \"/content/drive/MyDrive/Rebuilding_and_Modifying_GraphDTA\"\n",
        "data_path = f\"{project_root}/data\"\n",
        "affinity_df = pd.read_csv(f\"{data_path}/drug_protein_affinity.csv\")\n",
        "print(affinity_df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ek0qp0ZexOSn",
        "outputId": "c09b5a11-e70c-4ede-93b0-259014b4411d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Drug_Index  Protein_Index  Affinity\n",
            "0           0              0  7.366532\n",
            "1           0              1  5.000000\n",
            "2           0              3  5.000000\n",
            "3           0              5  5.000000\n",
            "4           0              7  5.000000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Load graph tensors\n",
        "import torch\n",
        "import pickle\n",
        "\n",
        "drug_graphs = torch.load(f\"{data_path}/davis_drugs_graph.pt\", weights_only=False)\n",
        "\n",
        "#Load raw protein sequences\n",
        "with open(f\"{data_path}/protein_index_sequences.pkl\", \"rb\") as f:\n",
        "  protein_sequences = pickle.load(f)\n"
      ],
      "metadata": {
        "id": "0X8PKtngwnjn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e6f25172-70d9-49f7-89b6-ab603280c1fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "A module that was compiled using NumPy 1.x cannot be run in\n",
            "NumPy 2.0.2 as it may crash. To support both 1.x and 2.x\n",
            "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
            "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
            "\n",
            "If you are a user of the module, the easiest solution will be to\n",
            "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
            "We expect that some modules will need time to support NumPy 2.\n",
            "\n",
            "Traceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
            "  File \"<frozen runpy>\", line 88, in _run_code\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/colab_kernel_launcher.py\", line 37, in <module>\n",
            "    ColabKernelApp.launch_instance()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n",
            "    app.start()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelapp.py\", line 712, in start\n",
            "    self.io_loop.start()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/tornado/platform/asyncio.py\", line 205, in start\n",
            "    self.asyncio_loop.run_forever()\n",
            "  File \"/usr/lib/python3.11/asyncio/base_events.py\", line 608, in run_forever\n",
            "    self._run_once()\n",
            "  File \"/usr/lib/python3.11/asyncio/base_events.py\", line 1936, in _run_once\n",
            "    handle._run()\n",
            "  File \"/usr/lib/python3.11/asyncio/events.py\", line 84, in _run\n",
            "    self._context.run(self._callback, *self._args)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 510, in dispatch_queue\n",
            "    await self.process_one()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 499, in process_one\n",
            "    await dispatch(*args)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 406, in dispatch_shell\n",
            "    await result\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 730, in execute_request\n",
            "    reply_content = await reply_content\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/ipkernel.py\", line 383, in do_execute\n",
            "    res = shell.run_cell(\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/zmqshell.py\", line 528, in run_cell\n",
            "    return super().run_cell(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n",
            "    result = self._run_cell(\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n",
            "    return runner(coro)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n",
            "    coro.send(None)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n",
            "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n",
            "    if (await self.run_code(code, result,  async_=asy)):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"<ipython-input-7-153cbfd9e59d>\", line 5, in <cell line: 0>\n",
            "    drug_graphs = torch.load(f\"{data_path}/davis_drugs_graph.pt\", weights_only=False)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/serialization.py\", line 809, in load\n",
            "    return _load(opened_zipfile, map_location, pickle_module, **pickle_load_args)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/serialization.py\", line 1172, in _load\n",
            "    result = unpickler.load()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/serialization.py\", line 1165, in find_class\n",
            "    return super().find_class(mod_name, name)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch_geometric/__init__.py\", line 21, in <module>\n",
            "    import torch_geometric.datasets\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch_geometric/datasets/__init__.py\", line 18, in <module>\n",
            "    from .qm9 import QM9\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch_geometric/datasets/qm9.py\", line 22, in <module>\n",
            "    conversion = torch.tensor([\n",
            "/usr/local/lib/python3.11/dist-packages/torch_geometric/datasets/qm9.py:22: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:84.)\n",
            "  conversion = torch.tensor([\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "\n",
        "class DTADataset(Dataset):\n",
        "    def __init__(self, affinity_df, drug_graphs, protein_sequences):\n",
        "        self.data = []\n",
        "        for _, row in affinity_df.iterrows():\n",
        "            d_idx = row[\"Drug_Index\"]\n",
        "            p_idx = row[\"Protein_Index\"]\n",
        "            y = row[\"Affinity\"]\n",
        "\n",
        "            if d_idx in drug_graphs and p_idx in protein_sequences:\n",
        "                drug_graph = drug_graphs[d_idx]\n",
        "                protein_seq = torch.tensor(protein_sequences[p_idx], dtype=torch.long)\n",
        "                self.data.append((drug_graph, protein_seq, torch.tensor([y], dtype=torch.float)))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx]\n"
      ],
      "metadata": {
        "id": "XoDpFHcdxRb0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Build collate function\n",
        "\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch_geometric.data import Batch\n",
        "\n",
        "def collate_fn(batch):\n",
        "  drug_graphs, protein_seqs, labels = zip(*batch)\n",
        "  drug_batch = Batch.from_data_list(drug_graphs)\n",
        "  padded_proteins = pad_sequence(protein_seqs, batch_first=True, padding_value = 0)\n",
        "  labels = torch.stack(labels)\n",
        "  return drug_batch, padded_proteins, labels"
      ],
      "metadata": {
        "id": "G_g0NQbiPT_n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Create train/val/test and loaders\n",
        "\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "\n",
        "full_dataset = DTADataset(affinity_df, drug_graphs, protein_sequences)\n",
        "train_size = int(0.8*len(full_dataset))\n",
        "val_size = int(0.1*len(full_dataset))\n",
        "test_size = len(full_dataset) - train_size - val_size\n",
        "\n",
        "train_set, val_set, test_set = random_split(full_dataset, [train_size, val_size, test_size])\n",
        "\n",
        "train_loader = DataLoader(train_set, batch_size = 512, shuffle=True, collate_fn=collate_fn)\n",
        "val_loader = DataLoader(val_set, batch_size=512, collate_fn=collate_fn)\n",
        "test_loader = DataLoader(test_set, batch_size=512, collate_fn=collate_fn)\n"
      ],
      "metadata": {
        "id": "E7V2Z1MFSa_3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Output check\n",
        "\n",
        "for drug_batch, protein_batch, affinity in train_loader:\n",
        "  print(\"Drug graph batch:\", drug_batch)\n",
        "  print(\"Protein batch shape:\", protein_batch.shape)\n",
        "  print(\"Affinity batch:\", affinity.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N8tX7nGET4lG",
        "outputId": "c66cdcbe-8906-424c-e36f-ae29991c9c6d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drug graph batch: DataBatch(x=[16279, 83], edge_index=[2, 35944], batch=[16279], ptr=[513])\n",
            "Protein batch shape: torch.Size([512, 2527])\n",
            "Affinity batch: torch.Size([512, 1])\n",
            "Drug graph batch: DataBatch(x=[16510, 83], edge_index=[2, 36556], batch=[16510], ptr=[513])\n",
            "Protein batch shape: torch.Size([512, 2549])\n",
            "Affinity batch: torch.Size([512, 1])\n",
            "Drug graph batch: DataBatch(x=[16262, 83], edge_index=[2, 36030], batch=[16262], ptr=[513])\n",
            "Protein batch shape: torch.Size([512, 2549])\n",
            "Affinity batch: torch.Size([512, 1])\n",
            "Drug graph batch: DataBatch(x=[16391, 83], edge_index=[2, 36184], batch=[16391], ptr=[513])\n",
            "Protein batch shape: torch.Size([512, 2549])\n",
            "Affinity batch: torch.Size([512, 1])\n",
            "Drug graph batch: DataBatch(x=[16513, 83], edge_index=[2, 36578], batch=[16513], ptr=[513])\n",
            "Protein batch shape: torch.Size([512, 2527])\n",
            "Affinity batch: torch.Size([512, 1])\n",
            "Drug graph batch: DataBatch(x=[16400, 83], edge_index=[2, 36284], batch=[16400], ptr=[513])\n",
            "Protein batch shape: torch.Size([512, 2347])\n",
            "Affinity batch: torch.Size([512, 1])\n",
            "Drug graph batch: DataBatch(x=[16515, 83], edge_index=[2, 36520], batch=[16515], ptr=[513])\n",
            "Protein batch shape: torch.Size([512, 2527])\n",
            "Affinity batch: torch.Size([512, 1])\n",
            "Drug graph batch: DataBatch(x=[16679, 83], edge_index=[2, 36854], batch=[16679], ptr=[513])\n",
            "Protein batch shape: torch.Size([512, 2549])\n",
            "Affinity batch: torch.Size([512, 1])\n",
            "Drug graph batch: DataBatch(x=[16480, 83], edge_index=[2, 36360], batch=[16480], ptr=[513])\n",
            "Protein batch shape: torch.Size([512, 2527])\n",
            "Affinity batch: torch.Size([512, 1])\n",
            "Drug graph batch: DataBatch(x=[16652, 83], edge_index=[2, 36862], batch=[16652], ptr=[513])\n",
            "Protein batch shape: torch.Size([512, 2549])\n",
            "Affinity batch: torch.Size([512, 1])\n",
            "Drug graph batch: DataBatch(x=[16575, 83], edge_index=[2, 36624], batch=[16575], ptr=[513])\n",
            "Protein batch shape: torch.Size([512, 2549])\n",
            "Affinity batch: torch.Size([512, 1])\n",
            "Drug graph batch: DataBatch(x=[16489, 83], edge_index=[2, 36522], batch=[16489], ptr=[513])\n",
            "Protein batch shape: torch.Size([512, 2549])\n",
            "Affinity batch: torch.Size([512, 1])\n",
            "Drug graph batch: DataBatch(x=[16301, 83], edge_index=[2, 36052], batch=[16301], ptr=[513])\n",
            "Protein batch shape: torch.Size([512, 2527])\n",
            "Affinity batch: torch.Size([512, 1])\n",
            "Drug graph batch: DataBatch(x=[16481, 83], edge_index=[2, 36458], batch=[16481], ptr=[513])\n",
            "Protein batch shape: torch.Size([512, 2549])\n",
            "Affinity batch: torch.Size([512, 1])\n",
            "Drug graph batch: DataBatch(x=[16248, 83], edge_index=[2, 35894], batch=[16248], ptr=[513])\n",
            "Protein batch shape: torch.Size([512, 2527])\n",
            "Affinity batch: torch.Size([512, 1])\n",
            "Drug graph batch: DataBatch(x=[16258, 83], edge_index=[2, 35862], batch=[16258], ptr=[513])\n",
            "Protein batch shape: torch.Size([512, 2549])\n",
            "Affinity batch: torch.Size([512, 1])\n",
            "Drug graph batch: DataBatch(x=[16385, 83], edge_index=[2, 36276], batch=[16385], ptr=[513])\n",
            "Protein batch shape: torch.Size([512, 2527])\n",
            "Affinity batch: torch.Size([512, 1])\n",
            "Drug graph batch: DataBatch(x=[16474, 83], edge_index=[2, 36402], batch=[16474], ptr=[513])\n",
            "Protein batch shape: torch.Size([512, 2549])\n",
            "Affinity batch: torch.Size([512, 1])\n",
            "Drug graph batch: DataBatch(x=[16179, 83], edge_index=[2, 35810], batch=[16179], ptr=[513])\n",
            "Protein batch shape: torch.Size([512, 2549])\n",
            "Affinity batch: torch.Size([512, 1])\n",
            "Drug graph batch: DataBatch(x=[16398, 83], edge_index=[2, 36344], batch=[16398], ptr=[513])\n",
            "Protein batch shape: torch.Size([512, 2549])\n",
            "Affinity batch: torch.Size([512, 1])\n",
            "Drug graph batch: DataBatch(x=[16504, 83], edge_index=[2, 36520], batch=[16504], ptr=[513])\n",
            "Protein batch shape: torch.Size([512, 2549])\n",
            "Affinity batch: torch.Size([512, 1])\n",
            "Drug graph batch: DataBatch(x=[16134, 83], edge_index=[2, 35696], batch=[16134], ptr=[513])\n",
            "Protein batch shape: torch.Size([512, 2549])\n",
            "Affinity batch: torch.Size([512, 1])\n",
            "Drug graph batch: DataBatch(x=[16503, 83], edge_index=[2, 36568], batch=[16503], ptr=[513])\n",
            "Protein batch shape: torch.Size([512, 2549])\n",
            "Affinity batch: torch.Size([512, 1])\n",
            "Drug graph batch: DataBatch(x=[16499, 83], edge_index=[2, 36452], batch=[16499], ptr=[513])\n",
            "Protein batch shape: torch.Size([512, 2549])\n",
            "Affinity batch: torch.Size([512, 1])\n",
            "Drug graph batch: DataBatch(x=[16359, 83], edge_index=[2, 36182], batch=[16359], ptr=[513])\n",
            "Protein batch shape: torch.Size([512, 2549])\n",
            "Affinity batch: torch.Size([512, 1])\n",
            "Drug graph batch: DataBatch(x=[16197, 83], edge_index=[2, 35816], batch=[16197], ptr=[513])\n",
            "Protein batch shape: torch.Size([512, 2549])\n",
            "Affinity batch: torch.Size([512, 1])\n",
            "Drug graph batch: DataBatch(x=[16344, 83], edge_index=[2, 36178], batch=[16344], ptr=[513])\n",
            "Protein batch shape: torch.Size([512, 2549])\n",
            "Affinity batch: torch.Size([512, 1])\n",
            "Drug graph batch: DataBatch(x=[16437, 83], edge_index=[2, 36406], batch=[16437], ptr=[513])\n",
            "Protein batch shape: torch.Size([512, 2549])\n",
            "Affinity batch: torch.Size([512, 1])\n",
            "Drug graph batch: DataBatch(x=[16565, 83], edge_index=[2, 36630], batch=[16565], ptr=[513])\n",
            "Protein batch shape: torch.Size([512, 2549])\n",
            "Affinity batch: torch.Size([512, 1])\n",
            "Drug graph batch: DataBatch(x=[16278, 83], edge_index=[2, 36004], batch=[16278], ptr=[513])\n",
            "Protein batch shape: torch.Size([512, 2549])\n",
            "Affinity batch: torch.Size([512, 1])\n",
            "Drug graph batch: DataBatch(x=[16322, 83], edge_index=[2, 36058], batch=[16322], ptr=[513])\n",
            "Protein batch shape: torch.Size([512, 2549])\n",
            "Affinity batch: torch.Size([512, 1])\n",
            "Drug graph batch: DataBatch(x=[16401, 83], edge_index=[2, 36326], batch=[16401], ptr=[513])\n",
            "Protein batch shape: torch.Size([512, 2527])\n",
            "Affinity batch: torch.Size([512, 1])\n",
            "Drug graph batch: DataBatch(x=[16406, 83], edge_index=[2, 36320], batch=[16406], ptr=[513])\n",
            "Protein batch shape: torch.Size([512, 2527])\n",
            "Affinity batch: torch.Size([512, 1])\n",
            "Drug graph batch: DataBatch(x=[16372, 83], edge_index=[2, 36172], batch=[16372], ptr=[513])\n",
            "Protein batch shape: torch.Size([512, 2527])\n",
            "Affinity batch: torch.Size([512, 1])\n",
            "Drug graph batch: DataBatch(x=[16676, 83], edge_index=[2, 36896], batch=[16676], ptr=[513])\n",
            "Protein batch shape: torch.Size([512, 2549])\n",
            "Affinity batch: torch.Size([512, 1])\n",
            "Drug graph batch: DataBatch(x=[16471, 83], edge_index=[2, 36410], batch=[16471], ptr=[513])\n",
            "Protein batch shape: torch.Size([512, 2527])\n",
            "Affinity batch: torch.Size([512, 1])\n",
            "Drug graph batch: DataBatch(x=[16512, 83], edge_index=[2, 36516], batch=[16512], ptr=[513])\n",
            "Protein batch shape: torch.Size([512, 2549])\n",
            "Affinity batch: torch.Size([512, 1])\n",
            "Drug graph batch: DataBatch(x=[16318, 83], edge_index=[2, 36082], batch=[16318], ptr=[513])\n",
            "Protein batch shape: torch.Size([512, 2527])\n",
            "Affinity batch: torch.Size([512, 1])\n",
            "Drug graph batch: DataBatch(x=[16429, 83], edge_index=[2, 36284], batch=[16429], ptr=[513])\n",
            "Protein batch shape: torch.Size([512, 2527])\n",
            "Affinity batch: torch.Size([512, 1])\n",
            "Drug graph batch: DataBatch(x=[16427, 83], edge_index=[2, 36334], batch=[16427], ptr=[513])\n",
            "Protein batch shape: torch.Size([512, 2527])\n",
            "Affinity batch: torch.Size([512, 1])\n",
            "Drug graph batch: DataBatch(x=[16384, 83], edge_index=[2, 36142], batch=[16384], ptr=[513])\n",
            "Protein batch shape: torch.Size([512, 2549])\n",
            "Affinity batch: torch.Size([512, 1])\n",
            "Drug graph batch: DataBatch(x=[16484, 83], edge_index=[2, 36468], batch=[16484], ptr=[513])\n",
            "Protein batch shape: torch.Size([512, 2549])\n",
            "Affinity batch: torch.Size([512, 1])\n",
            "Drug graph batch: DataBatch(x=[16434, 83], edge_index=[2, 36354], batch=[16434], ptr=[513])\n",
            "Protein batch shape: torch.Size([512, 2549])\n",
            "Affinity batch: torch.Size([512, 1])\n",
            "Drug graph batch: DataBatch(x=[16330, 83], edge_index=[2, 36110], batch=[16330], ptr=[513])\n",
            "Protein batch shape: torch.Size([512, 2527])\n",
            "Affinity batch: torch.Size([512, 1])\n",
            "Drug graph batch: DataBatch(x=[16504, 83], edge_index=[2, 36452], batch=[16504], ptr=[513])\n",
            "Protein batch shape: torch.Size([512, 2527])\n",
            "Affinity batch: torch.Size([512, 1])\n",
            "Drug graph batch: DataBatch(x=[16305, 83], edge_index=[2, 36040], batch=[16305], ptr=[513])\n",
            "Protein batch shape: torch.Size([512, 2549])\n",
            "Affinity batch: torch.Size([512, 1])\n",
            "Drug graph batch: DataBatch(x=[83, 83], edge_index=[2, 182], batch=[83], ptr=[4])\n",
            "Protein batch shape: torch.Size([3, 2549])\n",
            "Affinity batch: torch.Size([3, 1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#GraphDTA model  (GCN + 1D CNN)\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import GCNConv, global_max_pool\n",
        "\n",
        "class GraphDTA(nn.Module):\n",
        "  def __init__(self, num_atom_features=83, protein_vocab_size=21, protein_embed_dim=128, out_channel=128, kernel_size=8, drug_output_dim=128, protein_output_dim=128, fc_hidden_dims=[1024, 512], dropout_rate=0.2, output_dim=1):\n",
        "\n",
        "    super(GraphDTA, self).__init__()\n",
        "\n",
        "    #Drug encoder\n",
        "    self.gcn1 = GCNConv(num_atom_features, num_atom_features*2)\n",
        "    self.gcn2 = GCNConv(num_atom_features*2, num_atom_features*4)\n",
        "    self.gcn3 = GCNConv(num_atom_features*4, num_atom_features*8)\n",
        "    self.drug_fc1 = nn.Linear(num_atom_features*8, 1024)\n",
        "    self.drug_fc2 = nn.Linear(1024, 512)\n",
        "    self.drug_fc3 = nn.Linear(512, drug_output_dim)\n",
        "    self.relu = nn.ReLU()\n",
        "    self.dropout = nn.Dropout(p=dropout_rate)\n",
        "\n",
        "    #Protein encoder\n",
        "    self.protein_embedding = nn.Embedding(protein_vocab_size, protein_embed_dim, padding_idx=0)\n",
        "    self.conv1_protein = nn.Conv1d(protein_embed_dim, out_channels=out_channel, kernel_size=kernel_size)\n",
        "    self.conv2_protein = nn.Conv1d(out_channel, protein_output_dim, kernel_size=kernel_size)\n",
        "    self.protein_pool = nn.AdaptiveMaxPool1d(1) #output will be [batch_size, protein_output_dim, 1]\n",
        "\n",
        "    #Fusion layers (MLP)\n",
        "    self.fc1_combined = nn.Linear(drug_output_dim+protein_output_dim, fc_hidden_dims[0])\n",
        "    self.dropout1 = nn.Dropout(p=dropout_rate)\n",
        "    self.fc2_combined = nn.Linear(fc_hidden_dims[0], fc_hidden_dims[1])\n",
        "    self.dropout2 = nn.Dropout(p=dropout_rate)\n",
        "    self.out = nn.Linear(fc_hidden_dims[1], output_dim)\n",
        "\n",
        "    self.relu = nn.ReLU()\n",
        "    self.dropout = nn.Dropout(dropout_rate) #General dropout rate\n",
        "\n",
        "  def forward(self, drug_data, protein_seq):\n",
        "    #Drug graph forward\n",
        "    x_drug, edge_index_drug, batch_drug = drug_data.x, drug_data.edge_index, drug_data.batch\n",
        "    x_drug = self.relu(self.gcn1(x_drug, edge_index_drug))\n",
        "    x_drug = self.relu(self.gcn2(x_drug, edge_index_drug))\n",
        "    x_drug = self.relu(self.gcn3(x_drug, edge_index_drug))\n",
        "    drug_emb_pooled = global_max_pool(x_drug, batch_drug)\n",
        "\n",
        "    drug_emb = self.relu(self.drug_fc1(drug_emb_pooled))\n",
        "    drug_emb = self.dropout(drug_emb)\n",
        "    drug_emb = self.relu(self.drug_fc2(drug_emb))\n",
        "    drug_emb = self.dropout(drug_emb)\n",
        "    drug_emb = self.relu(self.drug_fc3(drug_emb))\n",
        "    drug_emb = self.dropout(drug_emb)\n",
        "\n",
        "    #Protein sequence forward\n",
        "    seq_emb = self.protein_embedding(protein_seq)\n",
        "    seq_emb = seq_emb.permute(0, 2, 1)\n",
        "    seq_conv = self.relu(self.conv1_protein(seq_emb))\n",
        "    seq_conv = self.relu(self.conv2_protein(seq_conv))\n",
        "    protein_emb = self.protein_pool(seq_conv).squeeze(-1)\n",
        "\n",
        "    #Fusion\n",
        "    combined_emb = torch.cat((drug_emb, protein_emb), dim = 1)\n",
        "    x_combined = self.relu(self.fc1_combined(combined_emb))\n",
        "    x_combined = self.dropout1(x_combined)\n",
        "    x_combined = self.relu(self.fc2_combined(x_combined))\n",
        "    x_combined = self.dropout2(x_combined)\n",
        "\n",
        "    output = self.out(x_combined)\n",
        "    if output.shape[-1] == 1:\n",
        "      output = output.squeeze(-1)\n",
        "    return output\n"
      ],
      "metadata": {
        "id": "RE8O-7dhUbEp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Define training functions\n",
        "def train(model, device, loader, optimizer, criterion):\n",
        "  model.train()\n",
        "  total_loss = 0\n",
        "  for drug_graph, protein_seq, affinity in loader:\n",
        "    drug_graph = drug_graph.to(device)\n",
        "    protein_seq = protein_seq.to(device)\n",
        "    affinity = affinity.to(device).squeeze()\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    output = model(drug_graph, protein_seq)\n",
        "    loss = criterion(output, affinity)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    total_loss += loss.item()\n",
        "    return total_loss / len(loader)\n"
      ],
      "metadata": {
        "id": "jlYXl7SFXmaV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Define RMSE (PyTorch)\n",
        "def rmse_torch(pred, true):\n",
        "    return torch.sqrt(torch.mean((pred - true) ** 2)).item()\n",
        "\n",
        "#Define CI(PyTorch)\n",
        "def concordance_index_torch(y_true, y_pred):\n",
        "    \"\"\"Returns CI (pure PyTorch)\"\"\"\n",
        "    concordant = 0.0\n",
        "    permissible = 0.0\n",
        "    n = len(y_true)\n",
        "\n",
        "    for i in range(n):\n",
        "        for j in range(i + 1, n):\n",
        "            if y_true[i] != y_true[j]:\n",
        "                permissible += 1\n",
        "                if (y_pred[i] - y_pred[j]) * (y_true[i] - y_true[j]) > 0:\n",
        "                    concordant += 1\n",
        "    return concordant / permissible if permissible != 0 else 0.0\n",
        "\n",
        "\n",
        "#Define evaluation function\n",
        "def evaluate(model, device, val_loader, loader):\n",
        "  model.eval()\n",
        "  y_pred, y_true = [], []\n",
        "  total_val_loss = 0\n",
        "  num_samples = 0\n",
        "  with torch.no_grad():\n",
        "    for drug_graph, protein_seq, affinity in val_loader:\n",
        "      drug_graph = drug_graph.to(device)\n",
        "      protein_seq = protein_seq.to(device)\n",
        "      affinity_label_device = affinity.to(device).squeeze()\n",
        "\n",
        "      output = model(drug_graph, protein_seq)\n",
        "\n",
        "      loss = criterion(output, affinity_label_device)\n",
        "      total_val_loss += loss.item() * affinity_label_device.size(0)\n",
        "      num_samples += affinity_label_device.size(0)\n",
        "\n",
        "      y_pred.extend(output.detach().cpu().tolist())\n",
        "      y_true.extend(affinity.squeeze().tolist())\n",
        "\n",
        "  y_pred_tensor = torch.tensor(y_pred)\n",
        "  y_true_tensor = torch.tensor(y_true)\n",
        "\n",
        "  avg_val_loss = total_val_loss / num_samples\n",
        "\n",
        "  metrics =  {\n",
        "        'val_loss': avg_val_loss,\n",
        "        'rmse': rmse_torch(y_pred_tensor, y_true_tensor),\n",
        "        'ci': concordance_index_torch(y_true, y_pred)\n",
        "    }\n",
        "  return metrics"
      ],
      "metadata": {
        "id": "xXcb_YHUX3nF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Initialize model, optimizer and loader\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = GraphDTA(num_atom_features=83, protein_vocab_size=21, protein_embed_dim=128, out_channel=128, kernel_size=8, drug_output_dim=128, protein_output_dim=128, fc_hidden_dims=[1024, 512], dropout_rate=0.2, output_dim=1).to(device)\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0005)"
      ],
      "metadata": {
        "id": "3lTHr_8EZzPc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Train the model\n",
        "\n",
        "for epoch in range(1000):\n",
        "  train_loss = train(model, device, train_loader, optimizer, criterion)\n",
        "  val_metrics = evaluate(model, device, val_loader, criterion)\n",
        "  print(f\"Epoch {epoch:02d} | Loss: {train_loss: .4f} | Val RMSE: {val_metrics['rmse']:.4f} | CI: {val_metrics['ci']:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bnwCT-AzbgZG",
        "outputId": "bda87802-1d73-45fe-edc4-16e3a62e3b46"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 00 | Loss:  0.0152 | Val RMSE: 0.8810 | CI: 0.6361\n",
            "Epoch 01 | Loss:  0.0181 | Val RMSE: 0.8796 | CI: 0.6367\n",
            "Epoch 02 | Loss:  0.0162 | Val RMSE: 0.8792 | CI: 0.6373\n",
            "Epoch 03 | Loss:  0.0162 | Val RMSE: 0.8807 | CI: 0.6380\n",
            "Epoch 04 | Loss:  0.0126 | Val RMSE: 0.8872 | CI: 0.6387\n",
            "Epoch 05 | Loss:  0.0154 | Val RMSE: 0.8953 | CI: 0.6393\n",
            "Epoch 06 | Loss:  0.0205 | Val RMSE: 0.8914 | CI: 0.6399\n",
            "Epoch 07 | Loss:  0.0191 | Val RMSE: 0.8821 | CI: 0.6404\n",
            "Epoch 08 | Loss:  0.0167 | Val RMSE: 0.8777 | CI: 0.6410\n",
            "Epoch 09 | Loss:  0.0159 | Val RMSE: 0.8793 | CI: 0.6415\n",
            "Epoch 10 | Loss:  0.0152 | Val RMSE: 0.8787 | CI: 0.6421\n",
            "Epoch 11 | Loss:  0.0201 | Val RMSE: 0.8774 | CI: 0.6426\n",
            "Epoch 12 | Loss:  0.0170 | Val RMSE: 0.8784 | CI: 0.6431\n",
            "Epoch 13 | Loss:  0.0179 | Val RMSE: 0.8821 | CI: 0.6436\n",
            "Epoch 14 | Loss:  0.0167 | Val RMSE: 0.8855 | CI: 0.6439\n",
            "Epoch 15 | Loss:  0.0151 | Val RMSE: 0.8888 | CI: 0.6444\n",
            "Epoch 16 | Loss:  0.0139 | Val RMSE: 0.8877 | CI: 0.6446\n",
            "Epoch 17 | Loss:  0.0163 | Val RMSE: 0.8818 | CI: 0.6449\n",
            "Epoch 18 | Loss:  0.0174 | Val RMSE: 0.8773 | CI: 0.6452\n",
            "Epoch 19 | Loss:  0.0151 | Val RMSE: 0.8759 | CI: 0.6457\n",
            "Epoch 20 | Loss:  0.0158 | Val RMSE: 0.8759 | CI: 0.6459\n",
            "Epoch 21 | Loss:  0.0148 | Val RMSE: 0.8760 | CI: 0.6463\n",
            "Epoch 22 | Loss:  0.0150 | Val RMSE: 0.8789 | CI: 0.6467\n",
            "Epoch 23 | Loss:  0.0163 | Val RMSE: 0.8851 | CI: 0.6470\n",
            "Epoch 24 | Loss:  0.0162 | Val RMSE: 0.8877 | CI: 0.6473\n",
            "Epoch 25 | Loss:  0.0179 | Val RMSE: 0.8831 | CI: 0.6476\n",
            "Epoch 26 | Loss:  0.0146 | Val RMSE: 0.8793 | CI: 0.6478\n",
            "Epoch 27 | Loss:  0.0184 | Val RMSE: 0.8757 | CI: 0.6481\n",
            "Epoch 28 | Loss:  0.0182 | Val RMSE: 0.8752 | CI: 0.6484\n",
            "Epoch 29 | Loss:  0.0156 | Val RMSE: 0.8761 | CI: 0.6488\n",
            "Epoch 30 | Loss:  0.0165 | Val RMSE: 0.8747 | CI: 0.6493\n",
            "Epoch 31 | Loss:  0.0196 | Val RMSE: 0.8748 | CI: 0.6497\n",
            "Epoch 32 | Loss:  0.0218 | Val RMSE: 0.8753 | CI: 0.6502\n",
            "Epoch 33 | Loss:  0.0183 | Val RMSE: 0.8760 | CI: 0.6508\n",
            "Epoch 34 | Loss:  0.0187 | Val RMSE: 0.8777 | CI: 0.6514\n",
            "Epoch 35 | Loss:  0.0159 | Val RMSE: 0.8813 | CI: 0.6520\n",
            "Epoch 36 | Loss:  0.0172 | Val RMSE: 0.8810 | CI: 0.6525\n",
            "Epoch 37 | Loss:  0.0186 | Val RMSE: 0.8753 | CI: 0.6529\n",
            "Epoch 38 | Loss:  0.0148 | Val RMSE: 0.8738 | CI: 0.6533\n",
            "Epoch 39 | Loss:  0.0167 | Val RMSE: 0.8734 | CI: 0.6534\n",
            "Epoch 40 | Loss:  0.0139 | Val RMSE: 0.8747 | CI: 0.6538\n",
            "Epoch 41 | Loss:  0.0170 | Val RMSE: 0.8784 | CI: 0.6542\n",
            "Epoch 42 | Loss:  0.0156 | Val RMSE: 0.8831 | CI: 0.6545\n",
            "Epoch 43 | Loss:  0.0159 | Val RMSE: 0.8829 | CI: 0.6548\n",
            "Epoch 44 | Loss:  0.0162 | Val RMSE: 0.8768 | CI: 0.6552\n",
            "Epoch 45 | Loss:  0.0167 | Val RMSE: 0.8733 | CI: 0.6554\n",
            "Epoch 46 | Loss:  0.0172 | Val RMSE: 0.8731 | CI: 0.6559\n",
            "Epoch 47 | Loss:  0.0155 | Val RMSE: 0.8730 | CI: 0.6566\n",
            "Epoch 48 | Loss:  0.0162 | Val RMSE: 0.8747 | CI: 0.6572\n",
            "Epoch 49 | Loss:  0.0136 | Val RMSE: 0.8774 | CI: 0.6577\n",
            "Epoch 50 | Loss:  0.0155 | Val RMSE: 0.8785 | CI: 0.6583\n",
            "Epoch 51 | Loss:  0.0156 | Val RMSE: 0.8754 | CI: 0.6594\n",
            "Epoch 52 | Loss:  0.0140 | Val RMSE: 0.8736 | CI: 0.6602\n",
            "Epoch 53 | Loss:  0.0129 | Val RMSE: 0.8737 | CI: 0.6611\n",
            "Epoch 54 | Loss:  0.0166 | Val RMSE: 0.8730 | CI: 0.6625\n",
            "Epoch 55 | Loss:  0.0176 | Val RMSE: 0.8716 | CI: 0.6641\n",
            "Epoch 56 | Loss:  0.0155 | Val RMSE: 0.8704 | CI: 0.6661\n",
            "Epoch 57 | Loss:  0.0159 | Val RMSE: 0.8709 | CI: 0.6678\n",
            "Epoch 58 | Loss:  0.0159 | Val RMSE: 0.8698 | CI: 0.6701\n",
            "Epoch 59 | Loss:  0.0125 | Val RMSE: 0.8696 | CI: 0.6729\n",
            "Epoch 60 | Loss:  0.0151 | Val RMSE: 0.8671 | CI: 0.6764\n",
            "Epoch 61 | Loss:  0.0163 | Val RMSE: 0.8646 | CI: 0.6798\n",
            "Epoch 62 | Loss:  0.0140 | Val RMSE: 0.8652 | CI: 0.6838\n",
            "Epoch 63 | Loss:  0.0141 | Val RMSE: 0.8673 | CI: 0.6876\n",
            "Epoch 64 | Loss:  0.0181 | Val RMSE: 0.8630 | CI: 0.6922\n",
            "Epoch 65 | Loss:  0.0172 | Val RMSE: 0.8568 | CI: 0.6962\n",
            "Epoch 66 | Loss:  0.0142 | Val RMSE: 0.8550 | CI: 0.7013\n",
            "Epoch 67 | Loss:  0.0164 | Val RMSE: 0.8539 | CI: 0.7065\n",
            "Epoch 68 | Loss:  0.0139 | Val RMSE: 0.8582 | CI: 0.7094\n",
            "Epoch 69 | Loss:  0.0145 | Val RMSE: 0.8545 | CI: 0.7142\n",
            "Epoch 70 | Loss:  0.0183 | Val RMSE: 0.8409 | CI: 0.7216\n",
            "Epoch 71 | Loss:  0.0136 | Val RMSE: 0.8379 | CI: 0.7250\n",
            "Epoch 72 | Loss:  0.0150 | Val RMSE: 0.8402 | CI: 0.7235\n",
            "Epoch 73 | Loss:  0.0133 | Val RMSE: 0.8466 | CI: 0.7219\n",
            "Epoch 74 | Loss:  0.0157 | Val RMSE: 0.8309 | CI: 0.7280\n",
            "Epoch 75 | Loss:  0.0171 | Val RMSE: 0.8424 | CI: 0.7339\n",
            "Epoch 76 | Loss:  0.0135 | Val RMSE: 0.8349 | CI: 0.7274\n",
            "Epoch 77 | Loss:  0.0138 | Val RMSE: 0.8846 | CI: 0.7157\n",
            "Epoch 78 | Loss:  0.0159 | Val RMSE: 0.8795 | CI: 0.7137\n",
            "Epoch 79 | Loss:  0.0183 | Val RMSE: 0.8362 | CI: 0.7218\n",
            "Epoch 80 | Loss:  0.0175 | Val RMSE: 0.8509 | CI: 0.7319\n",
            "Epoch 81 | Loss:  0.0142 | Val RMSE: 0.8397 | CI: 0.7268\n",
            "Epoch 82 | Loss:  0.0152 | Val RMSE: 0.8424 | CI: 0.7139\n",
            "Epoch 83 | Loss:  0.0146 | Val RMSE: 0.8751 | CI: 0.7045\n",
            "Epoch 84 | Loss:  0.0171 | Val RMSE: 0.8747 | CI: 0.7083\n",
            "Epoch 85 | Loss:  0.0140 | Val RMSE: 0.8485 | CI: 0.7201\n",
            "Epoch 86 | Loss:  0.0140 | Val RMSE: 0.8249 | CI: 0.7327\n",
            "Epoch 87 | Loss:  0.0136 | Val RMSE: 0.8363 | CI: 0.7399\n",
            "Epoch 88 | Loss:  0.0153 | Val RMSE: 0.8215 | CI: 0.7378\n",
            "Epoch 89 | Loss:  0.0143 | Val RMSE: 0.8389 | CI: 0.7306\n",
            "Epoch 90 | Loss:  0.0126 | Val RMSE: 0.8597 | CI: 0.7285\n",
            "Epoch 91 | Loss:  0.0116 | Val RMSE: 0.8511 | CI: 0.7319\n",
            "Epoch 92 | Loss:  0.0192 | Val RMSE: 0.8166 | CI: 0.7415\n",
            "Epoch 93 | Loss:  0.0150 | Val RMSE: 0.8289 | CI: 0.7477\n",
            "Epoch 94 | Loss:  0.0129 | Val RMSE: 0.8113 | CI: 0.7455\n",
            "Epoch 95 | Loss:  0.0152 | Val RMSE: 0.8377 | CI: 0.7398\n",
            "Epoch 96 | Loss:  0.0153 | Val RMSE: 0.8424 | CI: 0.7395\n",
            "Epoch 97 | Loss:  0.0118 | Val RMSE: 0.8264 | CI: 0.7439\n",
            "Epoch 98 | Loss:  0.0168 | Val RMSE: 0.8134 | CI: 0.7469\n",
            "Epoch 99 | Loss:  0.0117 | Val RMSE: 0.8103 | CI: 0.7484\n",
            "Epoch 100 | Loss:  0.0148 | Val RMSE: 0.8090 | CI: 0.7493\n",
            "Epoch 101 | Loss:  0.0132 | Val RMSE: 0.8095 | CI: 0.7495\n",
            "Epoch 102 | Loss:  0.0138 | Val RMSE: 0.8122 | CI: 0.7501\n",
            "Epoch 103 | Loss:  0.0147 | Val RMSE: 0.8076 | CI: 0.7515\n",
            "Epoch 104 | Loss:  0.0153 | Val RMSE: 0.8019 | CI: 0.7537\n",
            "Epoch 105 | Loss:  0.0149 | Val RMSE: 0.7998 | CI: 0.7538\n",
            "Epoch 106 | Loss:  0.0129 | Val RMSE: 0.8067 | CI: 0.7535\n",
            "Epoch 107 | Loss:  0.0142 | Val RMSE: 0.8021 | CI: 0.7546\n",
            "Epoch 108 | Loss:  0.0146 | Val RMSE: 0.7963 | CI: 0.7557\n",
            "Epoch 109 | Loss:  0.0120 | Val RMSE: 0.8059 | CI: 0.7523\n",
            "Epoch 110 | Loss:  0.0135 | Val RMSE: 0.8027 | CI: 0.7515\n",
            "Epoch 111 | Loss:  0.0129 | Val RMSE: 0.7945 | CI: 0.7534\n",
            "Epoch 112 | Loss:  0.0114 | Val RMSE: 0.8005 | CI: 0.7534\n",
            "Epoch 113 | Loss:  0.0126 | Val RMSE: 0.7930 | CI: 0.7548\n",
            "Epoch 114 | Loss:  0.0142 | Val RMSE: 0.8013 | CI: 0.7550\n",
            "Epoch 115 | Loss:  0.0112 | Val RMSE: 0.7937 | CI: 0.7576\n",
            "Epoch 116 | Loss:  0.0138 | Val RMSE: 0.7906 | CI: 0.7594\n",
            "Epoch 117 | Loss:  0.0133 | Val RMSE: 0.7898 | CI: 0.7615\n",
            "Epoch 118 | Loss:  0.0106 | Val RMSE: 0.8146 | CI: 0.7535\n",
            "Epoch 119 | Loss:  0.0180 | Val RMSE: 0.7910 | CI: 0.7582\n",
            "Epoch 120 | Loss:  0.0155 | Val RMSE: 0.8104 | CI: 0.7616\n",
            "Epoch 121 | Loss:  0.0149 | Val RMSE: 0.7913 | CI: 0.7564\n",
            "Epoch 122 | Loss:  0.0127 | Val RMSE: 0.8561 | CI: 0.7377\n",
            "Epoch 123 | Loss:  0.0166 | Val RMSE: 0.8164 | CI: 0.7462\n",
            "Epoch 124 | Loss:  0.0174 | Val RMSE: 0.8010 | CI: 0.7548\n",
            "Epoch 125 | Loss:  0.0140 | Val RMSE: 0.8070 | CI: 0.7547\n",
            "Epoch 126 | Loss:  0.0160 | Val RMSE: 0.7919 | CI: 0.7533\n",
            "Epoch 127 | Loss:  0.0110 | Val RMSE: 0.8130 | CI: 0.7500\n",
            "Epoch 128 | Loss:  0.0144 | Val RMSE: 0.7935 | CI: 0.7571\n",
            "Epoch 129 | Loss:  0.0123 | Val RMSE: 0.7861 | CI: 0.7605\n",
            "Epoch 130 | Loss:  0.0122 | Val RMSE: 0.7903 | CI: 0.7608\n",
            "Epoch 131 | Loss:  0.0158 | Val RMSE: 0.7918 | CI: 0.7613\n",
            "Epoch 132 | Loss:  0.0161 | Val RMSE: 0.7910 | CI: 0.7612\n",
            "Epoch 133 | Loss:  0.0124 | Val RMSE: 0.8043 | CI: 0.7585\n",
            "Epoch 134 | Loss:  0.0111 | Val RMSE: 0.8041 | CI: 0.7579\n",
            "Epoch 135 | Loss:  0.0140 | Val RMSE: 0.7831 | CI: 0.7607\n",
            "Epoch 136 | Loss:  0.0124 | Val RMSE: 0.7993 | CI: 0.7605\n",
            "Epoch 137 | Loss:  0.0147 | Val RMSE: 0.7872 | CI: 0.7595\n",
            "Epoch 138 | Loss:  0.0131 | Val RMSE: 0.8146 | CI: 0.7559\n",
            "Epoch 139 | Loss:  0.0140 | Val RMSE: 0.8219 | CI: 0.7553\n",
            "Epoch 140 | Loss:  0.0111 | Val RMSE: 0.7980 | CI: 0.7592\n",
            "Epoch 141 | Loss:  0.0135 | Val RMSE: 0.7958 | CI: 0.7618\n",
            "Epoch 142 | Loss:  0.0123 | Val RMSE: 0.7911 | CI: 0.7632\n",
            "Epoch 143 | Loss:  0.0129 | Val RMSE: 0.7914 | CI: 0.7638\n",
            "Epoch 144 | Loss:  0.0123 | Val RMSE: 0.8148 | CI: 0.7628\n",
            "Epoch 145 | Loss:  0.0167 | Val RMSE: 0.7853 | CI: 0.7665\n",
            "Epoch 146 | Loss:  0.0136 | Val RMSE: 0.7865 | CI: 0.7679\n",
            "Epoch 147 | Loss:  0.0122 | Val RMSE: 0.7829 | CI: 0.7679\n",
            "Epoch 148 | Loss:  0.0135 | Val RMSE: 0.7848 | CI: 0.7671\n",
            "Epoch 149 | Loss:  0.0144 | Val RMSE: 0.7957 | CI: 0.7654\n",
            "Epoch 150 | Loss:  0.0148 | Val RMSE: 0.8023 | CI: 0.7639\n",
            "Epoch 151 | Loss:  0.0103 | Val RMSE: 0.7923 | CI: 0.7655\n",
            "Epoch 152 | Loss:  0.0118 | Val RMSE: 0.7821 | CI: 0.7675\n",
            "Epoch 153 | Loss:  0.0130 | Val RMSE: 0.7822 | CI: 0.7679\n",
            "Epoch 154 | Loss:  0.0120 | Val RMSE: 0.7873 | CI: 0.7659\n",
            "Epoch 155 | Loss:  0.0101 | Val RMSE: 0.8128 | CI: 0.7633\n",
            "Epoch 156 | Loss:  0.0142 | Val RMSE: 0.7901 | CI: 0.7673\n",
            "Epoch 157 | Loss:  0.0129 | Val RMSE: 0.7801 | CI: 0.7699\n",
            "Epoch 158 | Loss:  0.0118 | Val RMSE: 0.7797 | CI: 0.7705\n",
            "Epoch 159 | Loss:  0.0130 | Val RMSE: 0.7873 | CI: 0.7695\n",
            "Epoch 160 | Loss:  0.0147 | Val RMSE: 0.7949 | CI: 0.7687\n",
            "Epoch 161 | Loss:  0.0125 | Val RMSE: 0.7886 | CI: 0.7685\n",
            "Epoch 162 | Loss:  0.0147 | Val RMSE: 0.7812 | CI: 0.7678\n",
            "Epoch 163 | Loss:  0.0101 | Val RMSE: 0.7797 | CI: 0.7684\n",
            "Epoch 164 | Loss:  0.0132 | Val RMSE: 0.7818 | CI: 0.7689\n",
            "Epoch 165 | Loss:  0.0136 | Val RMSE: 0.7928 | CI: 0.7673\n",
            "Epoch 166 | Loss:  0.0113 | Val RMSE: 0.7869 | CI: 0.7673\n",
            "Epoch 167 | Loss:  0.0136 | Val RMSE: 0.7757 | CI: 0.7695\n",
            "Epoch 168 | Loss:  0.0127 | Val RMSE: 0.7778 | CI: 0.7710\n",
            "Epoch 169 | Loss:  0.0128 | Val RMSE: 0.7744 | CI: 0.7711\n",
            "Epoch 170 | Loss:  0.0127 | Val RMSE: 0.7784 | CI: 0.7698\n",
            "Epoch 171 | Loss:  0.0138 | Val RMSE: 0.7885 | CI: 0.7684\n",
            "Epoch 172 | Loss:  0.0132 | Val RMSE: 0.7788 | CI: 0.7690\n",
            "Epoch 173 | Loss:  0.0106 | Val RMSE: 0.7735 | CI: 0.7706\n",
            "Epoch 174 | Loss:  0.0103 | Val RMSE: 0.7745 | CI: 0.7700\n",
            "Epoch 175 | Loss:  0.0120 | Val RMSE: 0.7820 | CI: 0.7691\n",
            "Epoch 176 | Loss:  0.0131 | Val RMSE: 0.7782 | CI: 0.7693\n",
            "Epoch 177 | Loss:  0.0129 | Val RMSE: 0.7808 | CI: 0.7689\n",
            "Epoch 178 | Loss:  0.0147 | Val RMSE: 0.7874 | CI: 0.7686\n",
            "Epoch 179 | Loss:  0.0110 | Val RMSE: 0.7925 | CI: 0.7684\n",
            "Epoch 180 | Loss:  0.0137 | Val RMSE: 0.8200 | CI: 0.7655\n",
            "Epoch 181 | Loss:  0.0138 | Val RMSE: 0.7811 | CI: 0.7665\n",
            "Epoch 182 | Loss:  0.0139 | Val RMSE: 0.7887 | CI: 0.7673\n",
            "Epoch 183 | Loss:  0.0144 | Val RMSE: 0.7886 | CI: 0.7672\n",
            "Epoch 184 | Loss:  0.0130 | Val RMSE: 0.7833 | CI: 0.7650\n",
            "Epoch 185 | Loss:  0.0142 | Val RMSE: 0.7984 | CI: 0.7641\n",
            "Epoch 186 | Loss:  0.0145 | Val RMSE: 0.8012 | CI: 0.7654\n",
            "Epoch 187 | Loss:  0.0111 | Val RMSE: 0.7931 | CI: 0.7675\n",
            "Epoch 188 | Loss:  0.0134 | Val RMSE: 0.7800 | CI: 0.7698\n",
            "Epoch 189 | Loss:  0.0133 | Val RMSE: 0.7777 | CI: 0.7714\n",
            "Epoch 190 | Loss:  0.0152 | Val RMSE: 0.7777 | CI: 0.7731\n",
            "Epoch 191 | Loss:  0.0156 | Val RMSE: 0.7768 | CI: 0.7735\n",
            "Epoch 192 | Loss:  0.0151 | Val RMSE: 0.7826 | CI: 0.7723\n",
            "Epoch 193 | Loss:  0.0140 | Val RMSE: 0.7890 | CI: 0.7716\n",
            "Epoch 194 | Loss:  0.0128 | Val RMSE: 0.7855 | CI: 0.7710\n",
            "Epoch 195 | Loss:  0.0134 | Val RMSE: 0.7773 | CI: 0.7709\n",
            "Epoch 196 | Loss:  0.0120 | Val RMSE: 0.7741 | CI: 0.7708\n",
            "Epoch 197 | Loss:  0.0092 | Val RMSE: 0.7811 | CI: 0.7692\n",
            "Epoch 198 | Loss:  0.0143 | Val RMSE: 0.7753 | CI: 0.7700\n",
            "Epoch 199 | Loss:  0.0129 | Val RMSE: 0.7734 | CI: 0.7702\n",
            "Epoch 200 | Loss:  0.0120 | Val RMSE: 0.7778 | CI: 0.7687\n",
            "Epoch 201 | Loss:  0.0115 | Val RMSE: 0.7808 | CI: 0.7702\n",
            "Epoch 202 | Loss:  0.0133 | Val RMSE: 0.7733 | CI: 0.7733\n",
            "Epoch 203 | Loss:  0.0138 | Val RMSE: 0.7745 | CI: 0.7736\n",
            "Epoch 204 | Loss:  0.0122 | Val RMSE: 0.7751 | CI: 0.7743\n",
            "Epoch 205 | Loss:  0.0117 | Val RMSE: 0.7759 | CI: 0.7751\n",
            "Epoch 206 | Loss:  0.0125 | Val RMSE: 0.7784 | CI: 0.7750\n",
            "Epoch 207 | Loss:  0.0147 | Val RMSE: 0.7740 | CI: 0.7752\n",
            "Epoch 208 | Loss:  0.0124 | Val RMSE: 0.7761 | CI: 0.7743\n",
            "Epoch 209 | Loss:  0.0125 | Val RMSE: 0.7864 | CI: 0.7712\n",
            "Epoch 210 | Loss:  0.0108 | Val RMSE: 0.7770 | CI: 0.7714\n",
            "Epoch 211 | Loss:  0.0105 | Val RMSE: 0.7785 | CI: 0.7703\n",
            "Epoch 212 | Loss:  0.0124 | Val RMSE: 0.7772 | CI: 0.7717\n",
            "Epoch 213 | Loss:  0.0123 | Val RMSE: 0.7757 | CI: 0.7736\n",
            "Epoch 214 | Loss:  0.0135 | Val RMSE: 0.7725 | CI: 0.7760\n",
            "Epoch 215 | Loss:  0.0113 | Val RMSE: 0.7740 | CI: 0.7767\n",
            "Epoch 216 | Loss:  0.0112 | Val RMSE: 0.7804 | CI: 0.7768\n",
            "Epoch 217 | Loss:  0.0122 | Val RMSE: 0.7844 | CI: 0.7771\n",
            "Epoch 218 | Loss:  0.0107 | Val RMSE: 0.7808 | CI: 0.7776\n",
            "Epoch 219 | Loss:  0.0116 | Val RMSE: 0.7754 | CI: 0.7782\n",
            "Epoch 220 | Loss:  0.0138 | Val RMSE: 0.7736 | CI: 0.7789\n",
            "Epoch 221 | Loss:  0.0135 | Val RMSE: 0.7739 | CI: 0.7786\n",
            "Epoch 222 | Loss:  0.0091 | Val RMSE: 0.7823 | CI: 0.7755\n",
            "Epoch 223 | Loss:  0.0134 | Val RMSE: 0.7917 | CI: 0.7732\n",
            "Epoch 224 | Loss:  0.0099 | Val RMSE: 0.7806 | CI: 0.7732\n",
            "Epoch 225 | Loss:  0.0125 | Val RMSE: 0.7747 | CI: 0.7748\n",
            "Epoch 226 | Loss:  0.0113 | Val RMSE: 0.7751 | CI: 0.7743\n",
            "Epoch 227 | Loss:  0.0132 | Val RMSE: 0.7773 | CI: 0.7716\n",
            "Epoch 228 | Loss:  0.0116 | Val RMSE: 0.7981 | CI: 0.7691\n",
            "Epoch 229 | Loss:  0.0139 | Val RMSE: 0.7827 | CI: 0.7713\n",
            "Epoch 230 | Loss:  0.0114 | Val RMSE: 0.7743 | CI: 0.7740\n",
            "Epoch 231 | Loss:  0.0132 | Val RMSE: 0.7762 | CI: 0.7771\n",
            "Epoch 232 | Loss:  0.0116 | Val RMSE: 0.7729 | CI: 0.7787\n",
            "Epoch 233 | Loss:  0.0139 | Val RMSE: 0.7792 | CI: 0.7799\n",
            "Epoch 234 | Loss:  0.0130 | Val RMSE: 0.7731 | CI: 0.7823\n",
            "Epoch 235 | Loss:  0.0142 | Val RMSE: 0.7714 | CI: 0.7839\n",
            "Epoch 236 | Loss:  0.0143 | Val RMSE: 0.7801 | CI: 0.7855\n",
            "Epoch 237 | Loss:  0.0122 | Val RMSE: 0.7743 | CI: 0.7828\n",
            "Epoch 238 | Loss:  0.0111 | Val RMSE: 0.8096 | CI: 0.7797\n",
            "Epoch 239 | Loss:  0.0114 | Val RMSE: 0.7790 | CI: 0.7801\n",
            "Epoch 240 | Loss:  0.0130 | Val RMSE: 0.7859 | CI: 0.7837\n",
            "Epoch 241 | Loss:  0.0146 | Val RMSE: 0.7872 | CI: 0.7833\n",
            "Epoch 242 | Loss:  0.0155 | Val RMSE: 0.7727 | CI: 0.7804\n",
            "Epoch 243 | Loss:  0.0106 | Val RMSE: 0.8314 | CI: 0.7780\n",
            "Epoch 244 | Loss:  0.0149 | Val RMSE: 0.8163 | CI: 0.7792\n",
            "Epoch 245 | Loss:  0.0156 | Val RMSE: 0.7692 | CI: 0.7829\n",
            "Epoch 246 | Loss:  0.0160 | Val RMSE: 0.8071 | CI: 0.7860\n",
            "Epoch 247 | Loss:  0.0149 | Val RMSE: 0.7683 | CI: 0.7830\n",
            "Epoch 248 | Loss:  0.0105 | Val RMSE: 0.8153 | CI: 0.7792\n",
            "Epoch 249 | Loss:  0.0132 | Val RMSE: 0.8159 | CI: 0.7783\n",
            "Epoch 250 | Loss:  0.0124 | Val RMSE: 0.7703 | CI: 0.7802\n",
            "Epoch 251 | Loss:  0.0109 | Val RMSE: 0.7763 | CI: 0.7820\n",
            "Epoch 252 | Loss:  0.0106 | Val RMSE: 0.7719 | CI: 0.7817\n",
            "Epoch 253 | Loss:  0.0109 | Val RMSE: 0.7751 | CI: 0.7797\n",
            "Epoch 254 | Loss:  0.0128 | Val RMSE: 0.7850 | CI: 0.7795\n",
            "Epoch 255 | Loss:  0.0122 | Val RMSE: 0.7858 | CI: 0.7798\n",
            "Epoch 256 | Loss:  0.0141 | Val RMSE: 0.7709 | CI: 0.7807\n",
            "Epoch 257 | Loss:  0.0115 | Val RMSE: 0.7724 | CI: 0.7817\n",
            "Epoch 258 | Loss:  0.0121 | Val RMSE: 0.7732 | CI: 0.7814\n",
            "Epoch 259 | Loss:  0.0113 | Val RMSE: 0.7700 | CI: 0.7802\n",
            "Epoch 260 | Loss:  0.0125 | Val RMSE: 0.7810 | CI: 0.7789\n",
            "Epoch 261 | Loss:  0.0144 | Val RMSE: 0.7836 | CI: 0.7783\n",
            "Epoch 262 | Loss:  0.0098 | Val RMSE: 0.7735 | CI: 0.7783\n",
            "Epoch 263 | Loss:  0.0120 | Val RMSE: 0.7716 | CI: 0.7780\n",
            "Epoch 264 | Loss:  0.0102 | Val RMSE: 0.7742 | CI: 0.7767\n",
            "Epoch 265 | Loss:  0.0114 | Val RMSE: 0.7791 | CI: 0.7764\n",
            "Epoch 266 | Loss:  0.0156 | Val RMSE: 0.7718 | CI: 0.7769\n",
            "Epoch 267 | Loss:  0.0128 | Val RMSE: 0.7723 | CI: 0.7771\n",
            "Epoch 268 | Loss:  0.0133 | Val RMSE: 0.7714 | CI: 0.7768\n",
            "Epoch 269 | Loss:  0.0124 | Val RMSE: 0.7789 | CI: 0.7756\n",
            "Epoch 270 | Loss:  0.0137 | Val RMSE: 0.7857 | CI: 0.7747\n",
            "Epoch 271 | Loss:  0.0133 | Val RMSE: 0.7780 | CI: 0.7753\n",
            "Epoch 272 | Loss:  0.0097 | Val RMSE: 0.7727 | CI: 0.7766\n",
            "Epoch 273 | Loss:  0.0123 | Val RMSE: 0.7724 | CI: 0.7779\n",
            "Epoch 274 | Loss:  0.0123 | Val RMSE: 0.7718 | CI: 0.7791\n",
            "Epoch 275 | Loss:  0.0120 | Val RMSE: 0.7735 | CI: 0.7795\n",
            "Epoch 276 | Loss:  0.0120 | Val RMSE: 0.7788 | CI: 0.7801\n",
            "Epoch 277 | Loss:  0.0151 | Val RMSE: 0.7719 | CI: 0.7814\n",
            "Epoch 278 | Loss:  0.0100 | Val RMSE: 0.7729 | CI: 0.7821\n",
            "Epoch 279 | Loss:  0.0120 | Val RMSE: 0.7753 | CI: 0.7823\n",
            "Epoch 280 | Loss:  0.0113 | Val RMSE: 0.7855 | CI: 0.7823\n",
            "Epoch 281 | Loss:  0.0148 | Val RMSE: 0.7722 | CI: 0.7833\n",
            "Epoch 282 | Loss:  0.0106 | Val RMSE: 0.7697 | CI: 0.7841\n",
            "Epoch 283 | Loss:  0.0108 | Val RMSE: 0.7701 | CI: 0.7840\n",
            "Epoch 284 | Loss:  0.0123 | Val RMSE: 0.7724 | CI: 0.7834\n",
            "Epoch 285 | Loss:  0.0147 | Val RMSE: 0.7682 | CI: 0.7831\n",
            "Epoch 286 | Loss:  0.0092 | Val RMSE: 0.7690 | CI: 0.7822\n",
            "Epoch 287 | Loss:  0.0116 | Val RMSE: 0.7716 | CI: 0.7817\n",
            "Epoch 288 | Loss:  0.0126 | Val RMSE: 0.7725 | CI: 0.7818\n",
            "Epoch 289 | Loss:  0.0161 | Val RMSE: 0.7697 | CI: 0.7822\n",
            "Epoch 290 | Loss:  0.0160 | Val RMSE: 0.7687 | CI: 0.7838\n",
            "Epoch 291 | Loss:  0.0104 | Val RMSE: 0.7671 | CI: 0.7834\n",
            "Epoch 292 | Loss:  0.0138 | Val RMSE: 0.7701 | CI: 0.7837\n",
            "Epoch 293 | Loss:  0.0117 | Val RMSE: 0.7706 | CI: 0.7844\n",
            "Epoch 294 | Loss:  0.0127 | Val RMSE: 0.7677 | CI: 0.7851\n",
            "Epoch 295 | Loss:  0.0151 | Val RMSE: 0.7663 | CI: 0.7860\n",
            "Epoch 296 | Loss:  0.0129 | Val RMSE: 0.7679 | CI: 0.7860\n",
            "Epoch 297 | Loss:  0.0109 | Val RMSE: 0.7774 | CI: 0.7846\n",
            "Epoch 298 | Loss:  0.0110 | Val RMSE: 0.7962 | CI: 0.7831\n",
            "Epoch 299 | Loss:  0.0139 | Val RMSE: 0.7689 | CI: 0.7840\n",
            "Epoch 300 | Loss:  0.0116 | Val RMSE: 0.7684 | CI: 0.7836\n",
            "Epoch 301 | Loss:  0.0110 | Val RMSE: 0.7681 | CI: 0.7821\n",
            "Epoch 302 | Loss:  0.0111 | Val RMSE: 0.7729 | CI: 0.7799\n",
            "Epoch 303 | Loss:  0.0151 | Val RMSE: 0.7724 | CI: 0.7796\n",
            "Epoch 304 | Loss:  0.0114 | Val RMSE: 0.7685 | CI: 0.7810\n",
            "Epoch 305 | Loss:  0.0109 | Val RMSE: 0.7690 | CI: 0.7808\n",
            "Epoch 306 | Loss:  0.0161 | Val RMSE: 0.7689 | CI: 0.7812\n",
            "Epoch 307 | Loss:  0.0128 | Val RMSE: 0.7699 | CI: 0.7807\n",
            "Epoch 308 | Loss:  0.0123 | Val RMSE: 0.7704 | CI: 0.7805\n",
            "Epoch 309 | Loss:  0.0096 | Val RMSE: 0.7816 | CI: 0.7801\n",
            "Epoch 310 | Loss:  0.0141 | Val RMSE: 0.7867 | CI: 0.7806\n",
            "Epoch 311 | Loss:  0.0110 | Val RMSE: 0.7752 | CI: 0.7830\n",
            "Epoch 312 | Loss:  0.0100 | Val RMSE: 0.7686 | CI: 0.7858\n",
            "Epoch 313 | Loss:  0.0131 | Val RMSE: 0.7674 | CI: 0.7869\n",
            "Epoch 314 | Loss:  0.0135 | Val RMSE: 0.7765 | CI: 0.7866\n",
            "Epoch 315 | Loss:  0.0121 | Val RMSE: 0.7852 | CI: 0.7859\n",
            "Epoch 316 | Loss:  0.0110 | Val RMSE: 0.7690 | CI: 0.7875\n",
            "Epoch 317 | Loss:  0.0118 | Val RMSE: 0.7640 | CI: 0.7881\n",
            "Epoch 318 | Loss:  0.0122 | Val RMSE: 0.7697 | CI: 0.7857\n",
            "Epoch 319 | Loss:  0.0099 | Val RMSE: 0.7844 | CI: 0.7828\n",
            "Epoch 320 | Loss:  0.0120 | Val RMSE: 0.7756 | CI: 0.7822\n",
            "Epoch 321 | Loss:  0.0106 | Val RMSE: 0.7742 | CI: 0.7812\n",
            "Epoch 322 | Loss:  0.0126 | Val RMSE: 0.7718 | CI: 0.7816\n",
            "Epoch 323 | Loss:  0.0126 | Val RMSE: 0.7714 | CI: 0.7835\n",
            "Epoch 324 | Loss:  0.0103 | Val RMSE: 0.7731 | CI: 0.7836\n",
            "Epoch 325 | Loss:  0.0110 | Val RMSE: 0.7820 | CI: 0.7850\n",
            "Epoch 326 | Loss:  0.0124 | Val RMSE: 0.7729 | CI: 0.7871\n",
            "Epoch 327 | Loss:  0.0138 | Val RMSE: 0.7636 | CI: 0.7893\n",
            "Epoch 328 | Loss:  0.0133 | Val RMSE: 0.7640 | CI: 0.7899\n",
            "Epoch 329 | Loss:  0.0152 | Val RMSE: 0.7674 | CI: 0.7887\n",
            "Epoch 330 | Loss:  0.0129 | Val RMSE: 0.7775 | CI: 0.7880\n",
            "Epoch 331 | Loss:  0.0112 | Val RMSE: 0.7720 | CI: 0.7880\n",
            "Epoch 332 | Loss:  0.0136 | Val RMSE: 0.7681 | CI: 0.7887\n",
            "Epoch 333 | Loss:  0.0114 | Val RMSE: 0.7650 | CI: 0.7871\n",
            "Epoch 334 | Loss:  0.0110 | Val RMSE: 0.7993 | CI: 0.7819\n",
            "Epoch 335 | Loss:  0.0117 | Val RMSE: 0.8077 | CI: 0.7789\n",
            "Epoch 336 | Loss:  0.0146 | Val RMSE: 0.7681 | CI: 0.7805\n",
            "Epoch 337 | Loss:  0.0125 | Val RMSE: 0.7807 | CI: 0.7808\n",
            "Epoch 338 | Loss:  0.0142 | Val RMSE: 0.7702 | CI: 0.7794\n",
            "Epoch 339 | Loss:  0.0107 | Val RMSE: 0.7780 | CI: 0.7790\n",
            "Epoch 340 | Loss:  0.0122 | Val RMSE: 0.7975 | CI: 0.7798\n",
            "Epoch 341 | Loss:  0.0125 | Val RMSE: 0.7854 | CI: 0.7821\n",
            "Epoch 342 | Loss:  0.0106 | Val RMSE: 0.7653 | CI: 0.7852\n",
            "Epoch 343 | Loss:  0.0119 | Val RMSE: 0.7691 | CI: 0.7883\n",
            "Epoch 344 | Loss:  0.0124 | Val RMSE: 0.7653 | CI: 0.7893\n",
            "Epoch 345 | Loss:  0.0149 | Val RMSE: 0.7831 | CI: 0.7873\n",
            "Epoch 346 | Loss:  0.0121 | Val RMSE: 0.7840 | CI: 0.7877\n",
            "Epoch 347 | Loss:  0.0119 | Val RMSE: 0.7665 | CI: 0.7890\n",
            "Epoch 348 | Loss:  0.0116 | Val RMSE: 0.7626 | CI: 0.7900\n",
            "Epoch 349 | Loss:  0.0119 | Val RMSE: 0.7623 | CI: 0.7913\n",
            "Epoch 350 | Loss:  0.0118 | Val RMSE: 0.7645 | CI: 0.7909\n",
            "Epoch 351 | Loss:  0.0139 | Val RMSE: 0.7683 | CI: 0.7912\n",
            "Epoch 352 | Loss:  0.0093 | Val RMSE: 0.7707 | CI: 0.7919\n",
            "Epoch 353 | Loss:  0.0126 | Val RMSE: 0.7614 | CI: 0.7934\n",
            "Epoch 354 | Loss:  0.0122 | Val RMSE: 0.7610 | CI: 0.7941\n",
            "Epoch 355 | Loss:  0.0123 | Val RMSE: 0.7684 | CI: 0.7918\n",
            "Epoch 356 | Loss:  0.0124 | Val RMSE: 0.7849 | CI: 0.7900\n",
            "Epoch 357 | Loss:  0.0123 | Val RMSE: 0.7656 | CI: 0.7906\n",
            "Epoch 358 | Loss:  0.0101 | Val RMSE: 0.7616 | CI: 0.7907\n",
            "Epoch 359 | Loss:  0.0118 | Val RMSE: 0.7617 | CI: 0.7896\n",
            "Epoch 360 | Loss:  0.0130 | Val RMSE: 0.7662 | CI: 0.7875\n",
            "Epoch 361 | Loss:  0.0124 | Val RMSE: 0.7682 | CI: 0.7864\n",
            "Epoch 362 | Loss:  0.0112 | Val RMSE: 0.7732 | CI: 0.7848\n",
            "Epoch 363 | Loss:  0.0094 | Val RMSE: 0.7780 | CI: 0.7831\n",
            "Epoch 364 | Loss:  0.0160 | Val RMSE: 0.7661 | CI: 0.7852\n",
            "Epoch 365 | Loss:  0.0135 | Val RMSE: 0.7679 | CI: 0.7873\n",
            "Epoch 366 | Loss:  0.0130 | Val RMSE: 0.7656 | CI: 0.7872\n",
            "Epoch 367 | Loss:  0.0147 | Val RMSE: 0.7727 | CI: 0.7881\n",
            "Epoch 368 | Loss:  0.0113 | Val RMSE: 0.7759 | CI: 0.7892\n",
            "Epoch 369 | Loss:  0.0139 | Val RMSE: 0.7724 | CI: 0.7898\n",
            "Epoch 370 | Loss:  0.0126 | Val RMSE: 0.7728 | CI: 0.7901\n",
            "Epoch 371 | Loss:  0.0133 | Val RMSE: 0.7794 | CI: 0.7886\n",
            "Epoch 372 | Loss:  0.0101 | Val RMSE: 0.8069 | CI: 0.7855\n",
            "Epoch 373 | Loss:  0.0113 | Val RMSE: 0.7746 | CI: 0.7849\n",
            "Epoch 374 | Loss:  0.0117 | Val RMSE: 0.7756 | CI: 0.7866\n",
            "Epoch 375 | Loss:  0.0129 | Val RMSE: 0.7691 | CI: 0.7852\n",
            "Epoch 376 | Loss:  0.0122 | Val RMSE: 0.7775 | CI: 0.7802\n",
            "Epoch 377 | Loss:  0.0127 | Val RMSE: 0.7979 | CI: 0.7781\n",
            "Epoch 378 | Loss:  0.0109 | Val RMSE: 0.7769 | CI: 0.7799\n",
            "Epoch 379 | Loss:  0.0118 | Val RMSE: 0.7661 | CI: 0.7832\n",
            "Epoch 380 | Loss:  0.0112 | Val RMSE: 0.7656 | CI: 0.7853\n",
            "Epoch 381 | Loss:  0.0118 | Val RMSE: 0.7636 | CI: 0.7865\n",
            "Epoch 382 | Loss:  0.0123 | Val RMSE: 0.7669 | CI: 0.7879\n",
            "Epoch 383 | Loss:  0.0119 | Val RMSE: 0.7641 | CI: 0.7899\n",
            "Epoch 384 | Loss:  0.0125 | Val RMSE: 0.7665 | CI: 0.7910\n",
            "Epoch 385 | Loss:  0.0122 | Val RMSE: 0.7812 | CI: 0.7912\n",
            "Epoch 386 | Loss:  0.0127 | Val RMSE: 0.7753 | CI: 0.7912\n",
            "Epoch 387 | Loss:  0.0123 | Val RMSE: 0.7645 | CI: 0.7910\n",
            "Epoch 388 | Loss:  0.0117 | Val RMSE: 0.7657 | CI: 0.7904\n",
            "Epoch 389 | Loss:  0.0121 | Val RMSE: 0.7717 | CI: 0.7878\n",
            "Epoch 390 | Loss:  0.0124 | Val RMSE: 0.7859 | CI: 0.7851\n",
            "Epoch 391 | Loss:  0.0122 | Val RMSE: 0.7703 | CI: 0.7844\n",
            "Epoch 392 | Loss:  0.0091 | Val RMSE: 0.7654 | CI: 0.7841\n",
            "Epoch 393 | Loss:  0.0122 | Val RMSE: 0.7652 | CI: 0.7844\n",
            "Epoch 394 | Loss:  0.0139 | Val RMSE: 0.7657 | CI: 0.7852\n",
            "Epoch 395 | Loss:  0.0119 | Val RMSE: 0.7729 | CI: 0.7843\n",
            "Epoch 396 | Loss:  0.0120 | Val RMSE: 0.7788 | CI: 0.7848\n",
            "Epoch 397 | Loss:  0.0136 | Val RMSE: 0.7650 | CI: 0.7870\n",
            "Epoch 398 | Loss:  0.0109 | Val RMSE: 0.7738 | CI: 0.7896\n",
            "Epoch 399 | Loss:  0.0123 | Val RMSE: 0.7654 | CI: 0.7881\n",
            "Epoch 400 | Loss:  0.0123 | Val RMSE: 0.7926 | CI: 0.7868\n",
            "Epoch 401 | Loss:  0.0137 | Val RMSE: 0.7820 | CI: 0.7869\n",
            "Epoch 402 | Loss:  0.0132 | Val RMSE: 0.7642 | CI: 0.7891\n",
            "Epoch 403 | Loss:  0.0143 | Val RMSE: 0.7661 | CI: 0.7890\n",
            "Epoch 404 | Loss:  0.0112 | Val RMSE: 0.7703 | CI: 0.7870\n",
            "Epoch 405 | Loss:  0.0145 | Val RMSE: 0.7710 | CI: 0.7870\n",
            "Epoch 406 | Loss:  0.0145 | Val RMSE: 0.7614 | CI: 0.7873\n",
            "Epoch 407 | Loss:  0.0122 | Val RMSE: 0.7609 | CI: 0.7879\n",
            "Epoch 408 | Loss:  0.0130 | Val RMSE: 0.7622 | CI: 0.7887\n",
            "Epoch 409 | Loss:  0.0108 | Val RMSE: 0.7676 | CI: 0.7870\n",
            "Epoch 410 | Loss:  0.0133 | Val RMSE: 0.7725 | CI: 0.7868\n",
            "Epoch 411 | Loss:  0.0113 | Val RMSE: 0.7610 | CI: 0.7883\n",
            "Epoch 412 | Loss:  0.0130 | Val RMSE: 0.7600 | CI: 0.7892\n",
            "Epoch 413 | Loss:  0.0121 | Val RMSE: 0.7593 | CI: 0.7871\n",
            "Epoch 414 | Loss:  0.0100 | Val RMSE: 0.7802 | CI: 0.7840\n",
            "Epoch 415 | Loss:  0.0102 | Val RMSE: 0.7926 | CI: 0.7822\n",
            "Epoch 416 | Loss:  0.0161 | Val RMSE: 0.7601 | CI: 0.7851\n",
            "Epoch 417 | Loss:  0.0103 | Val RMSE: 0.7620 | CI: 0.7855\n",
            "Epoch 418 | Loss:  0.0125 | Val RMSE: 0.7636 | CI: 0.7845\n",
            "Epoch 419 | Loss:  0.0076 | Val RMSE: 0.7973 | CI: 0.7832\n",
            "Epoch 420 | Loss:  0.0124 | Val RMSE: 0.7847 | CI: 0.7848\n",
            "Epoch 421 | Loss:  0.0127 | Val RMSE: 0.7592 | CI: 0.7881\n",
            "Epoch 422 | Loss:  0.0134 | Val RMSE: 0.7779 | CI: 0.7892\n",
            "Epoch 423 | Loss:  0.0130 | Val RMSE: 0.7595 | CI: 0.7862\n",
            "Epoch 424 | Loss:  0.0102 | Val RMSE: 0.7926 | CI: 0.7811\n",
            "Epoch 425 | Loss:  0.0110 | Val RMSE: 0.7669 | CI: 0.7808\n",
            "Epoch 426 | Loss:  0.0099 | Val RMSE: 0.7620 | CI: 0.7814\n",
            "Epoch 427 | Loss:  0.0116 | Val RMSE: 0.7620 | CI: 0.7799\n",
            "Epoch 428 | Loss:  0.0106 | Val RMSE: 0.7754 | CI: 0.7801\n",
            "Epoch 429 | Loss:  0.0088 | Val RMSE: 0.7798 | CI: 0.7822\n",
            "Epoch 430 | Loss:  0.0106 | Val RMSE: 0.7657 | CI: 0.7848\n",
            "Epoch 431 | Loss:  0.0130 | Val RMSE: 0.7670 | CI: 0.7872\n",
            "Epoch 432 | Loss:  0.0125 | Val RMSE: 0.7620 | CI: 0.7880\n",
            "Epoch 433 | Loss:  0.0126 | Val RMSE: 0.7618 | CI: 0.7888\n",
            "Epoch 434 | Loss:  0.0108 | Val RMSE: 0.7727 | CI: 0.7886\n",
            "Epoch 435 | Loss:  0.0117 | Val RMSE: 0.7675 | CI: 0.7890\n",
            "Epoch 436 | Loss:  0.0145 | Val RMSE: 0.7604 | CI: 0.7896\n",
            "Epoch 437 | Loss:  0.0118 | Val RMSE: 0.7565 | CI: 0.7884\n",
            "Epoch 438 | Loss:  0.0151 | Val RMSE: 0.7652 | CI: 0.7862\n",
            "Epoch 439 | Loss:  0.0083 | Val RMSE: 0.7767 | CI: 0.7838\n",
            "Epoch 440 | Loss:  0.0097 | Val RMSE: 0.7705 | CI: 0.7819\n",
            "Epoch 441 | Loss:  0.0139 | Val RMSE: 0.7627 | CI: 0.7812\n",
            "Epoch 442 | Loss:  0.0101 | Val RMSE: 0.7597 | CI: 0.7828\n",
            "Epoch 443 | Loss:  0.0102 | Val RMSE: 0.7578 | CI: 0.7856\n",
            "Epoch 444 | Loss:  0.0123 | Val RMSE: 0.7524 | CI: 0.7910\n",
            "Epoch 445 | Loss:  0.0109 | Val RMSE: 0.7616 | CI: 0.7924\n",
            "Epoch 446 | Loss:  0.0109 | Val RMSE: 0.7920 | CI: 0.7928\n",
            "Epoch 447 | Loss:  0.0118 | Val RMSE: 0.7666 | CI: 0.7930\n",
            "Epoch 448 | Loss:  0.0098 | Val RMSE: 0.7554 | CI: 0.7928\n",
            "Epoch 449 | Loss:  0.0115 | Val RMSE: 0.7535 | CI: 0.7903\n",
            "Epoch 450 | Loss:  0.0119 | Val RMSE: 0.7586 | CI: 0.7877\n",
            "Epoch 451 | Loss:  0.0093 | Val RMSE: 0.7669 | CI: 0.7858\n",
            "Epoch 452 | Loss:  0.0123 | Val RMSE: 0.7580 | CI: 0.7852\n",
            "Epoch 453 | Loss:  0.0113 | Val RMSE: 0.7532 | CI: 0.7855\n",
            "Epoch 454 | Loss:  0.0115 | Val RMSE: 0.7520 | CI: 0.7862\n",
            "Epoch 455 | Loss:  0.0113 | Val RMSE: 0.7553 | CI: 0.7869\n",
            "Epoch 456 | Loss:  0.0124 | Val RMSE: 0.7549 | CI: 0.7878\n",
            "Epoch 457 | Loss:  0.0134 | Val RMSE: 0.7485 | CI: 0.7892\n",
            "Epoch 458 | Loss:  0.0108 | Val RMSE: 0.7483 | CI: 0.7892\n",
            "Epoch 459 | Loss:  0.0103 | Val RMSE: 0.7704 | CI: 0.7877\n",
            "Epoch 460 | Loss:  0.0086 | Val RMSE: 0.7746 | CI: 0.7874\n",
            "Epoch 461 | Loss:  0.0119 | Val RMSE: 0.7482 | CI: 0.7899\n",
            "Epoch 462 | Loss:  0.0115 | Val RMSE: 0.7503 | CI: 0.7909\n",
            "Epoch 463 | Loss:  0.0114 | Val RMSE: 0.7465 | CI: 0.7905\n",
            "Epoch 464 | Loss:  0.0129 | Val RMSE: 0.7484 | CI: 0.7903\n",
            "Epoch 465 | Loss:  0.0093 | Val RMSE: 0.7666 | CI: 0.7894\n",
            "Epoch 466 | Loss:  0.0093 | Val RMSE: 0.7612 | CI: 0.7886\n",
            "Epoch 467 | Loss:  0.0093 | Val RMSE: 0.7490 | CI: 0.7872\n",
            "Epoch 468 | Loss:  0.0109 | Val RMSE: 0.7447 | CI: 0.7863\n",
            "Epoch 469 | Loss:  0.0128 | Val RMSE: 0.7446 | CI: 0.7839\n",
            "Epoch 470 | Loss:  0.0125 | Val RMSE: 0.7486 | CI: 0.7825\n",
            "Epoch 471 | Loss:  0.0118 | Val RMSE: 0.7483 | CI: 0.7821\n",
            "Epoch 472 | Loss:  0.0121 | Val RMSE: 0.7529 | CI: 0.7828\n",
            "Epoch 473 | Loss:  0.0092 | Val RMSE: 0.7547 | CI: 0.7834\n",
            "Epoch 474 | Loss:  0.0132 | Val RMSE: 0.7693 | CI: 0.7829\n",
            "Epoch 475 | Loss:  0.0119 | Val RMSE: 0.7444 | CI: 0.7840\n",
            "Epoch 476 | Loss:  0.0093 | Val RMSE: 0.7441 | CI: 0.7834\n",
            "Epoch 477 | Loss:  0.0126 | Val RMSE: 0.7441 | CI: 0.7833\n",
            "Epoch 478 | Loss:  0.0125 | Val RMSE: 0.7439 | CI: 0.7826\n",
            "Epoch 479 | Loss:  0.0104 | Val RMSE: 0.7594 | CI: 0.7815\n",
            "Epoch 480 | Loss:  0.0078 | Val RMSE: 0.7731 | CI: 0.7806\n",
            "Epoch 481 | Loss:  0.0113 | Val RMSE: 0.7433 | CI: 0.7837\n",
            "Epoch 482 | Loss:  0.0130 | Val RMSE: 0.7712 | CI: 0.7872\n",
            "Epoch 483 | Loss:  0.0124 | Val RMSE: 0.7451 | CI: 0.7889\n",
            "Epoch 484 | Loss:  0.0138 | Val RMSE: 0.7803 | CI: 0.7880\n",
            "Epoch 485 | Loss:  0.0140 | Val RMSE: 0.7440 | CI: 0.7890\n",
            "Epoch 486 | Loss:  0.0097 | Val RMSE: 0.7557 | CI: 0.7892\n",
            "Epoch 487 | Loss:  0.0155 | Val RMSE: 0.7381 | CI: 0.7886\n",
            "Epoch 488 | Loss:  0.0122 | Val RMSE: 0.7762 | CI: 0.7852\n",
            "Epoch 489 | Loss:  0.0112 | Val RMSE: 0.7533 | CI: 0.7856\n",
            "Epoch 490 | Loss:  0.0121 | Val RMSE: 0.7390 | CI: 0.7866\n",
            "Epoch 491 | Loss:  0.0104 | Val RMSE: 0.7451 | CI: 0.7872\n",
            "Epoch 492 | Loss:  0.0120 | Val RMSE: 0.7486 | CI: 0.7873\n",
            "Epoch 493 | Loss:  0.0103 | Val RMSE: 0.7720 | CI: 0.7864\n",
            "Epoch 494 | Loss:  0.0111 | Val RMSE: 0.7483 | CI: 0.7855\n",
            "Epoch 495 | Loss:  0.0119 | Val RMSE: 0.7507 | CI: 0.7833\n",
            "Epoch 496 | Loss:  0.0099 | Val RMSE: 0.7472 | CI: 0.7839\n",
            "Epoch 497 | Loss:  0.0116 | Val RMSE: 0.7446 | CI: 0.7824\n",
            "Epoch 498 | Loss:  0.0101 | Val RMSE: 0.7850 | CI: 0.7759\n",
            "Epoch 499 | Loss:  0.0112 | Val RMSE: 0.7577 | CI: 0.7789\n",
            "Epoch 500 | Loss:  0.0097 | Val RMSE: 0.7424 | CI: 0.7816\n",
            "Epoch 501 | Loss:  0.0111 | Val RMSE: 0.7435 | CI: 0.7841\n",
            "Epoch 502 | Loss:  0.0103 | Val RMSE: 0.7414 | CI: 0.7850\n",
            "Epoch 503 | Loss:  0.0117 | Val RMSE: 0.7595 | CI: 0.7859\n",
            "Epoch 504 | Loss:  0.0109 | Val RMSE: 0.7551 | CI: 0.7859\n",
            "Epoch 505 | Loss:  0.0094 | Val RMSE: 0.7404 | CI: 0.7850\n",
            "Epoch 506 | Loss:  0.0114 | Val RMSE: 0.7486 | CI: 0.7848\n",
            "Epoch 507 | Loss:  0.0099 | Val RMSE: 0.7386 | CI: 0.7854\n",
            "Epoch 508 | Loss:  0.0121 | Val RMSE: 0.7612 | CI: 0.7836\n",
            "Epoch 509 | Loss:  0.0138 | Val RMSE: 0.7368 | CI: 0.7879\n",
            "Epoch 510 | Loss:  0.0101 | Val RMSE: 0.7362 | CI: 0.7888\n",
            "Epoch 511 | Loss:  0.0097 | Val RMSE: 0.7462 | CI: 0.7894\n",
            "Epoch 512 | Loss:  0.0109 | Val RMSE: 0.7612 | CI: 0.7885\n",
            "Epoch 513 | Loss:  0.0111 | Val RMSE: 0.7371 | CI: 0.7884\n",
            "Epoch 514 | Loss:  0.0093 | Val RMSE: 0.7349 | CI: 0.7878\n",
            "Epoch 515 | Loss:  0.0103 | Val RMSE: 0.7391 | CI: 0.7846\n",
            "Epoch 516 | Loss:  0.0095 | Val RMSE: 0.7447 | CI: 0.7816\n",
            "Epoch 517 | Loss:  0.0106 | Val RMSE: 0.7354 | CI: 0.7836\n",
            "Epoch 518 | Loss:  0.0110 | Val RMSE: 0.7343 | CI: 0.7839\n",
            "Epoch 519 | Loss:  0.0086 | Val RMSE: 0.7438 | CI: 0.7833\n",
            "Epoch 520 | Loss:  0.0130 | Val RMSE: 0.7358 | CI: 0.7845\n",
            "Epoch 521 | Loss:  0.0106 | Val RMSE: 0.7377 | CI: 0.7838\n",
            "Epoch 522 | Loss:  0.0119 | Val RMSE: 0.7351 | CI: 0.7843\n",
            "Epoch 523 | Loss:  0.0095 | Val RMSE: 0.7324 | CI: 0.7845\n",
            "Epoch 524 | Loss:  0.0105 | Val RMSE: 0.7351 | CI: 0.7839\n",
            "Epoch 525 | Loss:  0.0108 | Val RMSE: 0.7359 | CI: 0.7838\n",
            "Epoch 526 | Loss:  0.0111 | Val RMSE: 0.7362 | CI: 0.7838\n",
            "Epoch 527 | Loss:  0.0105 | Val RMSE: 0.7344 | CI: 0.7856\n",
            "Epoch 528 | Loss:  0.0109 | Val RMSE: 0.7326 | CI: 0.7875\n",
            "Epoch 529 | Loss:  0.0112 | Val RMSE: 0.7335 | CI: 0.7875\n",
            "Epoch 530 | Loss:  0.0114 | Val RMSE: 0.7400 | CI: 0.7870\n",
            "Epoch 531 | Loss:  0.0121 | Val RMSE: 0.7394 | CI: 0.7866\n",
            "Epoch 532 | Loss:  0.0125 | Val RMSE: 0.7331 | CI: 0.7865\n",
            "Epoch 533 | Loss:  0.0126 | Val RMSE: 0.7354 | CI: 0.7847\n",
            "Epoch 534 | Loss:  0.0089 | Val RMSE: 0.7632 | CI: 0.7818\n",
            "Epoch 535 | Loss:  0.0099 | Val RMSE: 0.7435 | CI: 0.7876\n",
            "Epoch 536 | Loss:  0.0097 | Val RMSE: 0.7311 | CI: 0.7895\n",
            "Epoch 537 | Loss:  0.0125 | Val RMSE: 0.7500 | CI: 0.7876\n",
            "Epoch 538 | Loss:  0.0121 | Val RMSE: 0.7332 | CI: 0.7889\n",
            "Epoch 539 | Loss:  0.0129 | Val RMSE: 0.7403 | CI: 0.7896\n",
            "Epoch 540 | Loss:  0.0104 | Val RMSE: 0.7323 | CI: 0.7896\n",
            "Epoch 541 | Loss:  0.0121 | Val RMSE: 0.7307 | CI: 0.7893\n",
            "Epoch 542 | Loss:  0.0118 | Val RMSE: 0.7297 | CI: 0.7890\n",
            "Epoch 543 | Loss:  0.0096 | Val RMSE: 0.7372 | CI: 0.7884\n",
            "Epoch 544 | Loss:  0.0125 | Val RMSE: 0.7319 | CI: 0.7887\n",
            "Epoch 545 | Loss:  0.0105 | Val RMSE: 0.7296 | CI: 0.7885\n",
            "Epoch 546 | Loss:  0.0112 | Val RMSE: 0.7364 | CI: 0.7856\n",
            "Epoch 547 | Loss:  0.0107 | Val RMSE: 0.7362 | CI: 0.7827\n",
            "Epoch 548 | Loss:  0.0124 | Val RMSE: 0.7340 | CI: 0.7836\n",
            "Epoch 549 | Loss:  0.0112 | Val RMSE: 0.7320 | CI: 0.7843\n",
            "Epoch 550 | Loss:  0.0136 | Val RMSE: 0.7460 | CI: 0.7824\n",
            "Epoch 551 | Loss:  0.0097 | Val RMSE: 0.7525 | CI: 0.7858\n",
            "Epoch 552 | Loss:  0.0128 | Val RMSE: 0.7323 | CI: 0.7876\n",
            "Epoch 553 | Loss:  0.0099 | Val RMSE: 0.7356 | CI: 0.7875\n",
            "Epoch 554 | Loss:  0.0088 | Val RMSE: 0.7488 | CI: 0.7872\n",
            "Epoch 555 | Loss:  0.0108 | Val RMSE: 0.7442 | CI: 0.7870\n",
            "Epoch 556 | Loss:  0.0103 | Val RMSE: 0.7394 | CI: 0.7862\n",
            "Epoch 557 | Loss:  0.0107 | Val RMSE: 0.7515 | CI: 0.7843\n",
            "Epoch 558 | Loss:  0.0109 | Val RMSE: 0.7428 | CI: 0.7832\n",
            "Epoch 559 | Loss:  0.0111 | Val RMSE: 0.7347 | CI: 0.7890\n",
            "Epoch 560 | Loss:  0.0105 | Val RMSE: 0.7365 | CI: 0.7914\n",
            "Epoch 561 | Loss:  0.0131 | Val RMSE: 0.7470 | CI: 0.7912\n",
            "Epoch 562 | Loss:  0.0091 | Val RMSE: 0.7603 | CI: 0.7902\n",
            "Epoch 563 | Loss:  0.0101 | Val RMSE: 0.7328 | CI: 0.7902\n",
            "Epoch 564 | Loss:  0.0109 | Val RMSE: 0.7347 | CI: 0.7865\n",
            "Epoch 565 | Loss:  0.0096 | Val RMSE: 0.7382 | CI: 0.7835\n",
            "Epoch 566 | Loss:  0.0098 | Val RMSE: 0.7532 | CI: 0.7836\n",
            "Epoch 567 | Loss:  0.0102 | Val RMSE: 0.7564 | CI: 0.7854\n",
            "Epoch 568 | Loss:  0.0094 | Val RMSE: 0.7359 | CI: 0.7873\n",
            "Epoch 569 | Loss:  0.0137 | Val RMSE: 0.7433 | CI: 0.7880\n",
            "Epoch 570 | Loss:  0.0132 | Val RMSE: 0.7292 | CI: 0.7878\n",
            "Epoch 571 | Loss:  0.0105 | Val RMSE: 0.7395 | CI: 0.7881\n",
            "Epoch 572 | Loss:  0.0083 | Val RMSE: 0.7751 | CI: 0.7856\n",
            "Epoch 573 | Loss:  0.0106 | Val RMSE: 0.7410 | CI: 0.7836\n",
            "Epoch 574 | Loss:  0.0117 | Val RMSE: 0.7527 | CI: 0.7860\n",
            "Epoch 575 | Loss:  0.0104 | Val RMSE: 0.7287 | CI: 0.7843\n",
            "Epoch 576 | Loss:  0.0124 | Val RMSE: 0.7486 | CI: 0.7832\n",
            "Epoch 577 | Loss:  0.0109 | Val RMSE: 0.7477 | CI: 0.7863\n",
            "Epoch 578 | Loss:  0.0107 | Val RMSE: 0.7322 | CI: 0.7894\n",
            "Epoch 579 | Loss:  0.0131 | Val RMSE: 0.7447 | CI: 0.7889\n",
            "Epoch 580 | Loss:  0.0127 | Val RMSE: 0.7364 | CI: 0.7846\n",
            "Epoch 581 | Loss:  0.0104 | Val RMSE: 0.7609 | CI: 0.7800\n",
            "Epoch 582 | Loss:  0.0109 | Val RMSE: 0.7503 | CI: 0.7777\n",
            "Epoch 583 | Loss:  0.0139 | Val RMSE: 0.7393 | CI: 0.7839\n",
            "Epoch 584 | Loss:  0.0108 | Val RMSE: 0.7485 | CI: 0.7858\n",
            "Epoch 585 | Loss:  0.0108 | Val RMSE: 0.7375 | CI: 0.7878\n",
            "Epoch 586 | Loss:  0.0113 | Val RMSE: 0.7712 | CI: 0.7888\n",
            "Epoch 587 | Loss:  0.0090 | Val RMSE: 0.7633 | CI: 0.7891\n",
            "Epoch 588 | Loss:  0.0114 | Val RMSE: 0.7252 | CI: 0.7886\n",
            "Epoch 589 | Loss:  0.0128 | Val RMSE: 0.7724 | CI: 0.7842\n",
            "Epoch 590 | Loss:  0.0121 | Val RMSE: 0.7334 | CI: 0.7795\n",
            "Epoch 591 | Loss:  0.0104 | Val RMSE: 0.7744 | CI: 0.7798\n",
            "Epoch 592 | Loss:  0.0130 | Val RMSE: 0.7407 | CI: 0.7858\n",
            "Epoch 593 | Loss:  0.0121 | Val RMSE: 0.7361 | CI: 0.7891\n",
            "Epoch 594 | Loss:  0.0117 | Val RMSE: 0.7553 | CI: 0.7897\n",
            "Epoch 595 | Loss:  0.0131 | Val RMSE: 0.7560 | CI: 0.7892\n",
            "Epoch 596 | Loss:  0.0128 | Val RMSE: 0.7600 | CI: 0.7862\n",
            "Epoch 597 | Loss:  0.0104 | Val RMSE: 0.7289 | CI: 0.7844\n",
            "Epoch 598 | Loss:  0.0097 | Val RMSE: 0.7304 | CI: 0.7857\n",
            "Epoch 599 | Loss:  0.0115 | Val RMSE: 0.7278 | CI: 0.7871\n",
            "Epoch 600 | Loss:  0.0101 | Val RMSE: 0.7374 | CI: 0.7853\n",
            "Epoch 601 | Loss:  0.0091 | Val RMSE: 0.7513 | CI: 0.7860\n",
            "Epoch 602 | Loss:  0.0104 | Val RMSE: 0.7287 | CI: 0.7873\n",
            "Epoch 603 | Loss:  0.0095 | Val RMSE: 0.7284 | CI: 0.7879\n",
            "Epoch 604 | Loss:  0.0131 | Val RMSE: 0.7317 | CI: 0.7875\n",
            "Epoch 605 | Loss:  0.0114 | Val RMSE: 0.7259 | CI: 0.7882\n",
            "Epoch 606 | Loss:  0.0089 | Val RMSE: 0.7648 | CI: 0.7878\n",
            "Epoch 607 | Loss:  0.0114 | Val RMSE: 0.7446 | CI: 0.7881\n",
            "Epoch 608 | Loss:  0.0100 | Val RMSE: 0.7306 | CI: 0.7876\n",
            "Epoch 609 | Loss:  0.0112 | Val RMSE: 0.7454 | CI: 0.7875\n",
            "Epoch 610 | Loss:  0.0112 | Val RMSE: 0.7275 | CI: 0.7881\n",
            "Epoch 611 | Loss:  0.0123 | Val RMSE: 0.7581 | CI: 0.7867\n",
            "Epoch 612 | Loss:  0.0103 | Val RMSE: 0.7511 | CI: 0.7864\n",
            "Epoch 613 | Loss:  0.0139 | Val RMSE: 0.7251 | CI: 0.7882\n",
            "Epoch 614 | Loss:  0.0113 | Val RMSE: 0.7584 | CI: 0.7909\n",
            "Epoch 615 | Loss:  0.0124 | Val RMSE: 0.7235 | CI: 0.7890\n",
            "Epoch 616 | Loss:  0.0083 | Val RMSE: 0.7511 | CI: 0.7878\n",
            "Epoch 617 | Loss:  0.0115 | Val RMSE: 0.7263 | CI: 0.7902\n",
            "Epoch 618 | Loss:  0.0093 | Val RMSE: 0.7261 | CI: 0.7902\n",
            "Epoch 619 | Loss:  0.0123 | Val RMSE: 0.7236 | CI: 0.7869\n",
            "Epoch 620 | Loss:  0.0096 | Val RMSE: 0.7340 | CI: 0.7836\n",
            "Epoch 621 | Loss:  0.0116 | Val RMSE: 0.7415 | CI: 0.7835\n",
            "Epoch 622 | Loss:  0.0117 | Val RMSE: 0.7251 | CI: 0.7875\n",
            "Epoch 623 | Loss:  0.0110 | Val RMSE: 0.7398 | CI: 0.7883\n",
            "Epoch 624 | Loss:  0.0114 | Val RMSE: 0.7299 | CI: 0.7883\n",
            "Epoch 625 | Loss:  0.0112 | Val RMSE: 0.7351 | CI: 0.7868\n",
            "Epoch 626 | Loss:  0.0109 | Val RMSE: 0.7359 | CI: 0.7843\n",
            "Epoch 627 | Loss:  0.0115 | Val RMSE: 0.7284 | CI: 0.7817\n",
            "Epoch 628 | Loss:  0.0109 | Val RMSE: 0.7354 | CI: 0.7799\n",
            "Epoch 629 | Loss:  0.0101 | Val RMSE: 0.7310 | CI: 0.7819\n",
            "Epoch 630 | Loss:  0.0106 | Val RMSE: 0.7585 | CI: 0.7846\n",
            "Epoch 631 | Loss:  0.0113 | Val RMSE: 0.7435 | CI: 0.7858\n",
            "Epoch 632 | Loss:  0.0098 | Val RMSE: 0.7385 | CI: 0.7857\n",
            "Epoch 633 | Loss:  0.0096 | Val RMSE: 0.7286 | CI: 0.7874\n",
            "Epoch 634 | Loss:  0.0114 | Val RMSE: 0.7303 | CI: 0.7869\n",
            "Epoch 635 | Loss:  0.0102 | Val RMSE: 0.7359 | CI: 0.7819\n",
            "Epoch 636 | Loss:  0.0096 | Val RMSE: 0.7410 | CI: 0.7775\n",
            "Epoch 637 | Loss:  0.0120 | Val RMSE: 0.7302 | CI: 0.7825\n",
            "Epoch 638 | Loss:  0.0114 | Val RMSE: 0.7272 | CI: 0.7859\n",
            "Epoch 639 | Loss:  0.0131 | Val RMSE: 0.7311 | CI: 0.7891\n",
            "Epoch 640 | Loss:  0.0101 | Val RMSE: 0.7310 | CI: 0.7895\n",
            "Epoch 641 | Loss:  0.0120 | Val RMSE: 0.7540 | CI: 0.7880\n",
            "Epoch 642 | Loss:  0.0105 | Val RMSE: 0.7339 | CI: 0.7885\n",
            "Epoch 643 | Loss:  0.0107 | Val RMSE: 0.7327 | CI: 0.7876\n",
            "Epoch 644 | Loss:  0.0106 | Val RMSE: 0.7325 | CI: 0.7863\n",
            "Epoch 645 | Loss:  0.0115 | Val RMSE: 0.7344 | CI: 0.7867\n",
            "Epoch 646 | Loss:  0.0099 | Val RMSE: 0.7430 | CI: 0.7889\n",
            "Epoch 647 | Loss:  0.0127 | Val RMSE: 0.7314 | CI: 0.7902\n",
            "Epoch 648 | Loss:  0.0098 | Val RMSE: 0.7333 | CI: 0.7906\n",
            "Epoch 649 | Loss:  0.0110 | Val RMSE: 0.7275 | CI: 0.7916\n",
            "Epoch 650 | Loss:  0.0101 | Val RMSE: 0.7269 | CI: 0.7906\n",
            "Epoch 651 | Loss:  0.0102 | Val RMSE: 0.7247 | CI: 0.7862\n",
            "Epoch 652 | Loss:  0.0123 | Val RMSE: 0.7290 | CI: 0.7823\n",
            "Epoch 653 | Loss:  0.0125 | Val RMSE: 0.7341 | CI: 0.7887\n",
            "Epoch 654 | Loss:  0.0108 | Val RMSE: 0.7236 | CI: 0.7917\n",
            "Epoch 655 | Loss:  0.0092 | Val RMSE: 0.7781 | CI: 0.7929\n",
            "Epoch 656 | Loss:  0.0072 | Val RMSE: 0.7686 | CI: 0.7949\n",
            "Epoch 657 | Loss:  0.0129 | Val RMSE: 0.7305 | CI: 0.7945\n",
            "Epoch 658 | Loss:  0.0101 | Val RMSE: 0.7597 | CI: 0.7918\n",
            "Epoch 659 | Loss:  0.0132 | Val RMSE: 0.7331 | CI: 0.7864\n",
            "Epoch 660 | Loss:  0.0110 | Val RMSE: 0.8022 | CI: 0.7705\n",
            "Epoch 661 | Loss:  0.0123 | Val RMSE: 0.7896 | CI: 0.7640\n",
            "Epoch 662 | Loss:  0.0114 | Val RMSE: 0.7573 | CI: 0.7715\n",
            "Epoch 663 | Loss:  0.0108 | Val RMSE: 0.7479 | CI: 0.7873\n",
            "Epoch 664 | Loss:  0.0097 | Val RMSE: 0.7400 | CI: 0.7942\n",
            "Epoch 665 | Loss:  0.0098 | Val RMSE: 0.7674 | CI: 0.7940\n",
            "Epoch 666 | Loss:  0.0116 | Val RMSE: 0.7442 | CI: 0.7916\n",
            "Epoch 667 | Loss:  0.0110 | Val RMSE: 0.7765 | CI: 0.7898\n",
            "Epoch 668 | Loss:  0.0121 | Val RMSE: 0.7421 | CI: 0.7911\n",
            "Epoch 669 | Loss:  0.0111 | Val RMSE: 0.7628 | CI: 0.7888\n",
            "Epoch 670 | Loss:  0.0123 | Val RMSE: 0.7310 | CI: 0.7896\n",
            "Epoch 671 | Loss:  0.0093 | Val RMSE: 0.7383 | CI: 0.7869\n",
            "Epoch 672 | Loss:  0.0132 | Val RMSE: 0.7349 | CI: 0.7871\n",
            "Epoch 673 | Loss:  0.0108 | Val RMSE: 0.7416 | CI: 0.7867\n",
            "Epoch 674 | Loss:  0.0138 | Val RMSE: 0.7328 | CI: 0.7889\n",
            "Epoch 675 | Loss:  0.0092 | Val RMSE: 0.7313 | CI: 0.7905\n",
            "Epoch 676 | Loss:  0.0096 | Val RMSE: 0.7338 | CI: 0.7910\n",
            "Epoch 677 | Loss:  0.0121 | Val RMSE: 0.7288 | CI: 0.7919\n",
            "Epoch 678 | Loss:  0.0132 | Val RMSE: 0.7273 | CI: 0.7927\n",
            "Epoch 679 | Loss:  0.0098 | Val RMSE: 0.7282 | CI: 0.7931\n",
            "Epoch 680 | Loss:  0.0114 | Val RMSE: 0.7265 | CI: 0.7937\n",
            "Epoch 681 | Loss:  0.0118 | Val RMSE: 0.7275 | CI: 0.7923\n",
            "Epoch 682 | Loss:  0.0107 | Val RMSE: 0.7264 | CI: 0.7895\n",
            "Epoch 683 | Loss:  0.0111 | Val RMSE: 0.7314 | CI: 0.7909\n",
            "Epoch 684 | Loss:  0.0133 | Val RMSE: 0.7256 | CI: 0.7877\n",
            "Epoch 685 | Loss:  0.0121 | Val RMSE: 0.7263 | CI: 0.7880\n",
            "Epoch 686 | Loss:  0.0105 | Val RMSE: 0.7305 | CI: 0.7878\n",
            "Epoch 687 | Loss:  0.0096 | Val RMSE: 0.7318 | CI: 0.7864\n",
            "Epoch 688 | Loss:  0.0119 | Val RMSE: 0.7281 | CI: 0.7839\n",
            "Epoch 689 | Loss:  0.0103 | Val RMSE: 0.7293 | CI: 0.7822\n",
            "Epoch 690 | Loss:  0.0101 | Val RMSE: 0.7331 | CI: 0.7823\n",
            "Epoch 691 | Loss:  0.0115 | Val RMSE: 0.7296 | CI: 0.7862\n",
            "Epoch 692 | Loss:  0.0116 | Val RMSE: 0.7303 | CI: 0.7892\n",
            "Epoch 693 | Loss:  0.0108 | Val RMSE: 0.7344 | CI: 0.7913\n",
            "Epoch 694 | Loss:  0.0118 | Val RMSE: 0.7321 | CI: 0.7922\n",
            "Epoch 695 | Loss:  0.0118 | Val RMSE: 0.7253 | CI: 0.7890\n",
            "Epoch 696 | Loss:  0.0110 | Val RMSE: 0.7331 | CI: 0.7799\n",
            "Epoch 697 | Loss:  0.0110 | Val RMSE: 0.7429 | CI: 0.7718\n",
            "Epoch 698 | Loss:  0.0118 | Val RMSE: 0.7477 | CI: 0.7746\n",
            "Epoch 699 | Loss:  0.0135 | Val RMSE: 0.7289 | CI: 0.7903\n",
            "Epoch 700 | Loss:  0.0094 | Val RMSE: 0.7348 | CI: 0.7930\n",
            "Epoch 701 | Loss:  0.0114 | Val RMSE: 0.7519 | CI: 0.7919\n",
            "Epoch 702 | Loss:  0.0098 | Val RMSE: 0.7605 | CI: 0.7907\n",
            "Epoch 703 | Loss:  0.0100 | Val RMSE: 0.7538 | CI: 0.7921\n",
            "Epoch 704 | Loss:  0.0113 | Val RMSE: 0.7340 | CI: 0.7911\n",
            "Epoch 705 | Loss:  0.0085 | Val RMSE: 0.7386 | CI: 0.7874\n",
            "Epoch 706 | Loss:  0.0114 | Val RMSE: 0.7572 | CI: 0.7791\n",
            "Epoch 707 | Loss:  0.0105 | Val RMSE: 0.7561 | CI: 0.7783\n",
            "Epoch 708 | Loss:  0.0127 | Val RMSE: 0.7527 | CI: 0.7843\n",
            "Epoch 709 | Loss:  0.0089 | Val RMSE: 0.7514 | CI: 0.7899\n",
            "Epoch 710 | Loss:  0.0125 | Val RMSE: 0.7299 | CI: 0.7930\n",
            "Epoch 711 | Loss:  0.0085 | Val RMSE: 0.7437 | CI: 0.7937\n",
            "Epoch 712 | Loss:  0.0097 | Val RMSE: 0.7452 | CI: 0.7934\n",
            "Epoch 713 | Loss:  0.0105 | Val RMSE: 0.7754 | CI: 0.7923\n",
            "Epoch 714 | Loss:  0.0123 | Val RMSE: 0.7556 | CI: 0.7926\n",
            "Epoch 715 | Loss:  0.0110 | Val RMSE: 0.7305 | CI: 0.7937\n",
            "Epoch 716 | Loss:  0.0112 | Val RMSE: 0.7522 | CI: 0.7924\n",
            "Epoch 717 | Loss:  0.0101 | Val RMSE: 0.7494 | CI: 0.7896\n",
            "Epoch 718 | Loss:  0.0109 | Val RMSE: 0.8228 | CI: 0.7876\n",
            "Epoch 719 | Loss:  0.0142 | Val RMSE: 0.7527 | CI: 0.7873\n",
            "Epoch 720 | Loss:  0.0129 | Val RMSE: 0.7745 | CI: 0.7900\n",
            "Epoch 721 | Loss:  0.0111 | Val RMSE: 0.7354 | CI: 0.7924\n",
            "Epoch 722 | Loss:  0.0097 | Val RMSE: 0.7610 | CI: 0.7932\n",
            "Epoch 723 | Loss:  0.0094 | Val RMSE: 0.7981 | CI: 0.7928\n",
            "Epoch 724 | Loss:  0.0117 | Val RMSE: 0.7453 | CI: 0.7924\n",
            "Epoch 725 | Loss:  0.0102 | Val RMSE: 0.7727 | CI: 0.7919\n",
            "Epoch 726 | Loss:  0.0127 | Val RMSE: 0.7284 | CI: 0.7936\n",
            "Epoch 727 | Loss:  0.0117 | Val RMSE: 0.7714 | CI: 0.7884\n",
            "Epoch 728 | Loss:  0.0123 | Val RMSE: 0.7973 | CI: 0.7694\n",
            "Epoch 729 | Loss:  0.0117 | Val RMSE: 0.7804 | CI: 0.7540\n",
            "Epoch 730 | Loss:  0.0135 | Val RMSE: 0.7860 | CI: 0.7565\n",
            "Epoch 731 | Loss:  0.0127 | Val RMSE: 0.7612 | CI: 0.7742\n",
            "Epoch 732 | Loss:  0.0111 | Val RMSE: 0.7592 | CI: 0.7911\n",
            "Epoch 733 | Loss:  0.0120 | Val RMSE: 0.7356 | CI: 0.7969\n",
            "Epoch 734 | Loss:  0.0125 | Val RMSE: 0.7468 | CI: 0.7969\n",
            "Epoch 735 | Loss:  0.0117 | Val RMSE: 0.7398 | CI: 0.7971\n",
            "Epoch 736 | Loss:  0.0113 | Val RMSE: 0.7600 | CI: 0.7973\n",
            "Epoch 737 | Loss:  0.0129 | Val RMSE: 0.7541 | CI: 0.7967\n",
            "Epoch 738 | Loss:  0.0108 | Val RMSE: 0.7276 | CI: 0.7958\n",
            "Epoch 739 | Loss:  0.0097 | Val RMSE: 0.7273 | CI: 0.7947\n",
            "Epoch 740 | Loss:  0.0103 | Val RMSE: 0.7273 | CI: 0.7929\n",
            "Epoch 741 | Loss:  0.0090 | Val RMSE: 0.7502 | CI: 0.7897\n",
            "Epoch 742 | Loss:  0.0103 | Val RMSE: 0.7529 | CI: 0.7865\n",
            "Epoch 743 | Loss:  0.0096 | Val RMSE: 0.7296 | CI: 0.7884\n",
            "Epoch 744 | Loss:  0.0107 | Val RMSE: 0.7300 | CI: 0.7898\n",
            "Epoch 745 | Loss:  0.0101 | Val RMSE: 0.7245 | CI: 0.7909\n",
            "Epoch 746 | Loss:  0.0102 | Val RMSE: 0.7540 | CI: 0.7906\n",
            "Epoch 747 | Loss:  0.0126 | Val RMSE: 0.7522 | CI: 0.7925\n",
            "Epoch 748 | Loss:  0.0146 | Val RMSE: 0.7315 | CI: 0.7947\n",
            "Epoch 749 | Loss:  0.0104 | Val RMSE: 0.7415 | CI: 0.7949\n",
            "Epoch 750 | Loss:  0.0119 | Val RMSE: 0.7299 | CI: 0.7941\n",
            "Epoch 751 | Loss:  0.0106 | Val RMSE: 0.7759 | CI: 0.7920\n",
            "Epoch 752 | Loss:  0.0138 | Val RMSE: 0.7321 | CI: 0.7920\n",
            "Epoch 753 | Loss:  0.0090 | Val RMSE: 0.7261 | CI: 0.7921\n",
            "Epoch 754 | Loss:  0.0090 | Val RMSE: 0.7239 | CI: 0.7923\n",
            "Epoch 755 | Loss:  0.0099 | Val RMSE: 0.7253 | CI: 0.7927\n",
            "Epoch 756 | Loss:  0.0099 | Val RMSE: 0.7309 | CI: 0.7929\n",
            "Epoch 757 | Loss:  0.0089 | Val RMSE: 0.7270 | CI: 0.7928\n",
            "Epoch 758 | Loss:  0.0093 | Val RMSE: 0.7279 | CI: 0.7927\n",
            "Epoch 759 | Loss:  0.0085 | Val RMSE: 0.7279 | CI: 0.7906\n",
            "Epoch 760 | Loss:  0.0114 | Val RMSE: 0.7440 | CI: 0.7849\n",
            "Epoch 761 | Loss:  0.0130 | Val RMSE: 0.7291 | CI: 0.7812\n",
            "Epoch 762 | Loss:  0.0099 | Val RMSE: 0.7306 | CI: 0.7830\n",
            "Epoch 763 | Loss:  0.0092 | Val RMSE: 0.7289 | CI: 0.7859\n",
            "Epoch 764 | Loss:  0.0121 | Val RMSE: 0.7285 | CI: 0.7909\n",
            "Epoch 765 | Loss:  0.0109 | Val RMSE: 0.7239 | CI: 0.7930\n",
            "Epoch 766 | Loss:  0.0129 | Val RMSE: 0.7242 | CI: 0.7937\n",
            "Epoch 767 | Loss:  0.0103 | Val RMSE: 0.7252 | CI: 0.7938\n",
            "Epoch 768 | Loss:  0.0108 | Val RMSE: 0.7272 | CI: 0.7940\n",
            "Epoch 769 | Loss:  0.0102 | Val RMSE: 0.7328 | CI: 0.7935\n",
            "Epoch 770 | Loss:  0.0102 | Val RMSE: 0.7234 | CI: 0.7926\n",
            "Epoch 771 | Loss:  0.0123 | Val RMSE: 0.7252 | CI: 0.7907\n",
            "Epoch 772 | Loss:  0.0102 | Val RMSE: 0.7277 | CI: 0.7889\n",
            "Epoch 773 | Loss:  0.0101 | Val RMSE: 0.7315 | CI: 0.7890\n",
            "Epoch 774 | Loss:  0.0098 | Val RMSE: 0.7367 | CI: 0.7903\n",
            "Epoch 775 | Loss:  0.0092 | Val RMSE: 0.7228 | CI: 0.7930\n",
            "Epoch 776 | Loss:  0.0088 | Val RMSE: 0.7240 | CI: 0.7941\n",
            "Epoch 777 | Loss:  0.0115 | Val RMSE: 0.7319 | CI: 0.7941\n",
            "Epoch 778 | Loss:  0.0098 | Val RMSE: 0.7570 | CI: 0.7942\n",
            "Epoch 779 | Loss:  0.0099 | Val RMSE: 0.7299 | CI: 0.7955\n",
            "Epoch 780 | Loss:  0.0118 | Val RMSE: 0.7313 | CI: 0.7950\n",
            "Epoch 781 | Loss:  0.0132 | Val RMSE: 0.7299 | CI: 0.7941\n",
            "Epoch 782 | Loss:  0.0121 | Val RMSE: 0.7340 | CI: 0.7926\n",
            "Epoch 783 | Loss:  0.0102 | Val RMSE: 0.7590 | CI: 0.7909\n",
            "Epoch 784 | Loss:  0.0143 | Val RMSE: 0.7224 | CI: 0.7933\n",
            "Epoch 785 | Loss:  0.0106 | Val RMSE: 0.7354 | CI: 0.7946\n",
            "Epoch 786 | Loss:  0.0118 | Val RMSE: 0.7225 | CI: 0.7931\n",
            "Epoch 787 | Loss:  0.0098 | Val RMSE: 0.7570 | CI: 0.7912\n",
            "Epoch 788 | Loss:  0.0101 | Val RMSE: 0.7454 | CI: 0.7912\n",
            "Epoch 789 | Loss:  0.0089 | Val RMSE: 0.7215 | CI: 0.7919\n",
            "Epoch 790 | Loss:  0.0109 | Val RMSE: 0.7240 | CI: 0.7917\n",
            "Epoch 791 | Loss:  0.0110 | Val RMSE: 0.7223 | CI: 0.7907\n",
            "Epoch 792 | Loss:  0.0102 | Val RMSE: 0.7423 | CI: 0.7895\n",
            "Epoch 793 | Loss:  0.0105 | Val RMSE: 0.7354 | CI: 0.7901\n",
            "Epoch 794 | Loss:  0.0133 | Val RMSE: 0.7403 | CI: 0.7905\n",
            "Epoch 795 | Loss:  0.0091 | Val RMSE: 0.7344 | CI: 0.7910\n",
            "Epoch 796 | Loss:  0.0100 | Val RMSE: 0.7487 | CI: 0.7916\n",
            "Epoch 797 | Loss:  0.0106 | Val RMSE: 0.7600 | CI: 0.7911\n",
            "Epoch 798 | Loss:  0.0121 | Val RMSE: 0.7280 | CI: 0.7912\n",
            "Epoch 799 | Loss:  0.0092 | Val RMSE: 0.7329 | CI: 0.7910\n",
            "Epoch 800 | Loss:  0.0120 | Val RMSE: 0.7245 | CI: 0.7883\n",
            "Epoch 801 | Loss:  0.0096 | Val RMSE: 0.7375 | CI: 0.7858\n",
            "Epoch 802 | Loss:  0.0102 | Val RMSE: 0.7338 | CI: 0.7868\n",
            "Epoch 803 | Loss:  0.0104 | Val RMSE: 0.7243 | CI: 0.7890\n",
            "Epoch 804 | Loss:  0.0080 | Val RMSE: 0.7242 | CI: 0.7902\n",
            "Epoch 805 | Loss:  0.0124 | Val RMSE: 0.7261 | CI: 0.7899\n",
            "Epoch 806 | Loss:  0.0089 | Val RMSE: 0.7449 | CI: 0.7898\n",
            "Epoch 807 | Loss:  0.0120 | Val RMSE: 0.7373 | CI: 0.7901\n",
            "Epoch 808 | Loss:  0.0117 | Val RMSE: 0.7275 | CI: 0.7889\n",
            "Epoch 809 | Loss:  0.0112 | Val RMSE: 0.7291 | CI: 0.7884\n",
            "Epoch 810 | Loss:  0.0122 | Val RMSE: 0.7270 | CI: 0.7893\n",
            "Epoch 811 | Loss:  0.0102 | Val RMSE: 0.7403 | CI: 0.7899\n",
            "Epoch 812 | Loss:  0.0115 | Val RMSE: 0.7402 | CI: 0.7914\n",
            "Epoch 813 | Loss:  0.0118 | Val RMSE: 0.7251 | CI: 0.7920\n",
            "Epoch 814 | Loss:  0.0098 | Val RMSE: 0.7362 | CI: 0.7927\n",
            "Epoch 815 | Loss:  0.0109 | Val RMSE: 0.7307 | CI: 0.7942\n",
            "Epoch 816 | Loss:  0.0106 | Val RMSE: 0.7740 | CI: 0.7937\n",
            "Epoch 817 | Loss:  0.0097 | Val RMSE: 0.7387 | CI: 0.7928\n",
            "Epoch 818 | Loss:  0.0085 | Val RMSE: 0.7274 | CI: 0.7922\n",
            "Epoch 819 | Loss:  0.0094 | Val RMSE: 0.7289 | CI: 0.7932\n",
            "Epoch 820 | Loss:  0.0102 | Val RMSE: 0.7249 | CI: 0.7936\n",
            "Epoch 821 | Loss:  0.0103 | Val RMSE: 0.7379 | CI: 0.7940\n",
            "Epoch 822 | Loss:  0.0084 | Val RMSE: 0.7375 | CI: 0.7946\n",
            "Epoch 823 | Loss:  0.0113 | Val RMSE: 0.7215 | CI: 0.7947\n",
            "Epoch 824 | Loss:  0.0073 | Val RMSE: 0.7206 | CI: 0.7942\n",
            "Epoch 825 | Loss:  0.0112 | Val RMSE: 0.7213 | CI: 0.7936\n",
            "Epoch 826 | Loss:  0.0103 | Val RMSE: 0.7304 | CI: 0.7924\n",
            "Epoch 827 | Loss:  0.0098 | Val RMSE: 0.7351 | CI: 0.7911\n",
            "Epoch 828 | Loss:  0.0097 | Val RMSE: 0.7275 | CI: 0.7911\n",
            "Epoch 829 | Loss:  0.0122 | Val RMSE: 0.7232 | CI: 0.7929\n",
            "Epoch 830 | Loss:  0.0104 | Val RMSE: 0.7233 | CI: 0.7946\n",
            "Epoch 831 | Loss:  0.0118 | Val RMSE: 0.7290 | CI: 0.7951\n",
            "Epoch 832 | Loss:  0.0087 | Val RMSE: 0.7522 | CI: 0.7950\n",
            "Epoch 833 | Loss:  0.0094 | Val RMSE: 0.7320 | CI: 0.7962\n",
            "Epoch 834 | Loss:  0.0094 | Val RMSE: 0.7228 | CI: 0.7969\n",
            "Epoch 835 | Loss:  0.0108 | Val RMSE: 0.7210 | CI: 0.7965\n",
            "Epoch 836 | Loss:  0.0101 | Val RMSE: 0.7248 | CI: 0.7958\n",
            "Epoch 837 | Loss:  0.0108 | Val RMSE: 0.7299 | CI: 0.7943\n",
            "Epoch 838 | Loss:  0.0107 | Val RMSE: 0.7210 | CI: 0.7928\n",
            "Epoch 839 | Loss:  0.0104 | Val RMSE: 0.7222 | CI: 0.7925\n",
            "Epoch 840 | Loss:  0.0103 | Val RMSE: 0.7414 | CI: 0.7915\n",
            "Epoch 841 | Loss:  0.0116 | Val RMSE: 0.7449 | CI: 0.7900\n",
            "Epoch 842 | Loss:  0.0131 | Val RMSE: 0.7238 | CI: 0.7896\n",
            "Epoch 843 | Loss:  0.0138 | Val RMSE: 0.7381 | CI: 0.7878\n",
            "Epoch 844 | Loss:  0.0103 | Val RMSE: 0.7244 | CI: 0.7873\n",
            "Epoch 845 | Loss:  0.0104 | Val RMSE: 0.7340 | CI: 0.7873\n",
            "Epoch 846 | Loss:  0.0101 | Val RMSE: 0.7305 | CI: 0.7870\n",
            "Epoch 847 | Loss:  0.0109 | Val RMSE: 0.7248 | CI: 0.7868\n",
            "Epoch 848 | Loss:  0.0091 | Val RMSE: 0.7257 | CI: 0.7871\n",
            "Epoch 849 | Loss:  0.0091 | Val RMSE: 0.7249 | CI: 0.7868\n",
            "Epoch 850 | Loss:  0.0108 | Val RMSE: 0.7323 | CI: 0.7868\n",
            "Epoch 851 | Loss:  0.0095 | Val RMSE: 0.7316 | CI: 0.7865\n",
            "Epoch 852 | Loss:  0.0156 | Val RMSE: 0.7248 | CI: 0.7855\n",
            "Epoch 853 | Loss:  0.0119 | Val RMSE: 0.7259 | CI: 0.7847\n",
            "Epoch 854 | Loss:  0.0096 | Val RMSE: 0.7307 | CI: 0.7853\n",
            "Epoch 855 | Loss:  0.0115 | Val RMSE: 0.7291 | CI: 0.7870\n",
            "Epoch 856 | Loss:  0.0117 | Val RMSE: 0.7337 | CI: 0.7874\n",
            "Epoch 857 | Loss:  0.0157 | Val RMSE: 0.7255 | CI: 0.7866\n",
            "Epoch 858 | Loss:  0.0106 | Val RMSE: 0.7281 | CI: 0.7869\n",
            "Epoch 859 | Loss:  0.0118 | Val RMSE: 0.7273 | CI: 0.7878\n",
            "Epoch 860 | Loss:  0.0098 | Val RMSE: 0.7462 | CI: 0.7872\n",
            "Epoch 861 | Loss:  0.0099 | Val RMSE: 0.7331 | CI: 0.7873\n",
            "Epoch 862 | Loss:  0.0095 | Val RMSE: 0.7205 | CI: 0.7886\n",
            "Epoch 863 | Loss:  0.0096 | Val RMSE: 0.7217 | CI: 0.7890\n",
            "Epoch 864 | Loss:  0.0080 | Val RMSE: 0.7303 | CI: 0.7872\n",
            "Epoch 865 | Loss:  0.0108 | Val RMSE: 0.7328 | CI: 0.7880\n",
            "Epoch 866 | Loss:  0.0122 | Val RMSE: 0.7215 | CI: 0.7888\n",
            "Epoch 867 | Loss:  0.0109 | Val RMSE: 0.7212 | CI: 0.7884\n",
            "Epoch 868 | Loss:  0.0103 | Val RMSE: 0.7264 | CI: 0.7891\n",
            "Epoch 869 | Loss:  0.0104 | Val RMSE: 0.7221 | CI: 0.7903\n",
            "Epoch 870 | Loss:  0.0101 | Val RMSE: 0.7379 | CI: 0.7906\n",
            "Epoch 871 | Loss:  0.0103 | Val RMSE: 0.7358 | CI: 0.7912\n",
            "Epoch 872 | Loss:  0.0118 | Val RMSE: 0.7290 | CI: 0.7917\n",
            "Epoch 873 | Loss:  0.0119 | Val RMSE: 0.7252 | CI: 0.7918\n",
            "Epoch 874 | Loss:  0.0112 | Val RMSE: 0.7401 | CI: 0.7903\n",
            "Epoch 875 | Loss:  0.0085 | Val RMSE: 0.7683 | CI: 0.7896\n",
            "Epoch 876 | Loss:  0.0104 | Val RMSE: 0.7286 | CI: 0.7896\n",
            "Epoch 877 | Loss:  0.0086 | Val RMSE: 0.7351 | CI: 0.7897\n",
            "Epoch 878 | Loss:  0.0111 | Val RMSE: 0.7250 | CI: 0.7902\n",
            "Epoch 879 | Loss:  0.0110 | Val RMSE: 0.7371 | CI: 0.7918\n",
            "Epoch 880 | Loss:  0.0092 | Val RMSE: 0.7770 | CI: 0.7930\n",
            "Epoch 881 | Loss:  0.0130 | Val RMSE: 0.7344 | CI: 0.7929\n",
            "Epoch 882 | Loss:  0.0092 | Val RMSE: 0.7605 | CI: 0.7926\n",
            "Epoch 883 | Loss:  0.0125 | Val RMSE: 0.7227 | CI: 0.7919\n",
            "Epoch 884 | Loss:  0.0101 | Val RMSE: 0.7595 | CI: 0.7852\n",
            "Epoch 885 | Loss:  0.0129 | Val RMSE: 0.7392 | CI: 0.7801\n",
            "Epoch 886 | Loss:  0.0118 | Val RMSE: 0.7445 | CI: 0.7815\n",
            "Epoch 887 | Loss:  0.0112 | Val RMSE: 0.7301 | CI: 0.7900\n",
            "Epoch 888 | Loss:  0.0119 | Val RMSE: 0.7323 | CI: 0.7972\n",
            "Epoch 889 | Loss:  0.0125 | Val RMSE: 0.7328 | CI: 0.7988\n",
            "Epoch 890 | Loss:  0.0113 | Val RMSE: 0.7354 | CI: 0.7982\n",
            "Epoch 891 | Loss:  0.0100 | Val RMSE: 0.7306 | CI: 0.7979\n",
            "Epoch 892 | Loss:  0.0088 | Val RMSE: 0.7334 | CI: 0.7977\n",
            "Epoch 893 | Loss:  0.0082 | Val RMSE: 0.7330 | CI: 0.7971\n",
            "Epoch 894 | Loss:  0.0102 | Val RMSE: 0.7240 | CI: 0.7957\n",
            "Epoch 895 | Loss:  0.0089 | Val RMSE: 0.7279 | CI: 0.7937\n",
            "Epoch 896 | Loss:  0.0101 | Val RMSE: 0.7279 | CI: 0.7952\n",
            "Epoch 897 | Loss:  0.0084 | Val RMSE: 0.7359 | CI: 0.7967\n",
            "Epoch 898 | Loss:  0.0109 | Val RMSE: 0.7229 | CI: 0.7982\n",
            "Epoch 899 | Loss:  0.0107 | Val RMSE: 0.7249 | CI: 0.7982\n",
            "Epoch 900 | Loss:  0.0119 | Val RMSE: 0.7306 | CI: 0.7973\n",
            "Epoch 901 | Loss:  0.0113 | Val RMSE: 0.7342 | CI: 0.7970\n",
            "Epoch 902 | Loss:  0.0110 | Val RMSE: 0.7331 | CI: 0.7971\n",
            "Epoch 903 | Loss:  0.0106 | Val RMSE: 0.7248 | CI: 0.7961\n",
            "Epoch 904 | Loss:  0.0105 | Val RMSE: 0.7301 | CI: 0.7947\n",
            "Epoch 905 | Loss:  0.0120 | Val RMSE: 0.7301 | CI: 0.7938\n",
            "Epoch 906 | Loss:  0.0097 | Val RMSE: 0.7340 | CI: 0.7941\n",
            "Epoch 907 | Loss:  0.0106 | Val RMSE: 0.7476 | CI: 0.7946\n",
            "Epoch 908 | Loss:  0.0078 | Val RMSE: 0.7295 | CI: 0.7958\n",
            "Epoch 909 | Loss:  0.0132 | Val RMSE: 0.7299 | CI: 0.7964\n",
            "Epoch 910 | Loss:  0.0097 | Val RMSE: 0.7259 | CI: 0.7961\n",
            "Epoch 911 | Loss:  0.0109 | Val RMSE: 0.7351 | CI: 0.7946\n",
            "Epoch 912 | Loss:  0.0102 | Val RMSE: 0.7378 | CI: 0.7942\n",
            "Epoch 913 | Loss:  0.0118 | Val RMSE: 0.7235 | CI: 0.7955\n",
            "Epoch 914 | Loss:  0.0098 | Val RMSE: 0.7268 | CI: 0.7953\n",
            "Epoch 915 | Loss:  0.0111 | Val RMSE: 0.7239 | CI: 0.7958\n",
            "Epoch 916 | Loss:  0.0106 | Val RMSE: 0.7443 | CI: 0.7955\n",
            "Epoch 917 | Loss:  0.0129 | Val RMSE: 0.7357 | CI: 0.7955\n",
            "Epoch 918 | Loss:  0.0102 | Val RMSE: 0.7264 | CI: 0.7945\n",
            "Epoch 919 | Loss:  0.0100 | Val RMSE: 0.7286 | CI: 0.7942\n",
            "Epoch 920 | Loss:  0.0088 | Val RMSE: 0.7360 | CI: 0.7946\n",
            "Epoch 921 | Loss:  0.0105 | Val RMSE: 0.7431 | CI: 0.7948\n",
            "Epoch 922 | Loss:  0.0127 | Val RMSE: 0.7251 | CI: 0.7940\n",
            "Epoch 923 | Loss:  0.0109 | Val RMSE: 0.7394 | CI: 0.7942\n",
            "Epoch 924 | Loss:  0.0108 | Val RMSE: 0.7289 | CI: 0.7943\n",
            "Epoch 925 | Loss:  0.0113 | Val RMSE: 0.7445 | CI: 0.7930\n",
            "Epoch 926 | Loss:  0.0122 | Val RMSE: 0.7389 | CI: 0.7945\n",
            "Epoch 927 | Loss:  0.0101 | Val RMSE: 0.7349 | CI: 0.7936\n",
            "Epoch 928 | Loss:  0.0126 | Val RMSE: 0.7245 | CI: 0.7964\n",
            "Epoch 929 | Loss:  0.0100 | Val RMSE: 0.7318 | CI: 0.7987\n",
            "Epoch 930 | Loss:  0.0125 | Val RMSE: 0.7296 | CI: 0.7985\n",
            "Epoch 931 | Loss:  0.0134 | Val RMSE: 0.7499 | CI: 0.7977\n",
            "Epoch 932 | Loss:  0.0085 | Val RMSE: 0.7269 | CI: 0.7974\n",
            "Epoch 933 | Loss:  0.0104 | Val RMSE: 0.7214 | CI: 0.7956\n",
            "Epoch 934 | Loss:  0.0122 | Val RMSE: 0.7328 | CI: 0.7939\n",
            "Epoch 935 | Loss:  0.0099 | Val RMSE: 0.7236 | CI: 0.7936\n",
            "Epoch 936 | Loss:  0.0112 | Val RMSE: 0.7418 | CI: 0.7946\n",
            "Epoch 937 | Loss:  0.0107 | Val RMSE: 0.7400 | CI: 0.7951\n",
            "Epoch 938 | Loss:  0.0132 | Val RMSE: 0.7265 | CI: 0.7951\n",
            "Epoch 939 | Loss:  0.0097 | Val RMSE: 0.7257 | CI: 0.7955\n",
            "Epoch 940 | Loss:  0.0120 | Val RMSE: 0.7178 | CI: 0.7946\n",
            "Epoch 941 | Loss:  0.0097 | Val RMSE: 0.7580 | CI: 0.7915\n",
            "Epoch 942 | Loss:  0.0110 | Val RMSE: 0.7339 | CI: 0.7911\n",
            "Epoch 943 | Loss:  0.0091 | Val RMSE: 0.7235 | CI: 0.7907\n",
            "Epoch 944 | Loss:  0.0114 | Val RMSE: 0.7287 | CI: 0.7924\n",
            "Epoch 945 | Loss:  0.0124 | Val RMSE: 0.7149 | CI: 0.7957\n",
            "Epoch 946 | Loss:  0.0124 | Val RMSE: 0.7320 | CI: 0.7972\n",
            "Epoch 947 | Loss:  0.0095 | Val RMSE: 0.7295 | CI: 0.7973\n",
            "Epoch 948 | Loss:  0.0105 | Val RMSE: 0.7221 | CI: 0.7961\n",
            "Epoch 949 | Loss:  0.0080 | Val RMSE: 0.7188 | CI: 0.7960\n",
            "Epoch 950 | Loss:  0.0115 | Val RMSE: 0.7209 | CI: 0.7957\n",
            "Epoch 951 | Loss:  0.0102 | Val RMSE: 0.7218 | CI: 0.7942\n",
            "Epoch 952 | Loss:  0.0102 | Val RMSE: 0.7198 | CI: 0.7939\n",
            "Epoch 953 | Loss:  0.0101 | Val RMSE: 0.7217 | CI: 0.7935\n",
            "Epoch 954 | Loss:  0.0098 | Val RMSE: 0.7178 | CI: 0.7945\n",
            "Epoch 955 | Loss:  0.0088 | Val RMSE: 0.7177 | CI: 0.7961\n",
            "Epoch 956 | Loss:  0.0101 | Val RMSE: 0.7194 | CI: 0.7963\n",
            "Epoch 957 | Loss:  0.0092 | Val RMSE: 0.7178 | CI: 0.7959\n",
            "Epoch 958 | Loss:  0.0073 | Val RMSE: 0.7207 | CI: 0.7955\n",
            "Epoch 959 | Loss:  0.0100 | Val RMSE: 0.7189 | CI: 0.7960\n",
            "Epoch 960 | Loss:  0.0105 | Val RMSE: 0.7207 | CI: 0.7962\n",
            "Epoch 961 | Loss:  0.0119 | Val RMSE: 0.7217 | CI: 0.7961\n",
            "Epoch 962 | Loss:  0.0112 | Val RMSE: 0.7255 | CI: 0.7951\n",
            "Epoch 963 | Loss:  0.0109 | Val RMSE: 0.7237 | CI: 0.7930\n",
            "Epoch 964 | Loss:  0.0094 | Val RMSE: 0.7250 | CI: 0.7917\n",
            "Epoch 965 | Loss:  0.0115 | Val RMSE: 0.7231 | CI: 0.7931\n",
            "Epoch 966 | Loss:  0.0115 | Val RMSE: 0.7246 | CI: 0.7951\n",
            "Epoch 967 | Loss:  0.0104 | Val RMSE: 0.7269 | CI: 0.7963\n",
            "Epoch 968 | Loss:  0.0116 | Val RMSE: 0.7226 | CI: 0.7968\n",
            "Epoch 969 | Loss:  0.0096 | Val RMSE: 0.7219 | CI: 0.7973\n",
            "Epoch 970 | Loss:  0.0098 | Val RMSE: 0.7258 | CI: 0.7981\n",
            "Epoch 971 | Loss:  0.0137 | Val RMSE: 0.7205 | CI: 0.7982\n",
            "Epoch 972 | Loss:  0.0102 | Val RMSE: 0.7228 | CI: 0.7977\n",
            "Epoch 973 | Loss:  0.0105 | Val RMSE: 0.7233 | CI: 0.7965\n",
            "Epoch 974 | Loss:  0.0107 | Val RMSE: 0.7269 | CI: 0.7958\n",
            "Epoch 975 | Loss:  0.0093 | Val RMSE: 0.7382 | CI: 0.7963\n",
            "Epoch 976 | Loss:  0.0099 | Val RMSE: 0.7253 | CI: 0.7983\n",
            "Epoch 977 | Loss:  0.0120 | Val RMSE: 0.7293 | CI: 0.7992\n",
            "Epoch 978 | Loss:  0.0123 | Val RMSE: 0.7257 | CI: 0.7992\n",
            "Epoch 979 | Loss:  0.0111 | Val RMSE: 0.7475 | CI: 0.7990\n",
            "Epoch 980 | Loss:  0.0111 | Val RMSE: 0.7520 | CI: 0.7987\n",
            "Epoch 981 | Loss:  0.0097 | Val RMSE: 0.7206 | CI: 0.7979\n",
            "Epoch 982 | Loss:  0.0106 | Val RMSE: 0.7419 | CI: 0.7959\n",
            "Epoch 983 | Loss:  0.0113 | Val RMSE: 0.7203 | CI: 0.7969\n",
            "Epoch 984 | Loss:  0.0109 | Val RMSE: 0.7531 | CI: 0.7980\n",
            "Epoch 985 | Loss:  0.0119 | Val RMSE: 0.7470 | CI: 0.7984\n",
            "Epoch 986 | Loss:  0.0082 | Val RMSE: 0.7231 | CI: 0.7982\n",
            "Epoch 987 | Loss:  0.0117 | Val RMSE: 0.7272 | CI: 0.7979\n",
            "Epoch 988 | Loss:  0.0096 | Val RMSE: 0.7201 | CI: 0.7982\n",
            "Epoch 989 | Loss:  0.0100 | Val RMSE: 0.7434 | CI: 0.7976\n",
            "Epoch 990 | Loss:  0.0104 | Val RMSE: 0.7338 | CI: 0.7968\n",
            "Epoch 991 | Loss:  0.0095 | Val RMSE: 0.7223 | CI: 0.7968\n",
            "Epoch 992 | Loss:  0.0100 | Val RMSE: 0.7225 | CI: 0.7964\n",
            "Epoch 993 | Loss:  0.0092 | Val RMSE: 0.7271 | CI: 0.7962\n",
            "Epoch 994 | Loss:  0.0099 | Val RMSE: 0.7342 | CI: 0.7974\n",
            "Epoch 995 | Loss:  0.0105 | Val RMSE: 0.7212 | CI: 0.7988\n",
            "Epoch 996 | Loss:  0.0103 | Val RMSE: 0.7161 | CI: 0.8002\n",
            "Epoch 997 | Loss:  0.0095 | Val RMSE: 0.7160 | CI: 0.8006\n",
            "Epoch 998 | Loss:  0.0113 | Val RMSE: 0.7163 | CI: 0.8010\n",
            "Epoch 999 | Loss:  0.0096 | Val RMSE: 0.7254 | CI: 0.8006\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Save model weights\n",
        "\n",
        "model_path = f\"{data_path}/graphdta_baseline_weights.pt\"\n",
        "torch.save(model.state_dict(), model_path)\n",
        "print(f\"Saved model weights to {model_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MvNiyHvQSSr6",
        "outputId": "c1e100cc-a9a3-473a-be66-0abb76778bc5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved model weights to /content/drive/MyDrive/Rebuilding_and_Modifying_GraphDTA/data/graphdta_baseline_weights.pt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Save full model\n",
        "\n",
        "torch.save(model, f\"{data_path}/graphdta_baseline_full_model.pt\")\n",
        "print(f\"Saved full model.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xlRWpWJUSjX7",
        "outputId": "42c328ef-33a9-40f0-bb76-5e0ccc2f415f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved full model.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Check the predictions of the model\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "import random\n",
        "\n",
        "sample_indices = random.sample(range(len(test_set)), 5)\n",
        "samples = [test_set[i] for i in sample_indices]"
      ],
      "metadata": {
        "id": "oBbSHOIXS_7j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Run predictions using the model\n",
        "\n",
        "model.eval()\n",
        "results = []\n",
        "\n",
        "for drug_graph, protein_seq, true_affinity in samples:\n",
        "  drug_graph = drug_graph.to(device)\n",
        "  protein_seq = protein_seq.unsqueeze(0).to(device)\n",
        "\n",
        "  drug_graph.batch = torch.zeros(drug_graph.num_nodes, dtype = torch.long).to(device)\n",
        "\n",
        "  with torch.no_grad():\n",
        "    pred_affinity = model(drug_graph, protein_seq).item()\n",
        "\n",
        "  results.append({\n",
        "      \"True Affinity\": round(true_affinity.item(), 4),\n",
        "      \"Predicted Affinity\": round(pred_affinity, 4)\n",
        "  })\n"
      ],
      "metadata": {
        "id": "q8Q_Ps4DUBhF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Display Predictions\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "df_results = pd.DataFrame(results)\n",
        "\n",
        "print(df_results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T3W5YzWpU6Kh",
        "outputId": "b1acb0b0-4ae0-4aef-f63e-1881b6b136d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   True Affinity  Predicted Affinity\n",
            "0            5.0              4.9880\n",
            "1            5.0              4.9972\n",
            "2            5.0              4.8866\n",
            "3            5.0              4.9412\n",
            "4            5.0              4.8374\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9Zdt-mjnVAZz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}